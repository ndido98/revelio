{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Revelio Documentation : https://ndido98.github.io/revelio Source Code : https://github.com/ndido98/revelio PyPI : https://pypi.org/project/revelio/ A declarative framework for Morphing Attack Detection experiments Installation git clone git@github.com:ndido98/revelio cd revelio poetry install poetry run revelio Development Clone this repository Requirements: Poetry Python 3.10+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell Testing pytest Documentation The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release. Pre-commit Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install pre-commit install -t commit-msg Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Introduction"},{"location":"#revelio","text":"Documentation : https://ndido98.github.io/revelio Source Code : https://github.com/ndido98/revelio PyPI : https://pypi.org/project/revelio/ A declarative framework for Morphing Attack Detection experiments","title":"Revelio"},{"location":"#installation","text":"git clone git@github.com:ndido98/revelio cd revelio poetry install poetry run revelio","title":"Installation"},{"location":"#development","text":"Clone this repository Requirements: Poetry Python 3.10+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell","title":"Development"},{"location":"#testing","text":"pytest","title":"Testing"},{"location":"#documentation","text":"The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release.","title":"Documentation"},{"location":"#pre-commit","text":"Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install pre-commit install -t commit-msg Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Pre-commit"},{"location":"changelog/","text":"3.0.0 (2023-01-09) \u26a0 BREAKING CHANGES neuralnet: make get_state_dict return deep copy Features augmentation: add jpeg2000 compression step ( 6700c95 ) caching: abstract face detection/feature extraction load/save with cachers ( 8994668 ) feature-extraction: add stationary wavelet packets extraction ( 905c78a ) nn: add load_from_checkpoint argument to optimizer when fine tuning ( 32c41d5 ) utils: add glob_multiple utility function ( 51328a3 ) Bug Fixes cli: add pretty printing of exceptions when not verbose ( 26a374d ) dataset: allow data augmentation to skip failing step ( f5e6b96 ) neuralnet: make get_state_dict return deep copy ( 7236c50 ) 2.0.0 (2022-12-12) \u26a0 BREAKING CHANGES augmentation: change default N2 noise value for print&scan simulation dataset: remove automatic bgr to rgb conversion cli: reset seed before training and before evaluation augmentation: change signature of augmentation process_element Features augmentation: add grayscale augmentation ( b81d2db ) augmentation: add jpeg and resize augmentations ( 5b52035 ) augmentation: add print&scan augmentation step ( 458394b ) augmentation: add stack config to grayscale step ( a79e642 ) config: add json metrics file report ( 4032d95 ) dataset: allow for empty test set ( ef11f07 ) feature-extraction: add prnu, fourier and wavelets ( 4393e3e ) loaders: add cfd/cfdmorph loaders ( 4454797 ) loaders: add morph level args to CFDMorph loader ( f273cc5 ) loaders: add png and jpg loading for cfd and cfdmorph ( 0106e99 ) model: add feature inception resnet ( d247360 ) model: use 8 instead of 5 decimals in scores ( 5b818d4 ) preprocessing: add dataset-variant preprocessing ( daa6247 ) preprocessing: add select channel and color space conversions ( b8795b9 ) preprocessing: make maximum value configurable for each channel ( ef58d51 ) Bug Fixes augmentation: change default N2 noise value for print&scan simulation ( 428957d ) augmentation: make print&scan sigma proportional to image diagonal ( 4ce23e1 ) callbacks: fix missing image report if fine tuning ( 9b7abb6 ) cli: reset seed before training and before evaluation ( 3d8d62b ) dataset: fix use before assign in offline processing ( 677d798 ) model: mitigate memory leak in model evaluation ( a7919b6 ) Code Refactoring augmentation: change signature of augmentation process_element ( cab004f ) dataset: remove automatic bgr to rgb conversion ( 5e28a29 ) 1.0.6 (2022-12-08) Bug Fixes deps: update dependency scikit-learn to ~1.2.0 ( c856597 ) 1.0.5 (2022-12-07) Bug Fixes deps: update dependency wandb to v0.13.6 ( 65436b3 ) 1.0.4 (2022-11-20) Bug Fixes deps: update dependency numpy to v1.23.5 ( 061aea3 ) 1.0.3 (2022-11-09) Bug Fixes deps: update dependency tensorboard to ~2.11.0 ( c2bdf22 ) 1.0.2 (2022-11-04) Bug Fixes deps: update dependency wandb to v0.13.5 ( 2fa7a78 ) 1.0.1 (2022-11-03) Bug Fixes deps: update dependency matplotlib to v3.6.2 ( e192a41 ) 1.0.0 (2022-11-02) Features add logging ( 474f39f ) augmentation: add applies_to field ( 8a1a21e ) augmentation: add augmentation steps ( 2eb2b8e ) callbacks: add early stopping ( ad48f03 ) callbacks: add memory profiling to tensorboard ( 5fdd28f ) callbacks: add model checkpoint ( b5ffef8 ) callbacks: add steps count ( 13b7ea4 ) callbacks: add tensorboard batch viz, graph ( c7c868e ) callbacks: add tensorboard callback ( a86ddeb ) cli: add --no-warmup argument to skip warmup ( c9fce89 ) cli: add cli argparser ( ac5cc8c ) cli: add configurable warmup workers count ( 171bc21 ) cli: add model fitting to cli ( 21e0d0c ) cli: avoid creating train/val workers when only inferencing ( 0258090 ) config: add config model ( 6060d89 ) config: add configurable seed ( 0f43c1a ) config: add templating to scores files path ( 1a2c3e5 ) dataset: add check to make sure all elements have the same number of x ( 97993c9 ) dataset: add dataset element object ( 4754cfb ) dataset: add dataset factory and torch dataset ( c1080f9 ) dataset: add dataset loader ( 9627071 ) dataset: add explicit loader with args ( e105f53 ) dataset: add length ( fec1bae ) dataset: add stats printing ( 8bc2a73 ) dataset: add warmup function ( 3b78208 ) dataset: create testing groups and rework splitting ( ac433b3 ) face-detection: add dlib detector ( b78add1 ) face-detection: add face detector module ( 3767762 ) face-detection: add opencv and mtcnn detectors ( cf0cc4e ) feature-extraction: add feature extractors ( 2dafd04 ) loaders: add biometix morphed loader ( 1ecc55a ) loaders: add morphdb loader ( 6de92b8 ) loaders: add pmdb loader ( a74c60b ) loaders: add several loaders ( 553f258 ) losses: add adam ( 56992b3 ) metrics: add accuracy ( 2ebd862 ) metrics: add eer and bpcer@apcer ( 0f25bc7 ) metrics: add epoch_* metrics for tensorboard, checkpoint and early stopping callbacks ( 212351a ) metrics: add metrics ( 7938b00 ) metrics: add tpr and tnr ( cea5757 ) metrics: allow multiple values in one metric ( e45cddd ) metrics: expose device in which metrics are run ( c50a2fc ) model: add alexnet, vgg and resnet ( 81b603f ) model: add base model class ( ded81c9 ) model: add inception resnet ( 6fbe362 ) model: add mobilenet ( cbdfa5f ) model: add neural network class ( 4d0eff2 ) model: add random guesser ( a4dd65a ) model: add save/load checkpoint ( 097c78f ) model: add squeezenet ( 7e6b44d ) model: add vision transformer ( 1c441ff ) model: move scores file path eval to model score computation ( ecf21f8 ) optimizers: add binary cross entropy ( 3f34637 ) preprocessing: add normalization preprocessing ( 230650a ) preprocessing: add preprocessing phase after feature extraction ( 47c35e4 ) preprocessing: add uint8 -> float32 preprocessing ( 24bf94b ) registry: add - as ignored char ( 1b395c7 ) registry: add transparent registrable classes ( 3da9dd6 ) registry: allow snake case names ( 9925ad9 ) registry: make kwargs with _ assignable only explicitly ( 906c73c ) Bug Fixes add dataset root to dataset element ( c07e304 ) augmentation: change signature of step to take only the image ( bb161e2 ) callbacks: add mkdir to model checkpoint target directory ( dc8c170 ) callbacks: change bona fide to live image reporting ( f25c69b ) callbacks: fix tensorboard graph/image display ( 598ef5a ) callbacks: import early stopping ( d097f89 ) callbacks: remove tensorboard graph ( b3ed308 ) cli: create dataloaders just for warming up for better progress reporting ( bf77bb7 ) cli: disable persistent workers if no workers are used ( 8ff3170 ) cli: use consume to warm up the datasets ( 4a302ef ) config: allow for no preprocessing ( 5df3bfc ) config: change DirectoryPath to str for yet-to-be directories ( b096dbf ) config: fix arg name cannot start with underscore ( 9745aa3 ) config: make args default to empty ( 3ddb895 ) config: validate paths without checking their existence ( 397bad0 ) dataset: add missing y label to yielded element ( cb98631 ) dataset: add randomization of dataset at each epoch ( 7842a32 ) dataset: allow float32 images ( 755c1a2 ) dataset: apply color and channel transposion ( 3e9f853 ) dataset: force gc collection if not loaded from cache ( 45e1353 ) dataset: make face detection offline ( 8c97c30 ) dataset: remove offline processing when not warming up ( eb6ae71 ) dataset: remove warmup function and instead use boolean flag ( 28b42de ) dataset: skip elements if face detection or feature extraction fails ( d67702c ) dataset: use specialized list to avoid memory leaks ( cd3b3ac ) deps: update dependency matplotlib to v3.6.1 ( 429ca2e ) deps: update dependency numpy to v1.23.4 ( 70ab22f ) deps: update dependency scikit-learn to v1.1.3 ( cfc8a7e ) deps: update dependency scipy to v1.9.2 ( 7c84265 ) deps: update dependency scipy to v1.9.3 ( 5b49637 ) face-detection: clip bounding box inside image ( cfadaca ) face-detection: fix numpy arrays not json serializable ( 714ef60 ) face-detection: take biggest bounding box for opencv/mtcnn multiple results ( 8d96535 ) loaders: make amsl loader deterministic ( 5e8b92f ) metrics: adapt bpcer@apcer to be more lax ( 2374786 ) metrics: add conditional to remove nan cases ( 5158c87 ) metrics: improve display of accuracy and bpcer@apcer ( 4a68287 ) metrics: use abstract property for name ( acad600 ) model: add list case to _dict_to_device and fix prediction scores accumulation ( a8f6c47 ) model: add missing definition of resnet model if pretrained ( ef090a7 ) model: apply sigmoid to logits output, remove cumulative loss ( 5b3b2d6 ) model: change scores file format ( 6bc5b9d ) model: fix epoch loading from state dict ( c32ee11 ) model: import neural nets module for registration ( 08fc220 ) model: load metrics from model constructor ( 445cdff ) model: move metrics reset outside batch processing ( bbfbb68 ) model: move predictions to correct device when computing metrics ( 8f5df94 ) nn: don't load callbacks if not training ( c6879b2 ) preprocessing: add interpolation to resize ( 7ee7580 ) preprocessing: redo args validation for normalize ( db7e546 ) registry: fix bug when loading class with args ( 365d942 ) registry: move args sanitization to config ( a690b80 ) use more accurate way of counting steps in data loader ( c726320 )","title":"Changelog"},{"location":"changelog/#300-2023-01-09","text":"","title":"3.0.0 (2023-01-09)"},{"location":"changelog/#breaking-changes","text":"neuralnet: make get_state_dict return deep copy","title":"\u26a0 BREAKING CHANGES"},{"location":"changelog/#features","text":"augmentation: add jpeg2000 compression step ( 6700c95 ) caching: abstract face detection/feature extraction load/save with cachers ( 8994668 ) feature-extraction: add stationary wavelet packets extraction ( 905c78a ) nn: add load_from_checkpoint argument to optimizer when fine tuning ( 32c41d5 ) utils: add glob_multiple utility function ( 51328a3 )","title":"Features"},{"location":"changelog/#bug-fixes","text":"cli: add pretty printing of exceptions when not verbose ( 26a374d ) dataset: allow data augmentation to skip failing step ( f5e6b96 ) neuralnet: make get_state_dict return deep copy ( 7236c50 )","title":"Bug Fixes"},{"location":"changelog/#200-2022-12-12","text":"","title":"2.0.0 (2022-12-12)"},{"location":"changelog/#breaking-changes_1","text":"augmentation: change default N2 noise value for print&scan simulation dataset: remove automatic bgr to rgb conversion cli: reset seed before training and before evaluation augmentation: change signature of augmentation process_element","title":"\u26a0 BREAKING CHANGES"},{"location":"changelog/#features_1","text":"augmentation: add grayscale augmentation ( b81d2db ) augmentation: add jpeg and resize augmentations ( 5b52035 ) augmentation: add print&scan augmentation step ( 458394b ) augmentation: add stack config to grayscale step ( a79e642 ) config: add json metrics file report ( 4032d95 ) dataset: allow for empty test set ( ef11f07 ) feature-extraction: add prnu, fourier and wavelets ( 4393e3e ) loaders: add cfd/cfdmorph loaders ( 4454797 ) loaders: add morph level args to CFDMorph loader ( f273cc5 ) loaders: add png and jpg loading for cfd and cfdmorph ( 0106e99 ) model: add feature inception resnet ( d247360 ) model: use 8 instead of 5 decimals in scores ( 5b818d4 ) preprocessing: add dataset-variant preprocessing ( daa6247 ) preprocessing: add select channel and color space conversions ( b8795b9 ) preprocessing: make maximum value configurable for each channel ( ef58d51 )","title":"Features"},{"location":"changelog/#bug-fixes_1","text":"augmentation: change default N2 noise value for print&scan simulation ( 428957d ) augmentation: make print&scan sigma proportional to image diagonal ( 4ce23e1 ) callbacks: fix missing image report if fine tuning ( 9b7abb6 ) cli: reset seed before training and before evaluation ( 3d8d62b ) dataset: fix use before assign in offline processing ( 677d798 ) model: mitigate memory leak in model evaluation ( a7919b6 )","title":"Bug Fixes"},{"location":"changelog/#code-refactoring","text":"augmentation: change signature of augmentation process_element ( cab004f ) dataset: remove automatic bgr to rgb conversion ( 5e28a29 )","title":"Code Refactoring"},{"location":"changelog/#106-2022-12-08","text":"","title":"1.0.6 (2022-12-08)"},{"location":"changelog/#bug-fixes_2","text":"deps: update dependency scikit-learn to ~1.2.0 ( c856597 )","title":"Bug Fixes"},{"location":"changelog/#105-2022-12-07","text":"","title":"1.0.5 (2022-12-07)"},{"location":"changelog/#bug-fixes_3","text":"deps: update dependency wandb to v0.13.6 ( 65436b3 )","title":"Bug Fixes"},{"location":"changelog/#104-2022-11-20","text":"","title":"1.0.4 (2022-11-20)"},{"location":"changelog/#bug-fixes_4","text":"deps: update dependency numpy to v1.23.5 ( 061aea3 )","title":"Bug Fixes"},{"location":"changelog/#103-2022-11-09","text":"","title":"1.0.3 (2022-11-09)"},{"location":"changelog/#bug-fixes_5","text":"deps: update dependency tensorboard to ~2.11.0 ( c2bdf22 )","title":"Bug Fixes"},{"location":"changelog/#102-2022-11-04","text":"","title":"1.0.2 (2022-11-04)"},{"location":"changelog/#bug-fixes_6","text":"deps: update dependency wandb to v0.13.5 ( 2fa7a78 )","title":"Bug Fixes"},{"location":"changelog/#101-2022-11-03","text":"","title":"1.0.1 (2022-11-03)"},{"location":"changelog/#bug-fixes_7","text":"deps: update dependency matplotlib to v3.6.2 ( e192a41 )","title":"Bug Fixes"},{"location":"changelog/#100-2022-11-02","text":"","title":"1.0.0 (2022-11-02)"},{"location":"changelog/#features_2","text":"add logging ( 474f39f ) augmentation: add applies_to field ( 8a1a21e ) augmentation: add augmentation steps ( 2eb2b8e ) callbacks: add early stopping ( ad48f03 ) callbacks: add memory profiling to tensorboard ( 5fdd28f ) callbacks: add model checkpoint ( b5ffef8 ) callbacks: add steps count ( 13b7ea4 ) callbacks: add tensorboard batch viz, graph ( c7c868e ) callbacks: add tensorboard callback ( a86ddeb ) cli: add --no-warmup argument to skip warmup ( c9fce89 ) cli: add cli argparser ( ac5cc8c ) cli: add configurable warmup workers count ( 171bc21 ) cli: add model fitting to cli ( 21e0d0c ) cli: avoid creating train/val workers when only inferencing ( 0258090 ) config: add config model ( 6060d89 ) config: add configurable seed ( 0f43c1a ) config: add templating to scores files path ( 1a2c3e5 ) dataset: add check to make sure all elements have the same number of x ( 97993c9 ) dataset: add dataset element object ( 4754cfb ) dataset: add dataset factory and torch dataset ( c1080f9 ) dataset: add dataset loader ( 9627071 ) dataset: add explicit loader with args ( e105f53 ) dataset: add length ( fec1bae ) dataset: add stats printing ( 8bc2a73 ) dataset: add warmup function ( 3b78208 ) dataset: create testing groups and rework splitting ( ac433b3 ) face-detection: add dlib detector ( b78add1 ) face-detection: add face detector module ( 3767762 ) face-detection: add opencv and mtcnn detectors ( cf0cc4e ) feature-extraction: add feature extractors ( 2dafd04 ) loaders: add biometix morphed loader ( 1ecc55a ) loaders: add morphdb loader ( 6de92b8 ) loaders: add pmdb loader ( a74c60b ) loaders: add several loaders ( 553f258 ) losses: add adam ( 56992b3 ) metrics: add accuracy ( 2ebd862 ) metrics: add eer and bpcer@apcer ( 0f25bc7 ) metrics: add epoch_* metrics for tensorboard, checkpoint and early stopping callbacks ( 212351a ) metrics: add metrics ( 7938b00 ) metrics: add tpr and tnr ( cea5757 ) metrics: allow multiple values in one metric ( e45cddd ) metrics: expose device in which metrics are run ( c50a2fc ) model: add alexnet, vgg and resnet ( 81b603f ) model: add base model class ( ded81c9 ) model: add inception resnet ( 6fbe362 ) model: add mobilenet ( cbdfa5f ) model: add neural network class ( 4d0eff2 ) model: add random guesser ( a4dd65a ) model: add save/load checkpoint ( 097c78f ) model: add squeezenet ( 7e6b44d ) model: add vision transformer ( 1c441ff ) model: move scores file path eval to model score computation ( ecf21f8 ) optimizers: add binary cross entropy ( 3f34637 ) preprocessing: add normalization preprocessing ( 230650a ) preprocessing: add preprocessing phase after feature extraction ( 47c35e4 ) preprocessing: add uint8 -> float32 preprocessing ( 24bf94b ) registry: add - as ignored char ( 1b395c7 ) registry: add transparent registrable classes ( 3da9dd6 ) registry: allow snake case names ( 9925ad9 ) registry: make kwargs with _ assignable only explicitly ( 906c73c )","title":"Features"},{"location":"changelog/#bug-fixes_8","text":"add dataset root to dataset element ( c07e304 ) augmentation: change signature of step to take only the image ( bb161e2 ) callbacks: add mkdir to model checkpoint target directory ( dc8c170 ) callbacks: change bona fide to live image reporting ( f25c69b ) callbacks: fix tensorboard graph/image display ( 598ef5a ) callbacks: import early stopping ( d097f89 ) callbacks: remove tensorboard graph ( b3ed308 ) cli: create dataloaders just for warming up for better progress reporting ( bf77bb7 ) cli: disable persistent workers if no workers are used ( 8ff3170 ) cli: use consume to warm up the datasets ( 4a302ef ) config: allow for no preprocessing ( 5df3bfc ) config: change DirectoryPath to str for yet-to-be directories ( b096dbf ) config: fix arg name cannot start with underscore ( 9745aa3 ) config: make args default to empty ( 3ddb895 ) config: validate paths without checking their existence ( 397bad0 ) dataset: add missing y label to yielded element ( cb98631 ) dataset: add randomization of dataset at each epoch ( 7842a32 ) dataset: allow float32 images ( 755c1a2 ) dataset: apply color and channel transposion ( 3e9f853 ) dataset: force gc collection if not loaded from cache ( 45e1353 ) dataset: make face detection offline ( 8c97c30 ) dataset: remove offline processing when not warming up ( eb6ae71 ) dataset: remove warmup function and instead use boolean flag ( 28b42de ) dataset: skip elements if face detection or feature extraction fails ( d67702c ) dataset: use specialized list to avoid memory leaks ( cd3b3ac ) deps: update dependency matplotlib to v3.6.1 ( 429ca2e ) deps: update dependency numpy to v1.23.4 ( 70ab22f ) deps: update dependency scikit-learn to v1.1.3 ( cfc8a7e ) deps: update dependency scipy to v1.9.2 ( 7c84265 ) deps: update dependency scipy to v1.9.3 ( 5b49637 ) face-detection: clip bounding box inside image ( cfadaca ) face-detection: fix numpy arrays not json serializable ( 714ef60 ) face-detection: take biggest bounding box for opencv/mtcnn multiple results ( 8d96535 ) loaders: make amsl loader deterministic ( 5e8b92f ) metrics: adapt bpcer@apcer to be more lax ( 2374786 ) metrics: add conditional to remove nan cases ( 5158c87 ) metrics: improve display of accuracy and bpcer@apcer ( 4a68287 ) metrics: use abstract property for name ( acad600 ) model: add list case to _dict_to_device and fix prediction scores accumulation ( a8f6c47 ) model: add missing definition of resnet model if pretrained ( ef090a7 ) model: apply sigmoid to logits output, remove cumulative loss ( 5b3b2d6 ) model: change scores file format ( 6bc5b9d ) model: fix epoch loading from state dict ( c32ee11 ) model: import neural nets module for registration ( 08fc220 ) model: load metrics from model constructor ( 445cdff ) model: move metrics reset outside batch processing ( bbfbb68 ) model: move predictions to correct device when computing metrics ( 8f5df94 ) nn: don't load callbacks if not training ( c6879b2 ) preprocessing: add interpolation to resize ( 7ee7580 ) preprocessing: redo args validation for normalize ( db7e546 ) registry: fix bug when loading class with args ( 365d942 ) registry: move args sanitization to config ( a690b80 ) use more accurate way of counting steps in data loader ( c726320 )","title":"Bug Fixes"},{"location":"config-reference/","text":"Configuration file reference Revelio is a declarative framework for running Morphing Detection Attack experiments, therefore its configuration must be done using a YAML file. This section of the documentation contains all the allowed settings and their description. Main structure The main components of a configuration file are the following: seed (optional) datasets face detection augmentation feature extraction preprocessing experiment The order of the components is not relevant, but it is recommended to follow the order above, as it is the order in which the components are executed.","title":"Configuration file reference"},{"location":"config-reference/#configuration-file-reference","text":"Revelio is a declarative framework for running Morphing Detection Attack experiments, therefore its configuration must be done using a YAML file. This section of the documentation contains all the allowed settings and their description.","title":"Configuration file reference"},{"location":"config-reference/#main-structure","text":"The main components of a configuration file are the following: seed (optional) datasets face detection augmentation feature extraction preprocessing experiment The order of the components is not relevant, but it is recommended to follow the order above, as it is the order in which the components are executed.","title":"Main structure"},{"location":"config-reference/augmentation/","text":"Augmentation You can specify the data augmentation pipeline in the augmentation section of the configuration file. The augmentation pipeline is a list of transformations that are probabilistically applied to the images in the dataset. The transformations are applied in the order they are specified in the configuration file. An example of the most minimal non-empty augmentation pipeline is the following: augmentation : enabled : true steps : - uses : random_crop args : crop_size : 224 The enabled field is a boolean that indicates whether data augmentation should be applied to the dataset. If enabled is false , the augmentation pipeline is ignored and the dataset is not augmented. The steps field is a list of transformations that are applied to the dataset. Each transformation is specified by a dictionary with at least the uses field, which indicates the name of the transformation to use. A step can also have an args field, which is a dictionary of arguments that will be passed to the transformation. Note The uses field is case-insensitive, and all hyphens and underscores are ignored, so RANDOM_CROP , RandomCrop , random-crop and random_crop are all equivalent and will use the same RandomCrop transformation. There are some other fields that can be specified for each augmentation step: probability (optional float, default: 1.0 ): the probability that the transformation will be applied to the image. If probability is 0.5 , the transformation will be applied to half of the images in the dataset. applies_to (optional list of strings or ints, default: all ): the list which contains the indices of the images that the transformation will be applied to, relative to the dataset element (which can have more than one image). If applies_to is all , the transformation will be applied to all images in the dataset element. If applies_to is [0, 2] , the transformation will be applied to the first and third images in the dataset element. There are some special values for applies_to : all : the transformation will be applied to all images in the dataset element (this is the default value); if this value is used, no other value can be specified. probe : the transformation will be applied to the first image in the dataset element, which by convention is the probe image. live : the transformation will be applied to the second image in the dataset element, which by convention is the live image. Note The probability of applying a transformation to an image is on the whole step, not on each image in the dataset element. For instance, if the dataset element has 3 images, and the step has probability: 0.5 and applies_to: [0, 2] , the transformation will be atomically applied to both images with probability 0.5 : either both images will be transformed, or none of them will.","title":"Augmentation"},{"location":"config-reference/augmentation/#augmentation","text":"You can specify the data augmentation pipeline in the augmentation section of the configuration file. The augmentation pipeline is a list of transformations that are probabilistically applied to the images in the dataset. The transformations are applied in the order they are specified in the configuration file. An example of the most minimal non-empty augmentation pipeline is the following: augmentation : enabled : true steps : - uses : random_crop args : crop_size : 224 The enabled field is a boolean that indicates whether data augmentation should be applied to the dataset. If enabled is false , the augmentation pipeline is ignored and the dataset is not augmented. The steps field is a list of transformations that are applied to the dataset. Each transformation is specified by a dictionary with at least the uses field, which indicates the name of the transformation to use. A step can also have an args field, which is a dictionary of arguments that will be passed to the transformation. Note The uses field is case-insensitive, and all hyphens and underscores are ignored, so RANDOM_CROP , RandomCrop , random-crop and random_crop are all equivalent and will use the same RandomCrop transformation. There are some other fields that can be specified for each augmentation step: probability (optional float, default: 1.0 ): the probability that the transformation will be applied to the image. If probability is 0.5 , the transformation will be applied to half of the images in the dataset. applies_to (optional list of strings or ints, default: all ): the list which contains the indices of the images that the transformation will be applied to, relative to the dataset element (which can have more than one image). If applies_to is all , the transformation will be applied to all images in the dataset element. If applies_to is [0, 2] , the transformation will be applied to the first and third images in the dataset element. There are some special values for applies_to : all : the transformation will be applied to all images in the dataset element (this is the default value); if this value is used, no other value can be specified. probe : the transformation will be applied to the first image in the dataset element, which by convention is the probe image. live : the transformation will be applied to the second image in the dataset element, which by convention is the live image. Note The probability of applying a transformation to an image is on the whole step, not on each image in the dataset element. For instance, if the dataset element has 3 images, and the step has probability: 0.5 and applies_to: [0, 2] , the transformation will be atomically applied to both images with probability 0.5 : either both images will be transformed, or none of them will.","title":"Augmentation"},{"location":"config-reference/datasets/","text":"Datasets This section contains the definition of the datasets used in the experiment, during the training and evaluation phases. The most minimal dataset definition is done in the following way: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 - ... The name field is used to identify the dataset in the experiment, and the path field is the path to the root directory of the dataset. The split field is used to define the split of the dataset into training, validation and test sets. The three numbers (each between 0 and 1, both inclusive) represent the percentage of the dataset that will be used for each of the three sets. Note The sum of the three numbers must be at most 1. However, it's not required that the sum of the three numbers is exactly 1: for instance, if you don't want to insert the dataset in the validation set, you can set the val field to 0. The same applies to both train and test . This feature is particularly useful when you want to use only part of a dataset for testing, because having it in its entirety would unbalance the test set. These three fields are required, but you can also add other fields to the dataset definition, as described in the following sections. Dataset loading The way in which a dataset is loaded is defined by the so-called dataset loader , which is a class that scans the dataset root directory and returns a list of items with their respective classes, which are then used to create the whole dataset. By default, the name of the dataset also determines the dataset loader that will be used to load the dataset. For instance, if the dataset name is morphdb , the dataset loader that will be used is MorphDBLoader , which is the default dataset loader for the MorphDB dataset. Note The dataset name is case-insensitive, and all hyphens and underscores are ignored, so MORPHDB , MorphDB , morph-db and morph_db are all equivalent and will use the same MorphDBLoader . If you want to know how to implement a dataset loader, see the reference for further details . Some loaders can also accept additional parameters, which can be specified in the loader.args section of the dataset definition: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 loader : args : arg1 : value1 arg2 : value2 ... - ... The args field is a dictionary of arguments that will be passed to the dataset loader. Sometimes, it is useful to explicitly specify the dataset loader to use, even if the dataset name would normally imply a different loader. This can be done by specifying the loader.name field in the dataset definition: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 loader : name : MyCustomLoader args : arg1 : value1 arg2 : value2 ... - ... Warning Unlike the name field, the loader.name field is case-sensitive, and an exact match is required. Testing groups By default, the model is evaluated on the entirety of the test set. Sometimes it is useful to evaluate the performance of a model only on a subset of the test set, rather than on its entirety. Therefore, you can assign a dataset to multiple testing groups, and the metrics will be computed for each group separately. To do so, you can specify the testing_groups field in the dataset definition: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 testing_groups : - group1 - group2 - ... - ... Note The testing_groups field is ignored if the dataset is not in the test set. The model will be evaluated on each of the groups separately, and on the whole test set. Warning Make sure that each group contains at least one item for each class, otherwise some metrics such as EER and BPCER@APCER may not be computed.","title":"Datasets"},{"location":"config-reference/datasets/#datasets","text":"This section contains the definition of the datasets used in the experiment, during the training and evaluation phases. The most minimal dataset definition is done in the following way: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 - ... The name field is used to identify the dataset in the experiment, and the path field is the path to the root directory of the dataset. The split field is used to define the split of the dataset into training, validation and test sets. The three numbers (each between 0 and 1, both inclusive) represent the percentage of the dataset that will be used for each of the three sets. Note The sum of the three numbers must be at most 1. However, it's not required that the sum of the three numbers is exactly 1: for instance, if you don't want to insert the dataset in the validation set, you can set the val field to 0. The same applies to both train and test . This feature is particularly useful when you want to use only part of a dataset for testing, because having it in its entirety would unbalance the test set. These three fields are required, but you can also add other fields to the dataset definition, as described in the following sections.","title":"Datasets"},{"location":"config-reference/datasets/#dataset-loading","text":"The way in which a dataset is loaded is defined by the so-called dataset loader , which is a class that scans the dataset root directory and returns a list of items with their respective classes, which are then used to create the whole dataset. By default, the name of the dataset also determines the dataset loader that will be used to load the dataset. For instance, if the dataset name is morphdb , the dataset loader that will be used is MorphDBLoader , which is the default dataset loader for the MorphDB dataset. Note The dataset name is case-insensitive, and all hyphens and underscores are ignored, so MORPHDB , MorphDB , morph-db and morph_db are all equivalent and will use the same MorphDBLoader . If you want to know how to implement a dataset loader, see the reference for further details . Some loaders can also accept additional parameters, which can be specified in the loader.args section of the dataset definition: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 loader : args : arg1 : value1 arg2 : value2 ... - ... The args field is a dictionary of arguments that will be passed to the dataset loader. Sometimes, it is useful to explicitly specify the dataset loader to use, even if the dataset name would normally imply a different loader. This can be done by specifying the loader.name field in the dataset definition: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 loader : name : MyCustomLoader args : arg1 : value1 arg2 : value2 ... - ... Warning Unlike the name field, the loader.name field is case-sensitive, and an exact match is required.","title":"Dataset loading"},{"location":"config-reference/datasets/#testing-groups","text":"By default, the model is evaluated on the entirety of the test set. Sometimes it is useful to evaluate the performance of a model only on a subset of the test set, rather than on its entirety. Therefore, you can assign a dataset to multiple testing groups, and the metrics will be computed for each group separately. To do so, you can specify the testing_groups field in the dataset definition: datasets : - name : dataset_name path : /path/to/dataset/root split : train : 0.7 val : 0.1 test : 0.2 testing_groups : - group1 - group2 - ... - ... Note The testing_groups field is ignored if the dataset is not in the test set. The model will be evaluated on each of the groups separately, and on the whole test set. Warning Make sure that each group contains at least one item for each class, otherwise some metrics such as EER and BPCER@APCER may not be computed.","title":"Testing groups"},{"location":"config-reference/experiment/","text":"Experiment The experiment section of the configuration file contains the settings that are used to run the experiment. experiment : batch_size : 64 model : name : model-name args : arg1 : value1 arg2 : value2 ... training : enabled : true args : arg1 : value1 arg2 : value2 ... scores : bona_fide : /path/to/bona_fide_scores.txt morphed : /path/to/morphed_scores.txt metrics : /path/to/metrics.json metrics : - name : metric1 args : arg1 : value1 arg2 : value2 - name : metric2 - ... While many of the settings vary according to the model used, there are some settings that are common to all models. The batch_size setting specifies the batch size to use when training the model. The model setting specifies the model to use. It has two keys: name , which contains the name of the model, and args , which contains the arguments that will be passed when creating the model. Inside the model setting you can specify the checkpoint field, which contains the path to a checkpoint file. If this field is specified, the model will be loaded from the checkpoint file instead of being created from scratch. The training setting specifies the training settings. It has two keys: enabled , which is a boolean that specifies whether to train the model, and args , which contains the arguments that will be passed when training the model. If the enabled field is set to false , the model will not be trained, and the model specified in the model setting will run only in inference mode. Each model has its own set of training arguments. For instance, a neural network has a much more complicated set of training arguments than a simple linear model. The scores setting specifies the paths to the files containing the scores for the bona fide and morphed images. The scores are the result of the model evaluation on the test set images. The scores.metrics field contains the path to a JSON file where the metrics for each testing group will be saved. This field is optional. The metrics setting specifies the metrics to use to evaluate the model. It contains a list of metrics, each of which has a name and an optional args field. Available models There are two main models already available in Revelio: a neural network and a random guesser. Neural network The configuration of a neural network is quite complex, and it is described in the Neural network page. Random guesser The random guesser is a simple model that always returns a random score between 0 and 1. The random guesser has no arguments. Available metrics There are several metrics already available in Revelio: a complete list can be found in the Metrics page.","title":"Experiment"},{"location":"config-reference/experiment/#experiment","text":"The experiment section of the configuration file contains the settings that are used to run the experiment. experiment : batch_size : 64 model : name : model-name args : arg1 : value1 arg2 : value2 ... training : enabled : true args : arg1 : value1 arg2 : value2 ... scores : bona_fide : /path/to/bona_fide_scores.txt morphed : /path/to/morphed_scores.txt metrics : /path/to/metrics.json metrics : - name : metric1 args : arg1 : value1 arg2 : value2 - name : metric2 - ... While many of the settings vary according to the model used, there are some settings that are common to all models. The batch_size setting specifies the batch size to use when training the model. The model setting specifies the model to use. It has two keys: name , which contains the name of the model, and args , which contains the arguments that will be passed when creating the model. Inside the model setting you can specify the checkpoint field, which contains the path to a checkpoint file. If this field is specified, the model will be loaded from the checkpoint file instead of being created from scratch. The training setting specifies the training settings. It has two keys: enabled , which is a boolean that specifies whether to train the model, and args , which contains the arguments that will be passed when training the model. If the enabled field is set to false , the model will not be trained, and the model specified in the model setting will run only in inference mode. Each model has its own set of training arguments. For instance, a neural network has a much more complicated set of training arguments than a simple linear model. The scores setting specifies the paths to the files containing the scores for the bona fide and morphed images. The scores are the result of the model evaluation on the test set images. The scores.metrics field contains the path to a JSON file where the metrics for each testing group will be saved. This field is optional. The metrics setting specifies the metrics to use to evaluate the model. It contains a list of metrics, each of which has a name and an optional args field.","title":"Experiment"},{"location":"config-reference/experiment/#available-models","text":"There are two main models already available in Revelio: a neural network and a random guesser.","title":"Available models"},{"location":"config-reference/experiment/#neural-network","text":"The configuration of a neural network is quite complex, and it is described in the Neural network page.","title":"Neural network"},{"location":"config-reference/experiment/#random-guesser","text":"The random guesser is a simple model that always returns a random score between 0 and 1. The random guesser has no arguments.","title":"Random guesser"},{"location":"config-reference/experiment/#available-metrics","text":"There are several metrics already available in Revelio: a complete list can be found in the Metrics page.","title":"Available metrics"},{"location":"config-reference/experiment/metrics/","text":"Metrics There are several metrics already available in Revelio: Accuracy True positive rate True negative rate BPCER@APCER EER No metric has any arguments, except for the following: BPCER@APCER The BPCER@APCER metric has the following arguments: thresholds (required list of floats): the thresholds to use to compute the BPCER@APCER metric. The most used thresholds are 0.1, 0.05, 0.01 and 0.001.","title":"Metrics"},{"location":"config-reference/experiment/metrics/#metrics","text":"There are several metrics already available in Revelio: Accuracy True positive rate True negative rate BPCER@APCER EER No metric has any arguments, except for the following:","title":"Metrics"},{"location":"config-reference/experiment/metrics/#bpcerapcer","text":"The BPCER@APCER metric has the following arguments: thresholds (required list of floats): the thresholds to use to compute the BPCER@APCER metric. The most used thresholds are 0.1, 0.05, 0.01 and 0.001.","title":"BPCER@APCER"},{"location":"config-reference/experiment/neural-network/","text":"Neural network The neural network model is an abstract model that can be used to create any neural network model. It is then up to the user to specify the architecture of the neural network model. While the args in the model section vary according to the model, the args in the training section are the same for all neural network models. training : enabled : true args : epochs : 50 optimizer : name : SGD args : lr : 0.0005 loss : name : BCEWithLogitsLoss callbacks : - name : callback1 args : arg1 : value1 arg2 : value2 ... - ... The epochs argument specifies the number of epochs to train the model for. The optimizer argument specifies the optimizer to use. Its most basic configuration has two keys: name , which contains the name of the optimizer, and args , which contains the arguments that will be passed when creating the optimizer. There is a third argument, which is useful when fine tuning a pretrained model: load_from_checkpoint (default true ), which specifies whether to load the optimizer state from the checkpoint or not. By default, the optimizer state is loaded from the checkpoint, so that the optimizer can continue from where it left off. If you fine tune with a different optimizer than the original one, this argument is automatically set to false and it is not necessary to specify it. If you fine tune with the same optimizer, but you want to change its hyperparameters (e.g. the learning rate), you must set this argument to false and specify the new hyperparameters in the configuration file. Warning If you fine tune with the same optimizer, but you want to change its hyperparameters (e.g. the learning rate) and you forget to set load_from_checkpoint to false , the optimizer will continue from where it left off, and the new hyperparameters will be ignored, leading to unexpected results during training and validation. The loss argument specifies the loss function to use. It has two keys: name , which contains the name of the loss function, and args , which contains the arguments that will be passed when creating the loss function. The callbacks argument specifies the callbacks to use. It is a list of callbacks, each of which has a name and an optional args field. Available optimizers There are some optimizers already available in Revelio: SGD Adam In both optimizers the args field allows the same fields as the corresponding PyTorch optimizer; therefore, at least the lr field must be specified. Available loss functions There are some loss functions already available in Revelio: BCEWithLogitsLoss BCELoss In both loss functions the args field allows the same fields as the corresponding PyTorch loss function. Available callbacks There are some callbacks already available in Revelio: EarlyStopping ModelCheckpoint TensorBoard The details of each callback are described in the following sections. EarlyStopping The EarlyStopping callback stops the training when a monitored quantity has stopped improving. callbacks : - name : EarlyStopping args : monitor : val_loss min_delta : 0.001 patience : 5 direction : min restore_best_weights : true The EarlyStopping callback has the following arguments: monitor (optional string, default: val_loss ): the quantity to be monitored. It can be either loss or any of the metrics specified in the metrics section of the configuration file. If you want to monitor a validation-time metric, prepend the metric name with val_ . min_delta (optional float, default: 0.0 ): the minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta , will count as no improvement. patience (optional int, default: 0 ): the number of epochs with no improvement after which training will be stopped. direction (optional string, default: min ): whether to monitor the quantity in an increasing or decreasing way. It can be either min or max . restore_best_weights (optional bool, default: false ): whether to restore model weights from the epoch with the best value of the monitored quantity. If false , the model weights obtained at the last step of training are used. ModelCheckpoint The ModelCheckpoint callback saves the model after every epoch. callbacks : - name : ModelCheckpoint args : file_path : /path/to/checkpoint.pt monitor : val_loss min_delta : 0.001 direction : min save_best_only : true The ModelCheckpoint callback has the following arguments: file_path (string): the path to the file where the model will be saved. monitor (optional string, default: val_loss ): the quantity to be monitored. It can be either loss or any of the metrics specified in the metrics section of the configuration file. If you want to monitor a validation-time metric, prepend the metric name with val_ . min_delta (optional float, default: 0.0 ): the minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta , will count as no improvement. direction (optional string, default: min ): whether to monitor the quantity in an increasing or decreasing way. It can be either min or max . save_best_only (optional bool, default: false ): whether to save the model only if the monitored quantity has improved. The file_path argument can contain the following placeholders, wrapped around curly braces: {epoch} : the epoch number. {now} : the current date and time. {metric} : the value of the monitored metric (e.g. {val_loss:.4f} ). Formatting can be specified after the metric name, as in the example. For more information on formatting, see the Format String Syntax on the Python documentation . TensorBoard The TensorBoard callback logs the training metrics to TensorBoard. callbacks : - name : TensorBoard args : log_dir : /path/to/logs profile : false The TensorBoard callback has the following arguments: log_dir (string): the path to the directory where the logs will be saved. profile (optional bool, default: False ): whether to profile the training process. The TensorBoard callback reports all the metrics specified in the metrics section of the configuration file, as well as the loss. For each metric, the callback reports the value of the metric at the end of each epoch (with the epoch_ prefix), as well as the value of the metric at the end of each batch, both for training and validation. For example, if the metrics section of the configuration file contains accuracy , the callback will report the following metrics: epoch_accuracy epoch_val_accuracy accuracy val_accuracy Also, at the very first batch of the first epoch, the callback reports the images in the training and validation set, as well as the corresponding labels.","title":"Neural network"},{"location":"config-reference/experiment/neural-network/#neural-network","text":"The neural network model is an abstract model that can be used to create any neural network model. It is then up to the user to specify the architecture of the neural network model. While the args in the model section vary according to the model, the args in the training section are the same for all neural network models. training : enabled : true args : epochs : 50 optimizer : name : SGD args : lr : 0.0005 loss : name : BCEWithLogitsLoss callbacks : - name : callback1 args : arg1 : value1 arg2 : value2 ... - ... The epochs argument specifies the number of epochs to train the model for. The optimizer argument specifies the optimizer to use. Its most basic configuration has two keys: name , which contains the name of the optimizer, and args , which contains the arguments that will be passed when creating the optimizer. There is a third argument, which is useful when fine tuning a pretrained model: load_from_checkpoint (default true ), which specifies whether to load the optimizer state from the checkpoint or not. By default, the optimizer state is loaded from the checkpoint, so that the optimizer can continue from where it left off. If you fine tune with a different optimizer than the original one, this argument is automatically set to false and it is not necessary to specify it. If you fine tune with the same optimizer, but you want to change its hyperparameters (e.g. the learning rate), you must set this argument to false and specify the new hyperparameters in the configuration file. Warning If you fine tune with the same optimizer, but you want to change its hyperparameters (e.g. the learning rate) and you forget to set load_from_checkpoint to false , the optimizer will continue from where it left off, and the new hyperparameters will be ignored, leading to unexpected results during training and validation. The loss argument specifies the loss function to use. It has two keys: name , which contains the name of the loss function, and args , which contains the arguments that will be passed when creating the loss function. The callbacks argument specifies the callbacks to use. It is a list of callbacks, each of which has a name and an optional args field.","title":"Neural network"},{"location":"config-reference/experiment/neural-network/#available-optimizers","text":"There are some optimizers already available in Revelio: SGD Adam In both optimizers the args field allows the same fields as the corresponding PyTorch optimizer; therefore, at least the lr field must be specified.","title":"Available optimizers"},{"location":"config-reference/experiment/neural-network/#available-loss-functions","text":"There are some loss functions already available in Revelio: BCEWithLogitsLoss BCELoss In both loss functions the args field allows the same fields as the corresponding PyTorch loss function.","title":"Available loss functions"},{"location":"config-reference/experiment/neural-network/#available-callbacks","text":"There are some callbacks already available in Revelio: EarlyStopping ModelCheckpoint TensorBoard The details of each callback are described in the following sections.","title":"Available callbacks"},{"location":"config-reference/experiment/neural-network/#earlystopping","text":"The EarlyStopping callback stops the training when a monitored quantity has stopped improving. callbacks : - name : EarlyStopping args : monitor : val_loss min_delta : 0.001 patience : 5 direction : min restore_best_weights : true The EarlyStopping callback has the following arguments: monitor (optional string, default: val_loss ): the quantity to be monitored. It can be either loss or any of the metrics specified in the metrics section of the configuration file. If you want to monitor a validation-time metric, prepend the metric name with val_ . min_delta (optional float, default: 0.0 ): the minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta , will count as no improvement. patience (optional int, default: 0 ): the number of epochs with no improvement after which training will be stopped. direction (optional string, default: min ): whether to monitor the quantity in an increasing or decreasing way. It can be either min or max . restore_best_weights (optional bool, default: false ): whether to restore model weights from the epoch with the best value of the monitored quantity. If false , the model weights obtained at the last step of training are used.","title":"EarlyStopping"},{"location":"config-reference/experiment/neural-network/#modelcheckpoint","text":"The ModelCheckpoint callback saves the model after every epoch. callbacks : - name : ModelCheckpoint args : file_path : /path/to/checkpoint.pt monitor : val_loss min_delta : 0.001 direction : min save_best_only : true The ModelCheckpoint callback has the following arguments: file_path (string): the path to the file where the model will be saved. monitor (optional string, default: val_loss ): the quantity to be monitored. It can be either loss or any of the metrics specified in the metrics section of the configuration file. If you want to monitor a validation-time metric, prepend the metric name with val_ . min_delta (optional float, default: 0.0 ): the minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta , will count as no improvement. direction (optional string, default: min ): whether to monitor the quantity in an increasing or decreasing way. It can be either min or max . save_best_only (optional bool, default: false ): whether to save the model only if the monitored quantity has improved. The file_path argument can contain the following placeholders, wrapped around curly braces: {epoch} : the epoch number. {now} : the current date and time. {metric} : the value of the monitored metric (e.g. {val_loss:.4f} ). Formatting can be specified after the metric name, as in the example. For more information on formatting, see the Format String Syntax on the Python documentation .","title":"ModelCheckpoint"},{"location":"config-reference/experiment/neural-network/#tensorboard","text":"The TensorBoard callback logs the training metrics to TensorBoard. callbacks : - name : TensorBoard args : log_dir : /path/to/logs profile : false The TensorBoard callback has the following arguments: log_dir (string): the path to the directory where the logs will be saved. profile (optional bool, default: False ): whether to profile the training process. The TensorBoard callback reports all the metrics specified in the metrics section of the configuration file, as well as the loss. For each metric, the callback reports the value of the metric at the end of each epoch (with the epoch_ prefix), as well as the value of the metric at the end of each batch, both for training and validation. For example, if the metrics section of the configuration file contains accuracy , the callback will report the following metrics: epoch_accuracy epoch_val_accuracy accuracy val_accuracy Also, at the very first batch of the first epoch, the callback reports the images in the training and validation set, as well as the corresponding labels.","title":"TensorBoard"},{"location":"config-reference/face-detection/","text":"Face detection The face detection section of the configuration file contains the settings that are used to detect faces in the images of the dataset, if needed. face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : mtcnn_detector args : arg1 : value1 arg2 : value2 ... The enabled field is a boolean that indicates whether face detection should be performed or not. The output_path field is a string that indicates the path where the face detection output should be saved. The algorithm field has two subfields: name specifies the name of the face detector to use, and args is a dictionary of arguments that will be passed to it. Note The name field is case-insensitive, and all hyphens and underscores are ignored, so MTCNN_DETECTOR , MtcnnDetector , mtcnn-detector and mtcnn_detector are all equivalent and will use the same MTCNNDetector . Face detectors There are three face detectors already available in Revelio: dlib OpenCV MTCNN Both dlib and MTCNN also extract facial landmarks, while OpenCV does not. dlib The dlib face detector is based on the dlib library. The dlib face detector has the following arguments: landmark_predictor_path (optional path, default: None ): the path to the file containing the facial landmark predictor model. If None , facial landmarks will not be extracted. The landmark predictor model can be downloaded from the dlib website . face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : dlib_detector args : landmark_predictor_path : /path/to/landmark_predictor.dat OpenCV The opencv face detector is based on the OpenCV library. The opencv face detector has the following arguments: classifier_path (required path): the path to the file containing the OpenCV face detector model that the CascadeClassifier will load. The most common model is haarcascade_frontalface_default.xml , which is available on the OpenCV GitHub repository . face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : opencv_detector args : classifier_path : /path/to/classifier.xml MTCNN The mtcnn face detector is based on the MTCNN library. The mtcnn face detector has no arguments. face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : mtcnn_detector","title":"Face detection"},{"location":"config-reference/face-detection/#face-detection","text":"The face detection section of the configuration file contains the settings that are used to detect faces in the images of the dataset, if needed. face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : mtcnn_detector args : arg1 : value1 arg2 : value2 ... The enabled field is a boolean that indicates whether face detection should be performed or not. The output_path field is a string that indicates the path where the face detection output should be saved. The algorithm field has two subfields: name specifies the name of the face detector to use, and args is a dictionary of arguments that will be passed to it. Note The name field is case-insensitive, and all hyphens and underscores are ignored, so MTCNN_DETECTOR , MtcnnDetector , mtcnn-detector and mtcnn_detector are all equivalent and will use the same MTCNNDetector .","title":"Face detection"},{"location":"config-reference/face-detection/#face-detectors","text":"There are three face detectors already available in Revelio: dlib OpenCV MTCNN Both dlib and MTCNN also extract facial landmarks, while OpenCV does not.","title":"Face detectors"},{"location":"config-reference/face-detection/#dlib","text":"The dlib face detector is based on the dlib library. The dlib face detector has the following arguments: landmark_predictor_path (optional path, default: None ): the path to the file containing the facial landmark predictor model. If None , facial landmarks will not be extracted. The landmark predictor model can be downloaded from the dlib website . face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : dlib_detector args : landmark_predictor_path : /path/to/landmark_predictor.dat","title":"dlib"},{"location":"config-reference/face-detection/#opencv","text":"The opencv face detector is based on the OpenCV library. The opencv face detector has the following arguments: classifier_path (required path): the path to the file containing the OpenCV face detector model that the CascadeClassifier will load. The most common model is haarcascade_frontalface_default.xml , which is available on the OpenCV GitHub repository . face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : opencv_detector args : classifier_path : /path/to/classifier.xml","title":"OpenCV"},{"location":"config-reference/face-detection/#mtcnn","text":"The mtcnn face detector is based on the MTCNN library. The mtcnn face detector has no arguments. face_detection : enabled : true output_path : /path/to/face_detection_output algorithm : name : mtcnn_detector","title":"MTCNN"},{"location":"config-reference/feature-extraction/","text":"Feature extraction The feature extraction section of the configuration file contains the settings that are used to extract features from the images in the dataset. feature_extraction : enabled : false output_path : /path/to/feature_extraction_output algorithms : - name : feature_extractor args : arg1 : value1 arg2 : value2 ... The enabled field is a boolean that indicates whether feature extraction should be performed or not. The output_path field is a string that indicates the path where the feature extraction output should be saved. The algorithms list contains all the algorithms that will be used to extract features of different types from the images in the dataset. Each algorithm has two keys: name , which contains the name of the algorithm, and args , which contains the arguments that will be passed to the algorithm. Note The name field is case-insensitive, and all hyphens and underscores are ignored, so FEATURE_EXTRACTOR , FeatureExtractor , feature-extractor and feature_extractor are all equivalent and will use the same FeatureExtractor algorithm.","title":"Feature extraction"},{"location":"config-reference/feature-extraction/#feature-extraction","text":"The feature extraction section of the configuration file contains the settings that are used to extract features from the images in the dataset. feature_extraction : enabled : false output_path : /path/to/feature_extraction_output algorithms : - name : feature_extractor args : arg1 : value1 arg2 : value2 ... The enabled field is a boolean that indicates whether feature extraction should be performed or not. The output_path field is a string that indicates the path where the feature extraction output should be saved. The algorithms list contains all the algorithms that will be used to extract features of different types from the images in the dataset. Each algorithm has two keys: name , which contains the name of the algorithm, and args , which contains the arguments that will be passed to the algorithm. Note The name field is case-insensitive, and all hyphens and underscores are ignored, so FEATURE_EXTRACTOR , FeatureExtractor , feature-extractor and feature_extractor are all equivalent and will use the same FeatureExtractor algorithm.","title":"Feature extraction"},{"location":"config-reference/preprocessing/","text":"Preprocessing The preprocessing section of the configuration file contains the settings that are used to preprocess the images in the dataset, right before passing them to the model. preprocessing : steps : - uses : step1 args : arg1 : value1 arg2 : value2 ... - ... The steps list contains all the preprocessing steps that will be applied to the images in the dataset. Each step has two keys: uses , which contains the name of the step, and args , which contains the arguments that will be passed to the step. Note The uses field is case-insensitive, and all hyphens and underscores are ignored, so STEP1 , Step1 , step-1 and step_1 are all equivalent and will use the same Step1 step. The transformations are applied in the order they are specified in the configuration file. Sometimes it is useful to apply a certain preprocessing step only to a specific dataset, for example when the augmentation pipeline has already taken care of some of the preprocessing steps for the training set. If that's the case, the datasets key can be used to specify the datasets that the preprocessing step will be applied to. preprocessing : steps : - uses : step1 datasets : [ train , val ] args : arg1 : value1 arg2 : value2 ... - ... The datasets key (if present) is a list of dataset names ( train , val , or test ) that the preprocessing step will be applied to. If the datasets key is not present, the preprocessing step will be applied to all the datasets. Preprocessing steps There are some preprocessing steps already available in Revelio. Resize The resize step resizes the images to the specified size. The resize step has the following arguments: width (required int): the width of the resized images. height (required int): the height of the resized images. algorithm (optional string, default: cubic ): the algorithm to use to resize the images. The available algorithms are the same as the ones available in OpenCV: nearest : nearest-neighbor interpolation linear : bilinear interpolation cubic : bicubic interpolation (default) area : resampling using pixel area relation lanczos4 : Lanczos interpolation over 8x8 neighborhood keep_aspect_ratio (optional bool, default: True ): whether to keep the aspect ratio of the images when resizing them. fill_mode (optional string, default: constant ): the strategy used for filling in newly created pixels, which can appear when using the keep_aspect_ratio option. The available strategies are the same as the ones available in OpenCV: constant : the pixels are filled with black (default, e.g. 000000|abcdefgh|000000 ) reflect : the pixels are filled with the reflection of the image (e.g. gfedcb|abcdefgh|gfedcba ) replicate : the pixels are filled with the last pixel of the image (e.g. aaaaaa|abcdefgh|hhhhhhh ) wrap : the pixels are filled with the wrap of the image (e.g. cdefgh|abcdefgh|abcdefg ) To float The to_float step converts the images to floating point images in the range [0, 1] . The to_float step has no arguments. Normalize The normalize step normalizes the images using the specified mean and standard deviation. The normalize step has the following arguments: mean (optional list of floats, default: None ): the mean to use to normalize the images, one element per BGR channel. std (optional list of floats, default: None ): the standard deviation to use to normalize the images, one element per BGR channel. preset (optional string, default: None ): the preset to use to normalize the images. The available presets are: imagenet : the mean and standard deviation used to normalize the images in the ImageNet dataset. Either preset or both mean and std must be specified. Warning Watch out for the order of the channels in the mean and standard deviation values. The order is BGR (i.e. OpenCV convention), not RGB.","title":"Preprocessing"},{"location":"config-reference/preprocessing/#preprocessing","text":"The preprocessing section of the configuration file contains the settings that are used to preprocess the images in the dataset, right before passing them to the model. preprocessing : steps : - uses : step1 args : arg1 : value1 arg2 : value2 ... - ... The steps list contains all the preprocessing steps that will be applied to the images in the dataset. Each step has two keys: uses , which contains the name of the step, and args , which contains the arguments that will be passed to the step. Note The uses field is case-insensitive, and all hyphens and underscores are ignored, so STEP1 , Step1 , step-1 and step_1 are all equivalent and will use the same Step1 step. The transformations are applied in the order they are specified in the configuration file. Sometimes it is useful to apply a certain preprocessing step only to a specific dataset, for example when the augmentation pipeline has already taken care of some of the preprocessing steps for the training set. If that's the case, the datasets key can be used to specify the datasets that the preprocessing step will be applied to. preprocessing : steps : - uses : step1 datasets : [ train , val ] args : arg1 : value1 arg2 : value2 ... - ... The datasets key (if present) is a list of dataset names ( train , val , or test ) that the preprocessing step will be applied to. If the datasets key is not present, the preprocessing step will be applied to all the datasets.","title":"Preprocessing"},{"location":"config-reference/preprocessing/#preprocessing-steps","text":"There are some preprocessing steps already available in Revelio.","title":"Preprocessing steps"},{"location":"config-reference/preprocessing/#resize","text":"The resize step resizes the images to the specified size. The resize step has the following arguments: width (required int): the width of the resized images. height (required int): the height of the resized images. algorithm (optional string, default: cubic ): the algorithm to use to resize the images. The available algorithms are the same as the ones available in OpenCV: nearest : nearest-neighbor interpolation linear : bilinear interpolation cubic : bicubic interpolation (default) area : resampling using pixel area relation lanczos4 : Lanczos interpolation over 8x8 neighborhood keep_aspect_ratio (optional bool, default: True ): whether to keep the aspect ratio of the images when resizing them. fill_mode (optional string, default: constant ): the strategy used for filling in newly created pixels, which can appear when using the keep_aspect_ratio option. The available strategies are the same as the ones available in OpenCV: constant : the pixels are filled with black (default, e.g. 000000|abcdefgh|000000 ) reflect : the pixels are filled with the reflection of the image (e.g. gfedcb|abcdefgh|gfedcba ) replicate : the pixels are filled with the last pixel of the image (e.g. aaaaaa|abcdefgh|hhhhhhh ) wrap : the pixels are filled with the wrap of the image (e.g. cdefgh|abcdefgh|abcdefg )","title":"Resize"},{"location":"config-reference/preprocessing/#to-float","text":"The to_float step converts the images to floating point images in the range [0, 1] . The to_float step has no arguments.","title":"To float"},{"location":"config-reference/preprocessing/#normalize","text":"The normalize step normalizes the images using the specified mean and standard deviation. The normalize step has the following arguments: mean (optional list of floats, default: None ): the mean to use to normalize the images, one element per BGR channel. std (optional list of floats, default: None ): the standard deviation to use to normalize the images, one element per BGR channel. preset (optional string, default: None ): the preset to use to normalize the images. The available presets are: imagenet : the mean and standard deviation used to normalize the images in the ImageNet dataset. Either preset or both mean and std must be specified. Warning Watch out for the order of the channels in the mean and standard deviation values. The order is BGR (i.e. OpenCV convention), not RGB.","title":"Normalize"},{"location":"config-reference/seed/","text":"Seed Note This section is optional. The seed is an integer used to initialize the random number generator. This is useful to ensure reproducibility of the experiments. seed : 42 ...","title":"Seed"},{"location":"config-reference/seed/#seed","text":"Note This section is optional. The seed is an integer used to initialize the random number generator. This is useful to ensure reproducibility of the experiments. seed : 42 ...","title":"Seed"},{"location":"reference/SUMMARY/","text":"revelio augmentation grayscale jpeg print_scan resize step cli config config model augmentation dataset experiment face_detection feature_extraction preprocessing utils dataset dataset dataset_factory descriptors_list element loaders loader loaders face_detection detector dlib_detector mtcnn_detector opencv_detector feature_extraction extractor fourier_extractor prnu_extractor same_size_extractor stationary_wavelet_packets wavelets_extractor model metrics accuracy bpcer_at_apcer eer metric tnr tpr utils model nn alexnet callbacks callback early_stopping model_checkpoint tensorboard feature_inception_resnet inception_resnet losses loss losses mobilenet neuralnet optimizers optimizer optimizers resnet squeezenet utils vgg vision_transformer random_guess preprocessing color_space normalize resize select_channel step to_float registry registry utils caching cacher npz_cacher zstd_cacher files iterators logging random rounding","title":"SUMMARY"},{"location":"reference/revelio/","text":"","title":"revelio"},{"location":"reference/revelio/cli/","text":"","title":"cli"},{"location":"reference/revelio/augmentation/","text":"","title":"augmentation"},{"location":"reference/revelio/augmentation/grayscale/","text":"Grayscale Bases: AugmentationStep Applies a grayscale filter to the image. Source code in revelio/augmentation/grayscale.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Grayscale ( AugmentationStep ): # pragma: no cover \"\"\" Applies a grayscale filter to the image. \"\"\" def __init__ ( self , * , stack : int = 1 , ** kwargs : Any ): \"\"\" Applies a grayscale filter to the image. Args: stack: The number of times to stack the grayscale image. \"\"\" super () . __init__ ( ** kwargs ) if stack < 1 : raise ValueError ( \"Stack must be positive\" ) self . _stack = stack def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: grayscale = cv . cvtColor ( image , cv . COLOR_RGB2GRAY ) if self . _stack > 1 : grayscale = np . stack ([ grayscale ] * self . _stack , axis =- 1 ) return grayscale , landmarks __init__ ( * , stack : int = 1 , ** kwargs : Any ) Applies a grayscale filter to the image. Parameters: Name Type Description Default stack int The number of times to stack the grayscale image. 1 Source code in revelio/augmentation/grayscale.py 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , * , stack : int = 1 , ** kwargs : Any ): \"\"\" Applies a grayscale filter to the image. Args: stack: The number of times to stack the grayscale image. \"\"\" super () . __init__ ( ** kwargs ) if stack < 1 : raise ValueError ( \"Stack must be positive\" ) self . _stack = stack","title":"grayscale"},{"location":"reference/revelio/augmentation/grayscale/#revelio.augmentation.grayscale.Grayscale","text":"Bases: AugmentationStep Applies a grayscale filter to the image. Source code in revelio/augmentation/grayscale.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Grayscale ( AugmentationStep ): # pragma: no cover \"\"\" Applies a grayscale filter to the image. \"\"\" def __init__ ( self , * , stack : int = 1 , ** kwargs : Any ): \"\"\" Applies a grayscale filter to the image. Args: stack: The number of times to stack the grayscale image. \"\"\" super () . __init__ ( ** kwargs ) if stack < 1 : raise ValueError ( \"Stack must be positive\" ) self . _stack = stack def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: grayscale = cv . cvtColor ( image , cv . COLOR_RGB2GRAY ) if self . _stack > 1 : grayscale = np . stack ([ grayscale ] * self . _stack , axis =- 1 ) return grayscale , landmarks","title":"Grayscale"},{"location":"reference/revelio/augmentation/grayscale/#revelio.augmentation.grayscale.Grayscale.__init__","text":"Applies a grayscale filter to the image. Parameters: Name Type Description Default stack int The number of times to stack the grayscale image. 1 Source code in revelio/augmentation/grayscale.py 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , * , stack : int = 1 , ** kwargs : Any ): \"\"\" Applies a grayscale filter to the image. Args: stack: The number of times to stack the grayscale image. \"\"\" super () . __init__ ( ** kwargs ) if stack < 1 : raise ValueError ( \"Stack must be positive\" ) self . _stack = stack","title":"__init__()"},{"location":"reference/revelio/augmentation/jpeg/","text":"JPEGCompression Bases: AugmentationStep Applies either JPEG or JPEG2000 compression artifacts to the image. This augmentation can be used to test the robustness of a model to JPEG compression. There are several modes of operation: - If jpeg_quality is specified, the image will be compressed to the specified quality using the JPEG encoding. If more qualities are specified, one is chosen randomly. - if jpeg2000_quality is specified, the image will be compressed to the specified quality using the JPEG2000 encoding. If more qualities are specified, one is chosen randomly. - If max_bytes is specified, the image will be compressed to the highest quality that produces an image under the specified size. Source code in revelio/augmentation/jpeg.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class JPEGCompression ( AugmentationStep ): # pragma: no cover \"\"\" Applies either JPEG or JPEG2000 compression artifacts to the image. This augmentation can be used to test the robustness of a model to JPEG compression. There are several modes of operation: - If `jpeg_quality` is specified, the image will be compressed to the specified quality using the JPEG encoding. If more qualities are specified, one is chosen randomly. - if `jpeg2000_quality` is specified, the image will be compressed to the specified quality using the JPEG2000 encoding. If more qualities are specified, one is chosen randomly. - If `max_bytes` is specified, the image will be compressed to the highest quality that produces an image under the specified size. \"\"\" def __init__ ( self , * , jpeg_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , jpeg2000_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , max_bytes : Optional [ int ] = None , jpeg2000_probability : float = 0.0 , ** kwargs : Any , ): \"\"\" Applies JPEG compression artifacts to the image. Args: jpeg_quality: The quality to compress the image to, using the JPEG encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. jpeg2000_quality: The quality to compress the image to, using the JPEG2000 encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. max_bytes: The maximum number of bytes the image can be compressed to. The image will be compressed to the highest quality that produces an image under the specified size. jpeg2000_probability: The probability that the image will be compressed using the JPEG2000 encoding instead of the JPEG encoding. \"\"\" super () . __init__ ( ** kwargs ) if jpeg_quality is not None or jpeg2000_quality is not None : if jpeg_quality is not None : self . _validate_quality ( jpeg_quality , max_quality = 100 ) if jpeg2000_quality is not None : self . _validate_quality ( jpeg2000_quality , max_quality = 1000 ) elif max_bytes is not None : if max_bytes < 0 : raise ValueError ( \"Max bytes must be positive\" ) else : raise ValueError ( \"Either JPEG/JPEG2000 quality or max bytes must be specified\" ) self . _jpeg_quality = jpeg_quality self . _jpeg2000_quality = jpeg2000_quality self . _jpeg2000_probability = jpeg2000_probability self . _max_bytes = max_bytes def _validate_quality ( self , quality : int | list [ int ] | dict [ str , int ], max_quality : int ) -> None : if isinstance ( quality , int ) and ( quality < 0 or quality > max_quality ): raise ValueError ( f \"Quality must be between 0 and { max_quality } \" ) elif isinstance ( quality , list ): for q in quality : if q < 0 or q > max_quality : raise ValueError ( f \"Quality must be between 0 and { max_quality } \" ) elif isinstance ( quality , dict ): if quality . keys () != { \"min\" , \"max\" }: raise ValueError ( \"'min' and 'max' are required\" ) if quality [ \"min\" ] < 0 or quality [ \"min\" ] > max_quality : raise ValueError ( f \"Min quality must be between 0 and { max_quality } \" ) if quality [ \"max\" ] < 0 or quality [ \"max\" ] > max_quality : raise ValueError ( f \"Max quality must be between 0 and { max_quality } \" ) if quality [ \"min\" ] > quality [ \"max\" ]: raise ValueError ( \"Min quality must be less than max quality\" ) else : raise TypeError ( \"Quality must be an integer, list or dictionary\" ) def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: if random . random () < self . _jpeg2000_probability : algorithm = self . _jpeg2000_quality encoder = _encode_jpeg2000 max_allowed_quality = 1000 else : algorithm = self . _jpeg_quality encoder = _encode_jpeg max_allowed_quality = 100 if algorithm is not None : if isinstance ( algorithm , int ): chosen_quality = algorithm elif isinstance ( algorithm , list ): chosen_quality = random . choice ( algorithm ) elif isinstance ( algorithm , dict ): chosen_quality = random . randint ( algorithm [ \"min\" ], algorithm [ \"max\" ]) else : raise TypeError ( \"Quality must be an integer, list or dictionary\" ) return self . _convert_to_quality ( image , chosen_quality , encoder ), landmarks elif self . _max_bytes is not None : return ( self . _convert_to_max_bytes ( image , self . _max_bytes , max_allowed_quality , encoder ), landmarks , ) else : raise ValueError ( \"Either quality or max bytes must be specified\" ) def _convert_to_quality ( self , image : Image , quality : int , encoder : Callable [[ Image , int ], Optional [ bytes ]], ) -> Image : encoded = _try_encoder ( encoder , image , quality ) decoded = _try_decoder ( encoded ) return decoded def _convert_to_max_bytes ( self , image : Image , max_bytes : int , max_allowed_quality : int , encoder : Callable [[ Image , int ], Optional [ bytes ]], ) -> Image : # Find the highest quality that produces an image under the desired size min_quality , max_quality = 0 , max_allowed_quality while max_quality - min_quality > 1 : quality = ( min_quality + max_quality ) // 2 encoded = _try_encoder ( encoder , image , quality ) if len ( encoded ) > max_bytes : max_quality = quality else : min_quality = quality encoded = _try_encoder ( encoder , image , min_quality ) decoded = _try_decoder ( encoded ) return decoded __init__ ( * , jpeg_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , jpeg2000_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , max_bytes : Optional [ int ] = None , jpeg2000_probability : float = 0.0 , ** kwargs : Any ) Applies JPEG compression artifacts to the image. Parameters: Name Type Description Default jpeg_quality Optional [ int | list [ int ] | dict [ str , int ]] The quality to compress the image to, using the JPEG encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the min and max keys is provided, the quality is chosen randomly between the specified quality range. None jpeg2000_quality Optional [ int | list [ int ] | dict [ str , int ]] The quality to compress the image to, using the JPEG2000 encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the min and max keys is provided, the quality is chosen randomly between the specified quality range. None max_bytes Optional [ int ] The maximum number of bytes the image can be compressed to. The image will be compressed to the highest quality that produces an image under the specified size. None jpeg2000_probability float The probability that the image will be compressed using the JPEG2000 encoding instead of the JPEG encoding. 0.0 Source code in revelio/augmentation/jpeg.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , * , jpeg_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , jpeg2000_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , max_bytes : Optional [ int ] = None , jpeg2000_probability : float = 0.0 , ** kwargs : Any , ): \"\"\" Applies JPEG compression artifacts to the image. Args: jpeg_quality: The quality to compress the image to, using the JPEG encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. jpeg2000_quality: The quality to compress the image to, using the JPEG2000 encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. max_bytes: The maximum number of bytes the image can be compressed to. The image will be compressed to the highest quality that produces an image under the specified size. jpeg2000_probability: The probability that the image will be compressed using the JPEG2000 encoding instead of the JPEG encoding. \"\"\" super () . __init__ ( ** kwargs ) if jpeg_quality is not None or jpeg2000_quality is not None : if jpeg_quality is not None : self . _validate_quality ( jpeg_quality , max_quality = 100 ) if jpeg2000_quality is not None : self . _validate_quality ( jpeg2000_quality , max_quality = 1000 ) elif max_bytes is not None : if max_bytes < 0 : raise ValueError ( \"Max bytes must be positive\" ) else : raise ValueError ( \"Either JPEG/JPEG2000 quality or max bytes must be specified\" ) self . _jpeg_quality = jpeg_quality self . _jpeg2000_quality = jpeg2000_quality self . _jpeg2000_probability = jpeg2000_probability self . _max_bytes = max_bytes","title":"jpeg"},{"location":"reference/revelio/augmentation/jpeg/#revelio.augmentation.jpeg.JPEGCompression","text":"Bases: AugmentationStep Applies either JPEG or JPEG2000 compression artifacts to the image. This augmentation can be used to test the robustness of a model to JPEG compression. There are several modes of operation: - If jpeg_quality is specified, the image will be compressed to the specified quality using the JPEG encoding. If more qualities are specified, one is chosen randomly. - if jpeg2000_quality is specified, the image will be compressed to the specified quality using the JPEG2000 encoding. If more qualities are specified, one is chosen randomly. - If max_bytes is specified, the image will be compressed to the highest quality that produces an image under the specified size. Source code in revelio/augmentation/jpeg.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class JPEGCompression ( AugmentationStep ): # pragma: no cover \"\"\" Applies either JPEG or JPEG2000 compression artifacts to the image. This augmentation can be used to test the robustness of a model to JPEG compression. There are several modes of operation: - If `jpeg_quality` is specified, the image will be compressed to the specified quality using the JPEG encoding. If more qualities are specified, one is chosen randomly. - if `jpeg2000_quality` is specified, the image will be compressed to the specified quality using the JPEG2000 encoding. If more qualities are specified, one is chosen randomly. - If `max_bytes` is specified, the image will be compressed to the highest quality that produces an image under the specified size. \"\"\" def __init__ ( self , * , jpeg_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , jpeg2000_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , max_bytes : Optional [ int ] = None , jpeg2000_probability : float = 0.0 , ** kwargs : Any , ): \"\"\" Applies JPEG compression artifacts to the image. Args: jpeg_quality: The quality to compress the image to, using the JPEG encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. jpeg2000_quality: The quality to compress the image to, using the JPEG2000 encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. max_bytes: The maximum number of bytes the image can be compressed to. The image will be compressed to the highest quality that produces an image under the specified size. jpeg2000_probability: The probability that the image will be compressed using the JPEG2000 encoding instead of the JPEG encoding. \"\"\" super () . __init__ ( ** kwargs ) if jpeg_quality is not None or jpeg2000_quality is not None : if jpeg_quality is not None : self . _validate_quality ( jpeg_quality , max_quality = 100 ) if jpeg2000_quality is not None : self . _validate_quality ( jpeg2000_quality , max_quality = 1000 ) elif max_bytes is not None : if max_bytes < 0 : raise ValueError ( \"Max bytes must be positive\" ) else : raise ValueError ( \"Either JPEG/JPEG2000 quality or max bytes must be specified\" ) self . _jpeg_quality = jpeg_quality self . _jpeg2000_quality = jpeg2000_quality self . _jpeg2000_probability = jpeg2000_probability self . _max_bytes = max_bytes def _validate_quality ( self , quality : int | list [ int ] | dict [ str , int ], max_quality : int ) -> None : if isinstance ( quality , int ) and ( quality < 0 or quality > max_quality ): raise ValueError ( f \"Quality must be between 0 and { max_quality } \" ) elif isinstance ( quality , list ): for q in quality : if q < 0 or q > max_quality : raise ValueError ( f \"Quality must be between 0 and { max_quality } \" ) elif isinstance ( quality , dict ): if quality . keys () != { \"min\" , \"max\" }: raise ValueError ( \"'min' and 'max' are required\" ) if quality [ \"min\" ] < 0 or quality [ \"min\" ] > max_quality : raise ValueError ( f \"Min quality must be between 0 and { max_quality } \" ) if quality [ \"max\" ] < 0 or quality [ \"max\" ] > max_quality : raise ValueError ( f \"Max quality must be between 0 and { max_quality } \" ) if quality [ \"min\" ] > quality [ \"max\" ]: raise ValueError ( \"Min quality must be less than max quality\" ) else : raise TypeError ( \"Quality must be an integer, list or dictionary\" ) def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: if random . random () < self . _jpeg2000_probability : algorithm = self . _jpeg2000_quality encoder = _encode_jpeg2000 max_allowed_quality = 1000 else : algorithm = self . _jpeg_quality encoder = _encode_jpeg max_allowed_quality = 100 if algorithm is not None : if isinstance ( algorithm , int ): chosen_quality = algorithm elif isinstance ( algorithm , list ): chosen_quality = random . choice ( algorithm ) elif isinstance ( algorithm , dict ): chosen_quality = random . randint ( algorithm [ \"min\" ], algorithm [ \"max\" ]) else : raise TypeError ( \"Quality must be an integer, list or dictionary\" ) return self . _convert_to_quality ( image , chosen_quality , encoder ), landmarks elif self . _max_bytes is not None : return ( self . _convert_to_max_bytes ( image , self . _max_bytes , max_allowed_quality , encoder ), landmarks , ) else : raise ValueError ( \"Either quality or max bytes must be specified\" ) def _convert_to_quality ( self , image : Image , quality : int , encoder : Callable [[ Image , int ], Optional [ bytes ]], ) -> Image : encoded = _try_encoder ( encoder , image , quality ) decoded = _try_decoder ( encoded ) return decoded def _convert_to_max_bytes ( self , image : Image , max_bytes : int , max_allowed_quality : int , encoder : Callable [[ Image , int ], Optional [ bytes ]], ) -> Image : # Find the highest quality that produces an image under the desired size min_quality , max_quality = 0 , max_allowed_quality while max_quality - min_quality > 1 : quality = ( min_quality + max_quality ) // 2 encoded = _try_encoder ( encoder , image , quality ) if len ( encoded ) > max_bytes : max_quality = quality else : min_quality = quality encoded = _try_encoder ( encoder , image , min_quality ) decoded = _try_decoder ( encoded ) return decoded","title":"JPEGCompression"},{"location":"reference/revelio/augmentation/jpeg/#revelio.augmentation.jpeg.JPEGCompression.__init__","text":"Applies JPEG compression artifacts to the image. Parameters: Name Type Description Default jpeg_quality Optional [ int | list [ int ] | dict [ str , int ]] The quality to compress the image to, using the JPEG encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the min and max keys is provided, the quality is chosen randomly between the specified quality range. None jpeg2000_quality Optional [ int | list [ int ] | dict [ str , int ]] The quality to compress the image to, using the JPEG2000 encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the min and max keys is provided, the quality is chosen randomly between the specified quality range. None max_bytes Optional [ int ] The maximum number of bytes the image can be compressed to. The image will be compressed to the highest quality that produces an image under the specified size. None jpeg2000_probability float The probability that the image will be compressed using the JPEG2000 encoding instead of the JPEG encoding. 0.0 Source code in revelio/augmentation/jpeg.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , * , jpeg_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , jpeg2000_quality : Optional [ int | list [ int ] | dict [ str , int ]] = None , max_bytes : Optional [ int ] = None , jpeg2000_probability : float = 0.0 , ** kwargs : Any , ): \"\"\" Applies JPEG compression artifacts to the image. Args: jpeg_quality: The quality to compress the image to, using the JPEG encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. jpeg2000_quality: The quality to compress the image to, using the JPEG2000 encoding. If a list is provided, one quality is chosen randomly. If a dictionary with the `min` and `max` keys is provided, the quality is chosen randomly between the specified quality range. max_bytes: The maximum number of bytes the image can be compressed to. The image will be compressed to the highest quality that produces an image under the specified size. jpeg2000_probability: The probability that the image will be compressed using the JPEG2000 encoding instead of the JPEG encoding. \"\"\" super () . __init__ ( ** kwargs ) if jpeg_quality is not None or jpeg2000_quality is not None : if jpeg_quality is not None : self . _validate_quality ( jpeg_quality , max_quality = 100 ) if jpeg2000_quality is not None : self . _validate_quality ( jpeg2000_quality , max_quality = 1000 ) elif max_bytes is not None : if max_bytes < 0 : raise ValueError ( \"Max bytes must be positive\" ) else : raise ValueError ( \"Either JPEG/JPEG2000 quality or max bytes must be specified\" ) self . _jpeg_quality = jpeg_quality self . _jpeg2000_quality = jpeg2000_quality self . _jpeg2000_probability = jpeg2000_probability self . _max_bytes = max_bytes","title":"__init__()"},{"location":"reference/revelio/augmentation/print_scan/","text":"PrintScan Bases: AugmentationStep Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Source code in revelio/augmentation/print_scan.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class PrintScan ( AugmentationStep ): # pragma: no cover \"\"\" Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. \"\"\" def __init__ ( self , * , apply_edge_noise : bool = True , apply_dark_area_noise : bool = True , color_correction_alpha : float = 8.3 , color_correction_beta_k : float = 20.0 , color_correction_beta_x : float = 35.0 , color_correction_gamma : float = 0.6 , cutoff_noise_threshold : float = 32.0 , max_n2_noise_value : float = 20.0 , ** kwargs : Any , ): \"\"\" Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Args: apply_edge_noise: Whether to apply a simulated edge noise. apply_dark_area_noise: Whether to apply a simulated dark area noise. color_correction_alpha: Alpha parameter of the gamma correction. color_correction_beta_k: Beta_k parameter of the gamma correction. color_correction_beta_x: Beta_x parameter of the gamma correction. color_correction_gamma: Gamma parameter of the gamma correction. cutoff_noise_threshold: Threshold for the dark area noise. max_n2_noise_value: Maximum value for the dark area noise. \"\"\" super () . __init__ ( ** kwargs ) self . _apply_edge_noise = apply_edge_noise self . _apply_dark_area_noise = apply_dark_area_noise self . _color_correction_alpha = color_correction_alpha self . _color_correction_beta_k = color_correction_beta_k self . _color_correction_beta_x = color_correction_beta_x self . _color_correction_gamma = color_correction_gamma self . _cutoff_noise_threshold = cutoff_noise_threshold self . _max_n2_noise_value = max_n2_noise_value self . _precompute_gradient_constants () self . _precompute_gamma_lut () self . _precompute_dark_area_lut () def _precompute_gradient_constants ( self ) -> None : self . _gradient_x_weight = 0.4 self . _gradient_y_weight = 1.0 - self . _gradient_x_weight self . _max_gradient_magnitude = ( self . _gradient_x_weight * 255 * 255 + self . _gradient_y_weight * 255 * 255 ) def _precompute_gamma_lut ( self ) -> None : lut = np . arange ( 256 , dtype = np . int16 ) lut = np . round ( self . _color_correction_alpha * ( np . maximum ( lut - self . _color_correction_beta_x , 0 ) ** self . _color_correction_gamma ) + self . _color_correction_beta_k ) self . _gamma_lut = np . clip ( lut , 0 , 255 ) . astype ( np . uint8 ) def _precompute_dark_area_lut ( self ) -> None : lut = np . arange ( 256 , dtype = np . float64 ) lut = 1 / ( 1 + np . exp ( lut - self . _cutoff_noise_threshold )) lut = np . round ( lut * self . _max_n2_noise_value ) self . _dark_area_lut = np . clip ( lut , 0 , 255 ) . astype ( np . uint8 ) def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: image = self . _psf ( image ) if self . _apply_edge_noise : image = self . _edge_noise ( image ) image = self . _gamma_correction ( image ) if self . _apply_dark_area_noise : image = self . _dark_area_noise ( image ) sigma = _get_sigma ( image ) image = cv . GaussianBlur ( image , ksize = None , sigmaX = sigma , sigmaY = sigma ) return image , landmarks def _compute_gradient_magnitude ( self , image : Image ) -> np . ndarray : gradient_x = cv . sepFilter2D ( image , - 1 , _PREWITT_KERNEL_SEP2 , _PREWITT_KERNEL_SEP1 ) gradient_y = cv . sepFilter2D ( image , - 1 , _PREWITT_KERNEL_SEP1 , _PREWITT_KERNEL_SEP2 ) return ( # type: ignore self . _gradient_x_weight * gradient_x * gradient_x + self . _gradient_y_weight * gradient_y * gradient_y ) / self . _max_gradient_magnitude def _edge_noise ( self , image : Image ) -> Image : magnitude = self . _compute_gradient_magnitude ( image ) noise = ( _gaussians_in_range ( image . shape , mean = 128 , numbers_range = ( 0 , 255 )) * magnitude ) summed = image . astype ( np . int16 ) + noise return np . clip ( np . round ( summed ), 0 , 255 ) . astype ( np . uint8 ) # type: ignore def _gamma_correction ( self , image : Image ) -> Image : return cv . LUT ( image , self . _gamma_lut ) # type: ignore def _dark_area_noise ( self , image : Image ) -> Image : noise = np . round ( cv . LUT ( image , self . _dark_area_lut ) . astype ( np . float32 ) * np . random . random ( image . shape ) ) . astype ( np . uint8 ) summed = image . astype ( np . int16 ) + noise return np . clip ( np . round ( summed ), 0 , 255 ) . astype ( np . uint8 ) # type: ignore def _psf ( self , image : Image ) -> Image : sigma = _get_sigma ( image ) return np . round ( # type: ignore cv . GaussianBlur ( image , ksize = None , sigmaX = sigma , sigmaY = sigma ) ) . astype ( np . uint8 ) __init__ ( * , apply_edge_noise : bool = True , apply_dark_area_noise : bool = True , color_correction_alpha : float = 8.3 , color_correction_beta_k : float = 20.0 , color_correction_beta_x : float = 35.0 , color_correction_gamma : float = 0.6 , cutoff_noise_threshold : float = 32.0 , max_n2_noise_value : float = 20.0 , ** kwargs : Any ) Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Parameters: Name Type Description Default apply_edge_noise bool Whether to apply a simulated edge noise. True apply_dark_area_noise bool Whether to apply a simulated dark area noise. True color_correction_alpha float Alpha parameter of the gamma correction. 8.3 color_correction_beta_k float Beta_k parameter of the gamma correction. 20.0 color_correction_beta_x float Beta_x parameter of the gamma correction. 35.0 color_correction_gamma float Gamma parameter of the gamma correction. 0.6 cutoff_noise_threshold float Threshold for the dark area noise. 32.0 max_n2_noise_value float Maximum value for the dark area noise. 20.0 Source code in revelio/augmentation/print_scan.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , * , apply_edge_noise : bool = True , apply_dark_area_noise : bool = True , color_correction_alpha : float = 8.3 , color_correction_beta_k : float = 20.0 , color_correction_beta_x : float = 35.0 , color_correction_gamma : float = 0.6 , cutoff_noise_threshold : float = 32.0 , max_n2_noise_value : float = 20.0 , ** kwargs : Any , ): \"\"\" Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Args: apply_edge_noise: Whether to apply a simulated edge noise. apply_dark_area_noise: Whether to apply a simulated dark area noise. color_correction_alpha: Alpha parameter of the gamma correction. color_correction_beta_k: Beta_k parameter of the gamma correction. color_correction_beta_x: Beta_x parameter of the gamma correction. color_correction_gamma: Gamma parameter of the gamma correction. cutoff_noise_threshold: Threshold for the dark area noise. max_n2_noise_value: Maximum value for the dark area noise. \"\"\" super () . __init__ ( ** kwargs ) self . _apply_edge_noise = apply_edge_noise self . _apply_dark_area_noise = apply_dark_area_noise self . _color_correction_alpha = color_correction_alpha self . _color_correction_beta_k = color_correction_beta_k self . _color_correction_beta_x = color_correction_beta_x self . _color_correction_gamma = color_correction_gamma self . _cutoff_noise_threshold = cutoff_noise_threshold self . _max_n2_noise_value = max_n2_noise_value self . _precompute_gradient_constants () self . _precompute_gamma_lut () self . _precompute_dark_area_lut ()","title":"print_scan"},{"location":"reference/revelio/augmentation/print_scan/#revelio.augmentation.print_scan.PrintScan","text":"Bases: AugmentationStep Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Source code in revelio/augmentation/print_scan.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class PrintScan ( AugmentationStep ): # pragma: no cover \"\"\" Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. \"\"\" def __init__ ( self , * , apply_edge_noise : bool = True , apply_dark_area_noise : bool = True , color_correction_alpha : float = 8.3 , color_correction_beta_k : float = 20.0 , color_correction_beta_x : float = 35.0 , color_correction_gamma : float = 0.6 , cutoff_noise_threshold : float = 32.0 , max_n2_noise_value : float = 20.0 , ** kwargs : Any , ): \"\"\" Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Args: apply_edge_noise: Whether to apply a simulated edge noise. apply_dark_area_noise: Whether to apply a simulated dark area noise. color_correction_alpha: Alpha parameter of the gamma correction. color_correction_beta_k: Beta_k parameter of the gamma correction. color_correction_beta_x: Beta_x parameter of the gamma correction. color_correction_gamma: Gamma parameter of the gamma correction. cutoff_noise_threshold: Threshold for the dark area noise. max_n2_noise_value: Maximum value for the dark area noise. \"\"\" super () . __init__ ( ** kwargs ) self . _apply_edge_noise = apply_edge_noise self . _apply_dark_area_noise = apply_dark_area_noise self . _color_correction_alpha = color_correction_alpha self . _color_correction_beta_k = color_correction_beta_k self . _color_correction_beta_x = color_correction_beta_x self . _color_correction_gamma = color_correction_gamma self . _cutoff_noise_threshold = cutoff_noise_threshold self . _max_n2_noise_value = max_n2_noise_value self . _precompute_gradient_constants () self . _precompute_gamma_lut () self . _precompute_dark_area_lut () def _precompute_gradient_constants ( self ) -> None : self . _gradient_x_weight = 0.4 self . _gradient_y_weight = 1.0 - self . _gradient_x_weight self . _max_gradient_magnitude = ( self . _gradient_x_weight * 255 * 255 + self . _gradient_y_weight * 255 * 255 ) def _precompute_gamma_lut ( self ) -> None : lut = np . arange ( 256 , dtype = np . int16 ) lut = np . round ( self . _color_correction_alpha * ( np . maximum ( lut - self . _color_correction_beta_x , 0 ) ** self . _color_correction_gamma ) + self . _color_correction_beta_k ) self . _gamma_lut = np . clip ( lut , 0 , 255 ) . astype ( np . uint8 ) def _precompute_dark_area_lut ( self ) -> None : lut = np . arange ( 256 , dtype = np . float64 ) lut = 1 / ( 1 + np . exp ( lut - self . _cutoff_noise_threshold )) lut = np . round ( lut * self . _max_n2_noise_value ) self . _dark_area_lut = np . clip ( lut , 0 , 255 ) . astype ( np . uint8 ) def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: image = self . _psf ( image ) if self . _apply_edge_noise : image = self . _edge_noise ( image ) image = self . _gamma_correction ( image ) if self . _apply_dark_area_noise : image = self . _dark_area_noise ( image ) sigma = _get_sigma ( image ) image = cv . GaussianBlur ( image , ksize = None , sigmaX = sigma , sigmaY = sigma ) return image , landmarks def _compute_gradient_magnitude ( self , image : Image ) -> np . ndarray : gradient_x = cv . sepFilter2D ( image , - 1 , _PREWITT_KERNEL_SEP2 , _PREWITT_KERNEL_SEP1 ) gradient_y = cv . sepFilter2D ( image , - 1 , _PREWITT_KERNEL_SEP1 , _PREWITT_KERNEL_SEP2 ) return ( # type: ignore self . _gradient_x_weight * gradient_x * gradient_x + self . _gradient_y_weight * gradient_y * gradient_y ) / self . _max_gradient_magnitude def _edge_noise ( self , image : Image ) -> Image : magnitude = self . _compute_gradient_magnitude ( image ) noise = ( _gaussians_in_range ( image . shape , mean = 128 , numbers_range = ( 0 , 255 )) * magnitude ) summed = image . astype ( np . int16 ) + noise return np . clip ( np . round ( summed ), 0 , 255 ) . astype ( np . uint8 ) # type: ignore def _gamma_correction ( self , image : Image ) -> Image : return cv . LUT ( image , self . _gamma_lut ) # type: ignore def _dark_area_noise ( self , image : Image ) -> Image : noise = np . round ( cv . LUT ( image , self . _dark_area_lut ) . astype ( np . float32 ) * np . random . random ( image . shape ) ) . astype ( np . uint8 ) summed = image . astype ( np . int16 ) + noise return np . clip ( np . round ( summed ), 0 , 255 ) . astype ( np . uint8 ) # type: ignore def _psf ( self , image : Image ) -> Image : sigma = _get_sigma ( image ) return np . round ( # type: ignore cv . GaussianBlur ( image , ksize = None , sigmaX = sigma , sigmaY = sigma ) ) . astype ( np . uint8 )","title":"PrintScan"},{"location":"reference/revelio/augmentation/print_scan/#revelio.augmentation.print_scan.PrintScan.__init__","text":"Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Parameters: Name Type Description Default apply_edge_noise bool Whether to apply a simulated edge noise. True apply_dark_area_noise bool Whether to apply a simulated dark area noise. True color_correction_alpha float Alpha parameter of the gamma correction. 8.3 color_correction_beta_k float Beta_k parameter of the gamma correction. 20.0 color_correction_beta_x float Beta_x parameter of the gamma correction. 35.0 color_correction_gamma float Gamma parameter of the gamma correction. 0.6 cutoff_noise_threshold float Threshold for the dark area noise. 32.0 max_n2_noise_value float Maximum value for the dark area noise. 20.0 Source code in revelio/augmentation/print_scan.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , * , apply_edge_noise : bool = True , apply_dark_area_noise : bool = True , color_correction_alpha : float = 8.3 , color_correction_beta_k : float = 20.0 , color_correction_beta_x : float = 35.0 , color_correction_gamma : float = 0.6 , cutoff_noise_threshold : float = 32.0 , max_n2_noise_value : float = 20.0 , ** kwargs : Any , ): \"\"\" Apply a simulated print and scan process to the image, based on \"Face morphing detection in the presence of printing/scanning and heterogeneous image sources\" by Ferrara et al. Args: apply_edge_noise: Whether to apply a simulated edge noise. apply_dark_area_noise: Whether to apply a simulated dark area noise. color_correction_alpha: Alpha parameter of the gamma correction. color_correction_beta_k: Beta_k parameter of the gamma correction. color_correction_beta_x: Beta_x parameter of the gamma correction. color_correction_gamma: Gamma parameter of the gamma correction. cutoff_noise_threshold: Threshold for the dark area noise. max_n2_noise_value: Maximum value for the dark area noise. \"\"\" super () . __init__ ( ** kwargs ) self . _apply_edge_noise = apply_edge_noise self . _apply_dark_area_noise = apply_dark_area_noise self . _color_correction_alpha = color_correction_alpha self . _color_correction_beta_k = color_correction_beta_k self . _color_correction_beta_x = color_correction_beta_x self . _color_correction_gamma = color_correction_gamma self . _cutoff_noise_threshold = cutoff_noise_threshold self . _max_n2_noise_value = max_n2_noise_value self . _precompute_gradient_constants () self . _precompute_gamma_lut () self . _precompute_dark_area_lut ()","title":"__init__()"},{"location":"reference/revelio/augmentation/resize/","text":"Resize Bases: AugmentationStep Applies a resize filter to the image. Both width and height can be either a fixed value or a range. The aspect ratio can be preserved or not, with a given probability. The interpolation algorithm and fill mode are chosen randomly from the provided lists. Source code in revelio/augmentation/resize.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class Resize ( AugmentationStep ): # pragma: no cover \"\"\" Applies a resize filter to the image. Both width and height can be either a fixed value or a range. The aspect ratio can be preserved or not, with a given probability. The interpolation algorithm and fill mode are chosen randomly from the provided lists. \"\"\" def __init__ ( self , * , width : Optional [ int ] = None , height : Optional [ int ] = None , from_width : Optional [ int ] = None , to_width : Optional [ int ] = None , from_height : Optional [ int ] = None , to_height : Optional [ int ] = None , algorithms : Optional [ list [ str ]] = None , fill_modes : Optional [ list [ str ]] = None , keep_aspect_ratio_probability : float = 1.0 , ** kwargs : Any , ) -> None : \"\"\" Applies a resize filter to the image. Args: width: The fixed width of the image. height: The fixed height of the image. from_width: The minimum width of the image. to_width: The maximum width of the image. from_height: The minimum height of the image. to_height: The maximum height of the image. algorithms: The list of algorithms to choose from. fill_modes: The list of fill modes to choose from. keep_aspect_ratio_probability: The probability of keeping the aspect ratio. \"\"\" super () . __init__ ( ** kwargs ) self . _parse_sizes ( width , height , from_width , to_width , from_height , to_height ) self . _parse_algorithms ( algorithms ) self . _parse_fill_modes ( fill_modes ) self . _parse_keep_aspect_ratio_probability ( keep_aspect_ratio_probability ) def _parse_sizes ( self , width : Optional [ int ], height : Optional [ int ], from_width : Optional [ int ], to_width : Optional [ int ], from_height : Optional [ int ], to_height : Optional [ int ], ) -> None : if width is not None and height is not None : # We have both width and height _validate_positive ( \"Width\" , width ) _validate_positive ( \"Height\" , height ) elif width is not None and from_height is not None and to_height is not None : # We have only width, we expect a random height _validate_positive ( \"Width\" , width ) _validate_positive ( \"From height\" , from_height ) _validate_positive ( \"To height\" , to_height ) _validate_greater ( \"To height\" , \"From height\" , to_height , from_height ) elif height is not None and from_width is not None and to_width is not None : # We have only height, we expect a random width _validate_positive ( \"Height\" , height ) _validate_positive ( \"From width\" , from_width ) _validate_positive ( \"To width\" , to_width ) _validate_greater ( \"To width\" , \"From width\" , to_width , from_width ) else : raise ValueError ( \"You must specify both width or height, \" \"either by specifying a fixed value or a range\" ) self . _width = width self . _height = height self . _from_width = from_width self . _to_width = to_width self . _from_height = from_height self . _to_height = to_height def _parse_algorithms ( self , algorithms : Optional [ list [ str ]]) -> None : if algorithms is None : self . _algorithms = list ( _VALID_ALGORITHMS . keys ()) return for algorithm in algorithms : if algorithm not in _VALID_ALGORITHMS : raise ValueError ( f \"Invalid algorithm: { algorithm } \" ) self . _algorithms = algorithms def _parse_fill_modes ( self , fill_modes : Optional [ list [ str ]]) -> None : if fill_modes is None : self . _fill_modes = list ( _VALID_FILL_MODES . keys ()) return for fill_mode in fill_modes : if fill_mode not in _VALID_FILL_MODES : raise ValueError ( f \"Invalid fill mode: { fill_mode } \" ) self . _fill_modes = fill_modes def _parse_keep_aspect_ratio_probability ( self , keep_aspect_ratio_probability : float ) -> None : if keep_aspect_ratio_probability < 0 or keep_aspect_ratio_probability > 1 : raise ValueError ( \"Keep aspect ratio probability must be between 0 and 1\" ) self . _keep_aspect_ratio_probability = keep_aspect_ratio_probability def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: algorithm = random . choice ( self . _algorithms ) fill_mode = random . choice ( self . _fill_modes ) keep_aspect_ratio = random . random () < self . _keep_aspect_ratio_probability if self . _width is not None : width = self . _width elif self . _from_width is not None and self . _to_width is not None : width = random . randint ( self . _from_width , self . _to_width ) else : raise ValueError ( \"Invalid width\" ) if self . _height is not None : height = self . _height elif self . _from_height is not None and self . _to_height is not None : height = random . randint ( self . _from_height , self . _to_height ) else : raise ValueError ( \"Invalid height\" ) if not keep_aspect_ratio : scale_factors = ( width / image . shape [ 1 ], height / image . shape [ 0 ]) new_img = cv . resize ( image , ( height , width ), interpolation = _VALID_ALGORITHMS [ algorithm ], ) if landmarks is not None : new_landmarks = landmarks * np . array ( scale_factors ) else : new_landmarks = None else : new_size = ( height , width ) old_size = image . shape [: 2 ] scale_factor = min ( n / o for n , o in zip ( new_size , old_size )) rescaled = cv . resize ( image , None , fx = scale_factor , fy = scale_factor , interpolation = _VALID_ALGORITHMS [ algorithm ], ) top_bottom , left_right = tuple ( d - s for d , s in zip ( new_size , rescaled . shape [: 2 ]) ) top = top_bottom // 2 bottom = top_bottom - top left = left_right // 2 right = left_right - left new_img = cv . copyMakeBorder ( rescaled , top , bottom , left , right , _VALID_FILL_MODES [ fill_mode ], ( 0 , 0 , 0 ), ) if landmarks is not None : new_landmarks = landmarks * scale_factor + np . array ([ left , top ]) else : new_landmarks = None return new_img , new_landmarks __init__ ( * , width : Optional [ int ] = None , height : Optional [ int ] = None , from_width : Optional [ int ] = None , to_width : Optional [ int ] = None , from_height : Optional [ int ] = None , to_height : Optional [ int ] = None , algorithms : Optional [ list [ str ]] = None , fill_modes : Optional [ list [ str ]] = None , keep_aspect_ratio_probability : float = 1.0 , ** kwargs : Any ) -> None Applies a resize filter to the image. Parameters: Name Type Description Default width Optional [ int ] The fixed width of the image. None height Optional [ int ] The fixed height of the image. None from_width Optional [ int ] The minimum width of the image. None to_width Optional [ int ] The maximum width of the image. None from_height Optional [ int ] The minimum height of the image. None to_height Optional [ int ] The maximum height of the image. None algorithms Optional [ list [ str ]] The list of algorithms to choose from. None fill_modes Optional [ list [ str ]] The list of fill modes to choose from. None keep_aspect_ratio_probability float The probability of keeping the aspect ratio. 1.0 Source code in revelio/augmentation/resize.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , * , width : Optional [ int ] = None , height : Optional [ int ] = None , from_width : Optional [ int ] = None , to_width : Optional [ int ] = None , from_height : Optional [ int ] = None , to_height : Optional [ int ] = None , algorithms : Optional [ list [ str ]] = None , fill_modes : Optional [ list [ str ]] = None , keep_aspect_ratio_probability : float = 1.0 , ** kwargs : Any , ) -> None : \"\"\" Applies a resize filter to the image. Args: width: The fixed width of the image. height: The fixed height of the image. from_width: The minimum width of the image. to_width: The maximum width of the image. from_height: The minimum height of the image. to_height: The maximum height of the image. algorithms: The list of algorithms to choose from. fill_modes: The list of fill modes to choose from. keep_aspect_ratio_probability: The probability of keeping the aspect ratio. \"\"\" super () . __init__ ( ** kwargs ) self . _parse_sizes ( width , height , from_width , to_width , from_height , to_height ) self . _parse_algorithms ( algorithms ) self . _parse_fill_modes ( fill_modes ) self . _parse_keep_aspect_ratio_probability ( keep_aspect_ratio_probability )","title":"resize"},{"location":"reference/revelio/augmentation/resize/#revelio.augmentation.resize.Resize","text":"Bases: AugmentationStep Applies a resize filter to the image. Both width and height can be either a fixed value or a range. The aspect ratio can be preserved or not, with a given probability. The interpolation algorithm and fill mode are chosen randomly from the provided lists. Source code in revelio/augmentation/resize.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class Resize ( AugmentationStep ): # pragma: no cover \"\"\" Applies a resize filter to the image. Both width and height can be either a fixed value or a range. The aspect ratio can be preserved or not, with a given probability. The interpolation algorithm and fill mode are chosen randomly from the provided lists. \"\"\" def __init__ ( self , * , width : Optional [ int ] = None , height : Optional [ int ] = None , from_width : Optional [ int ] = None , to_width : Optional [ int ] = None , from_height : Optional [ int ] = None , to_height : Optional [ int ] = None , algorithms : Optional [ list [ str ]] = None , fill_modes : Optional [ list [ str ]] = None , keep_aspect_ratio_probability : float = 1.0 , ** kwargs : Any , ) -> None : \"\"\" Applies a resize filter to the image. Args: width: The fixed width of the image. height: The fixed height of the image. from_width: The minimum width of the image. to_width: The maximum width of the image. from_height: The minimum height of the image. to_height: The maximum height of the image. algorithms: The list of algorithms to choose from. fill_modes: The list of fill modes to choose from. keep_aspect_ratio_probability: The probability of keeping the aspect ratio. \"\"\" super () . __init__ ( ** kwargs ) self . _parse_sizes ( width , height , from_width , to_width , from_height , to_height ) self . _parse_algorithms ( algorithms ) self . _parse_fill_modes ( fill_modes ) self . _parse_keep_aspect_ratio_probability ( keep_aspect_ratio_probability ) def _parse_sizes ( self , width : Optional [ int ], height : Optional [ int ], from_width : Optional [ int ], to_width : Optional [ int ], from_height : Optional [ int ], to_height : Optional [ int ], ) -> None : if width is not None and height is not None : # We have both width and height _validate_positive ( \"Width\" , width ) _validate_positive ( \"Height\" , height ) elif width is not None and from_height is not None and to_height is not None : # We have only width, we expect a random height _validate_positive ( \"Width\" , width ) _validate_positive ( \"From height\" , from_height ) _validate_positive ( \"To height\" , to_height ) _validate_greater ( \"To height\" , \"From height\" , to_height , from_height ) elif height is not None and from_width is not None and to_width is not None : # We have only height, we expect a random width _validate_positive ( \"Height\" , height ) _validate_positive ( \"From width\" , from_width ) _validate_positive ( \"To width\" , to_width ) _validate_greater ( \"To width\" , \"From width\" , to_width , from_width ) else : raise ValueError ( \"You must specify both width or height, \" \"either by specifying a fixed value or a range\" ) self . _width = width self . _height = height self . _from_width = from_width self . _to_width = to_width self . _from_height = from_height self . _to_height = to_height def _parse_algorithms ( self , algorithms : Optional [ list [ str ]]) -> None : if algorithms is None : self . _algorithms = list ( _VALID_ALGORITHMS . keys ()) return for algorithm in algorithms : if algorithm not in _VALID_ALGORITHMS : raise ValueError ( f \"Invalid algorithm: { algorithm } \" ) self . _algorithms = algorithms def _parse_fill_modes ( self , fill_modes : Optional [ list [ str ]]) -> None : if fill_modes is None : self . _fill_modes = list ( _VALID_FILL_MODES . keys ()) return for fill_mode in fill_modes : if fill_mode not in _VALID_FILL_MODES : raise ValueError ( f \"Invalid fill mode: { fill_mode } \" ) self . _fill_modes = fill_modes def _parse_keep_aspect_ratio_probability ( self , keep_aspect_ratio_probability : float ) -> None : if keep_aspect_ratio_probability < 0 or keep_aspect_ratio_probability > 1 : raise ValueError ( \"Keep aspect ratio probability must be between 0 and 1\" ) self . _keep_aspect_ratio_probability = keep_aspect_ratio_probability def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: algorithm = random . choice ( self . _algorithms ) fill_mode = random . choice ( self . _fill_modes ) keep_aspect_ratio = random . random () < self . _keep_aspect_ratio_probability if self . _width is not None : width = self . _width elif self . _from_width is not None and self . _to_width is not None : width = random . randint ( self . _from_width , self . _to_width ) else : raise ValueError ( \"Invalid width\" ) if self . _height is not None : height = self . _height elif self . _from_height is not None and self . _to_height is not None : height = random . randint ( self . _from_height , self . _to_height ) else : raise ValueError ( \"Invalid height\" ) if not keep_aspect_ratio : scale_factors = ( width / image . shape [ 1 ], height / image . shape [ 0 ]) new_img = cv . resize ( image , ( height , width ), interpolation = _VALID_ALGORITHMS [ algorithm ], ) if landmarks is not None : new_landmarks = landmarks * np . array ( scale_factors ) else : new_landmarks = None else : new_size = ( height , width ) old_size = image . shape [: 2 ] scale_factor = min ( n / o for n , o in zip ( new_size , old_size )) rescaled = cv . resize ( image , None , fx = scale_factor , fy = scale_factor , interpolation = _VALID_ALGORITHMS [ algorithm ], ) top_bottom , left_right = tuple ( d - s for d , s in zip ( new_size , rescaled . shape [: 2 ]) ) top = top_bottom // 2 bottom = top_bottom - top left = left_right // 2 right = left_right - left new_img = cv . copyMakeBorder ( rescaled , top , bottom , left , right , _VALID_FILL_MODES [ fill_mode ], ( 0 , 0 , 0 ), ) if landmarks is not None : new_landmarks = landmarks * scale_factor + np . array ([ left , top ]) else : new_landmarks = None return new_img , new_landmarks","title":"Resize"},{"location":"reference/revelio/augmentation/resize/#revelio.augmentation.resize.Resize.__init__","text":"Applies a resize filter to the image. Parameters: Name Type Description Default width Optional [ int ] The fixed width of the image. None height Optional [ int ] The fixed height of the image. None from_width Optional [ int ] The minimum width of the image. None to_width Optional [ int ] The maximum width of the image. None from_height Optional [ int ] The minimum height of the image. None to_height Optional [ int ] The maximum height of the image. None algorithms Optional [ list [ str ]] The list of algorithms to choose from. None fill_modes Optional [ list [ str ]] The list of fill modes to choose from. None keep_aspect_ratio_probability float The probability of keeping the aspect ratio. 1.0 Source code in revelio/augmentation/resize.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , * , width : Optional [ int ] = None , height : Optional [ int ] = None , from_width : Optional [ int ] = None , to_width : Optional [ int ] = None , from_height : Optional [ int ] = None , to_height : Optional [ int ] = None , algorithms : Optional [ list [ str ]] = None , fill_modes : Optional [ list [ str ]] = None , keep_aspect_ratio_probability : float = 1.0 , ** kwargs : Any , ) -> None : \"\"\" Applies a resize filter to the image. Args: width: The fixed width of the image. height: The fixed height of the image. from_width: The minimum width of the image. to_width: The maximum width of the image. from_height: The minimum height of the image. to_height: The maximum height of the image. algorithms: The list of algorithms to choose from. fill_modes: The list of fill modes to choose from. keep_aspect_ratio_probability: The probability of keeping the aspect ratio. \"\"\" super () . __init__ ( ** kwargs ) self . _parse_sizes ( width , height , from_width , to_width , from_height , to_height ) self . _parse_algorithms ( algorithms ) self . _parse_fill_modes ( fill_modes ) self . _parse_keep_aspect_ratio_probability ( keep_aspect_ratio_probability )","title":"__init__()"},{"location":"reference/revelio/augmentation/step/","text":"This module contains the class that defines an augmentation step, from which all augmentation steps inherit. A user can define their own augmentation steps by subclassing this class and overriding the process_element method. AugmentationStep Bases: Registrable An augmentation step is applied to a dataset element with a certain probability. The affected images of the dataset element can be specified with the applies_to parameter in the configuration file. If applies_to is set to \"all\", the augmentation step is applied to all images of the dataset element. If applies_to is set to a list of integers, the augmentation step is applied to the images with the specified indices. The probability parameter specifies the probability (between 0 and 1 inclusive) with which the augmentation step is applied to a dataset element. The process_element method is called for each image of the dataset element that is affected by the augmentation step. An augmentation step must implement the process_element method, which takes an image, its landmarks and returns its augmented version. Only dataset elements which are part of the training set are augmented; validation and test sets are not augmented. Source code in revelio/augmentation/step.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class AugmentationStep ( Registrable ): \"\"\" An augmentation step is applied to a dataset element with a certain probability. The affected images of the dataset element can be specified with the `applies_to` parameter in the configuration file. If `applies_to` is set to \"all\", the augmentation step is applied to all images of the dataset element. If `applies_to` is set to a list of integers, the augmentation step is applied to the images with the specified indices. The probability parameter specifies the probability (between 0 and 1 inclusive) with which the augmentation step is applied to a dataset element. The process_element method is called for each image of the dataset element that is affected by the augmentation step. An augmentation step must implement the `process_element` method, which takes an image, its landmarks and returns its augmented version. Only dataset elements which are part of the training set are augmented; validation and test sets are not augmented. \"\"\" def __init__ ( self , * , _applies_to : list [ int ] | Literal [ \"all\" ], _probability : float ) -> None : self . _applies_to = _applies_to self . _probability = _probability @abstractmethod def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: \"\"\" Processes a single image of a dataset element, and returns its augmented version. Args: image: The image to be augmented. landmarks: The landmarks of the image to be augmented. Returns: A tuple containing the augmented image and its landmarks. \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its augmented version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be augmented. Returns: The augmented dataset element. \"\"\" if random . random () < self . _probability : new_xs = [] for i , x in enumerate ( elem . x ): if self . _applies_to == \"all\" or i in self . _applies_to : try : new_img , new_landmarks = self . process_element ( x . image , x . landmarks ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if new_img is None : raise RuntimeError ( f \"Failed to process { x . path } : image is None\" ) new_x = ElementImage ( path = x . path , image = new_img , landmarks = new_landmarks , features = x . features , ) new_xs . append ( new_x ) else : new_xs . append ( x ) return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ) else : return elem process ( elem : DatasetElement ) -> DatasetElement Processes a dataset element, and returns its augmented version. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to be augmented. required Returns: Type Description DatasetElement The augmented dataset element. Source code in revelio/augmentation/step.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its augmented version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be augmented. Returns: The augmented dataset element. \"\"\" if random . random () < self . _probability : new_xs = [] for i , x in enumerate ( elem . x ): if self . _applies_to == \"all\" or i in self . _applies_to : try : new_img , new_landmarks = self . process_element ( x . image , x . landmarks ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if new_img is None : raise RuntimeError ( f \"Failed to process { x . path } : image is None\" ) new_x = ElementImage ( path = x . path , image = new_img , landmarks = new_landmarks , features = x . features , ) new_xs . append ( new_x ) else : new_xs . append ( x ) return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ) else : return elem process_element ( image : Image , landmarks : Optional [ Landmarks ]) -> tuple [ Image , Optional [ Landmarks ]] abstractmethod Processes a single image of a dataset element, and returns its augmented version. Parameters: Name Type Description Default image Image The image to be augmented. required landmarks Optional [ Landmarks ] The landmarks of the image to be augmented. required Returns: Type Description tuple [ Image , Optional [ Landmarks ]] A tuple containing the augmented image and its landmarks. Source code in revelio/augmentation/step.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @abstractmethod def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: \"\"\" Processes a single image of a dataset element, and returns its augmented version. Args: image: The image to be augmented. landmarks: The landmarks of the image to be augmented. Returns: A tuple containing the augmented image and its landmarks. \"\"\" raise NotImplementedError # pragma: no cover","title":"step"},{"location":"reference/revelio/augmentation/step/#revelio.augmentation.step.AugmentationStep","text":"Bases: Registrable An augmentation step is applied to a dataset element with a certain probability. The affected images of the dataset element can be specified with the applies_to parameter in the configuration file. If applies_to is set to \"all\", the augmentation step is applied to all images of the dataset element. If applies_to is set to a list of integers, the augmentation step is applied to the images with the specified indices. The probability parameter specifies the probability (between 0 and 1 inclusive) with which the augmentation step is applied to a dataset element. The process_element method is called for each image of the dataset element that is affected by the augmentation step. An augmentation step must implement the process_element method, which takes an image, its landmarks and returns its augmented version. Only dataset elements which are part of the training set are augmented; validation and test sets are not augmented. Source code in revelio/augmentation/step.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class AugmentationStep ( Registrable ): \"\"\" An augmentation step is applied to a dataset element with a certain probability. The affected images of the dataset element can be specified with the `applies_to` parameter in the configuration file. If `applies_to` is set to \"all\", the augmentation step is applied to all images of the dataset element. If `applies_to` is set to a list of integers, the augmentation step is applied to the images with the specified indices. The probability parameter specifies the probability (between 0 and 1 inclusive) with which the augmentation step is applied to a dataset element. The process_element method is called for each image of the dataset element that is affected by the augmentation step. An augmentation step must implement the `process_element` method, which takes an image, its landmarks and returns its augmented version. Only dataset elements which are part of the training set are augmented; validation and test sets are not augmented. \"\"\" def __init__ ( self , * , _applies_to : list [ int ] | Literal [ \"all\" ], _probability : float ) -> None : self . _applies_to = _applies_to self . _probability = _probability @abstractmethod def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: \"\"\" Processes a single image of a dataset element, and returns its augmented version. Args: image: The image to be augmented. landmarks: The landmarks of the image to be augmented. Returns: A tuple containing the augmented image and its landmarks. \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its augmented version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be augmented. Returns: The augmented dataset element. \"\"\" if random . random () < self . _probability : new_xs = [] for i , x in enumerate ( elem . x ): if self . _applies_to == \"all\" or i in self . _applies_to : try : new_img , new_landmarks = self . process_element ( x . image , x . landmarks ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if new_img is None : raise RuntimeError ( f \"Failed to process { x . path } : image is None\" ) new_x = ElementImage ( path = x . path , image = new_img , landmarks = new_landmarks , features = x . features , ) new_xs . append ( new_x ) else : new_xs . append ( x ) return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ) else : return elem","title":"AugmentationStep"},{"location":"reference/revelio/augmentation/step/#revelio.augmentation.step.AugmentationStep.process","text":"Processes a dataset element, and returns its augmented version. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to be augmented. required Returns: Type Description DatasetElement The augmented dataset element. Source code in revelio/augmentation/step.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its augmented version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be augmented. Returns: The augmented dataset element. \"\"\" if random . random () < self . _probability : new_xs = [] for i , x in enumerate ( elem . x ): if self . _applies_to == \"all\" or i in self . _applies_to : try : new_img , new_landmarks = self . process_element ( x . image , x . landmarks ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if new_img is None : raise RuntimeError ( f \"Failed to process { x . path } : image is None\" ) new_x = ElementImage ( path = x . path , image = new_img , landmarks = new_landmarks , features = x . features , ) new_xs . append ( new_x ) else : new_xs . append ( x ) return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ) else : return elem","title":"process()"},{"location":"reference/revelio/augmentation/step/#revelio.augmentation.step.AugmentationStep.process_element","text":"Processes a single image of a dataset element, and returns its augmented version. Parameters: Name Type Description Default image Image The image to be augmented. required landmarks Optional [ Landmarks ] The landmarks of the image to be augmented. required Returns: Type Description tuple [ Image , Optional [ Landmarks ]] A tuple containing the augmented image and its landmarks. Source code in revelio/augmentation/step.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @abstractmethod def process_element ( self , image : Image , landmarks : Optional [ Landmarks ] ) -> tuple [ Image , Optional [ Landmarks ]]: \"\"\" Processes a single image of a dataset element, and returns its augmented version. Args: image: The image to be augmented. landmarks: The landmarks of the image to be augmented. Returns: A tuple containing the augmented image and its landmarks. \"\"\" raise NotImplementedError # pragma: no cover","title":"process_element()"},{"location":"reference/revelio/config/","text":"","title":"config"},{"location":"reference/revelio/config/config/","text":"","title":"config"},{"location":"reference/revelio/config/model/","text":"","title":"model"},{"location":"reference/revelio/config/model/augmentation/","text":"","title":"augmentation"},{"location":"reference/revelio/config/model/dataset/","text":"","title":"dataset"},{"location":"reference/revelio/config/model/experiment/","text":"","title":"experiment"},{"location":"reference/revelio/config/model/face_detection/","text":"","title":"face_detection"},{"location":"reference/revelio/config/model/feature_extraction/","text":"","title":"feature_extraction"},{"location":"reference/revelio/config/model/preprocessing/","text":"","title":"preprocessing"},{"location":"reference/revelio/config/model/utils/","text":"","title":"utils"},{"location":"reference/revelio/dataset/","text":"","title":"dataset"},{"location":"reference/revelio/dataset/dataset/","text":"","title":"dataset"},{"location":"reference/revelio/dataset/dataset_factory/","text":"","title":"dataset_factory"},{"location":"reference/revelio/dataset/descriptors_list/","text":"","title":"descriptors_list"},{"location":"reference/revelio/dataset/element/","text":"DatasetElement An element of the dataset. Attributes: Name Type Description x tuple [ ElementImage , ...] The image(s) of the dataset element. y ElementClass The class of the dataset element. original_dataset str The name of the original dataset from which the element was taken. It is equal to the dataset name specified in the configuration file. dataset_root_path Path The path to the root directory of the dataset. Source code in revelio/dataset/element.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class DatasetElement : \"\"\" An element of the dataset. Attributes: x: The image(s) of the dataset element. y: The class of the dataset element. original_dataset: The name of the original dataset from which the element was taken. It is equal to the dataset name specified in the configuration file. dataset_root_path: The path to the root directory of the dataset. \"\"\" _dataset_root_path : Path _original_dataset : str _x : tuple [ ElementImage , ... ] _y : ElementClass def __init__ ( self , x : tuple [ ElementImage , ... ], y : ElementClass , * , dataset_root_path : Path , original_dataset : str , ) -> None : \"\"\" Creates a new dataset element. Args: x: The image(s) of the dataset element. y: The class of the dataset element. dataset_root_path: The path to the root directory of the dataset. original_dataset: The name of the original dataset from which the element was taken. \"\"\" self . _x = x self . _y = y self . _dataset_root_path = dataset_root_path self . _original_dataset = original_dataset @property def dataset_root_path ( self ) -> Path : \"\"\" Gets the path to the root directory of the dataset. \"\"\" return self . _dataset_root_path @property def original_dataset ( self ) -> str : \"\"\" Gets the name of the original dataset from which the element was taken. \"\"\" return self . _original_dataset @property def x ( self ) -> tuple [ ElementImage , ... ]: \"\"\" Gets the image(s) of the dataset element. \"\"\" return self . _x @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y def __repr__ ( self ) -> str : return f \"DatasetElement(x= { self . x } , y= { self . y } )\" __init__ ( x : tuple [ ElementImage , ... ], y : ElementClass , * , dataset_root_path : Path , original_dataset : str ) -> None Creates a new dataset element. Parameters: Name Type Description Default x tuple [ ElementImage , ...] The image(s) of the dataset element. required y ElementClass The class of the dataset element. required dataset_root_path Path The path to the root directory of the dataset. required original_dataset str The name of the original dataset from which the element was taken. required Source code in revelio/dataset/element.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , x : tuple [ ElementImage , ... ], y : ElementClass , * , dataset_root_path : Path , original_dataset : str , ) -> None : \"\"\" Creates a new dataset element. Args: x: The image(s) of the dataset element. y: The class of the dataset element. dataset_root_path: The path to the root directory of the dataset. original_dataset: The name of the original dataset from which the element was taken. \"\"\" self . _x = x self . _y = y self . _dataset_root_path = dataset_root_path self . _original_dataset = original_dataset dataset_root_path () -> Path property Gets the path to the root directory of the dataset. Source code in revelio/dataset/element.py 193 194 195 196 197 198 @property def dataset_root_path ( self ) -> Path : \"\"\" Gets the path to the root directory of the dataset. \"\"\" return self . _dataset_root_path original_dataset () -> str property Gets the name of the original dataset from which the element was taken. Source code in revelio/dataset/element.py 200 201 202 203 204 205 @property def original_dataset ( self ) -> str : \"\"\" Gets the name of the original dataset from which the element was taken. \"\"\" return self . _original_dataset x () -> tuple [ ElementImage , ... ] property Gets the image(s) of the dataset element. Source code in revelio/dataset/element.py 207 208 209 210 211 212 @property def x ( self ) -> tuple [ ElementImage , ... ]: \"\"\" Gets the image(s) of the dataset element. \"\"\" return self . _x y () -> ElementClass property Gets the class of the dataset element. Source code in revelio/dataset/element.py 214 215 216 217 218 219 @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y DatasetElementDescriptor A descriptor of a dataset element before it is loaded into memory. Attributes: Name Type Description x tuple [ Path , ...] The path to the image file(s). y ElementClass The class of the dataset element. Source code in revelio/dataset/element.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class DatasetElementDescriptor : \"\"\" A descriptor of a dataset element before it is loaded into memory. Attributes: x: The path to the image file(s). y: The class of the dataset element. \"\"\" _x : tuple [ Path , ... ] _y : ElementClass _root_path : Path _dataset_name : str def __init__ ( self , x : tuple [ Path , ... ], y : ElementClass ) -> None : \"\"\" Creates a new dataset element descriptor. Args: x: The path to the image file(s). y: The class of the dataset element. \"\"\" self . _x = x self . _y = y @property def x ( self ) -> tuple [ Path , ... ]: \"\"\" Gets the path to the image file(s). \"\"\" return self . _x @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y def __repr__ ( self ) -> str : return f \"DatasetElementDescriptor(x= { self . x } , y= { self . y } )\" def __eq__ ( self , other : Any ) -> bool : if not isinstance ( other , DatasetElementDescriptor ): return NotImplemented return ( self . x == other . x and self . y == other . y and self . _root_path == other . _root_path and self . _dataset_name == other . _dataset_name ) def __hash__ ( self ) -> int : return hash (( self . x , self . y , self . _root_path , self . _dataset_name )) __init__ ( x : tuple [ Path , ... ], y : ElementClass ) -> None Creates a new dataset element descriptor. Parameters: Name Type Description Default x tuple [ Path , ...] The path to the image file(s). required y ElementClass The class of the dataset element. required Source code in revelio/dataset/element.py 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , x : tuple [ Path , ... ], y : ElementClass ) -> None : \"\"\" Creates a new dataset element descriptor. Args: x: The path to the image file(s). y: The class of the dataset element. \"\"\" self . _x = x self . _y = y x () -> tuple [ Path , ... ] property Gets the path to the image file(s). Source code in revelio/dataset/element.py 46 47 48 49 50 51 @property def x ( self ) -> tuple [ Path , ... ]: \"\"\" Gets the path to the image file(s). \"\"\" return self . _x y () -> ElementClass property Gets the class of the dataset element. Source code in revelio/dataset/element.py 53 54 55 56 57 58 @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y ElementClass Bases: Enum The class of a dataset element, which can be either bona fide or morphed. Source code in revelio/dataset/element.py 12 13 14 15 16 17 18 class ElementClass ( Enum ): \"\"\" The class of a dataset element, which can be either bona fide or morphed. \"\"\" BONA_FIDE = 0.0 MORPHED = 1.0 ElementImage An image that is part of a dataset element. An image is represented with a Numpy array with shape (height, width, channels) and dtype uint8 or float32. The channels are ordered as BGR, following the OpenCV convention. Attributes: Name Type Description path Path The path to the image file. image Image The image. landmarks Optional [ Landmarks ] The facial landmarks of the image (if present). features dict [ str , np . ndarray ] The features of the image produced by each feature extractor (if present). Source code in revelio/dataset/element.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class ElementImage : \"\"\" An image that is part of a dataset element. An image is represented with a Numpy array with shape (height, width, channels) and dtype uint8 or float32. The channels are ordered as BGR, following the OpenCV convention. Attributes: path: The path to the image file. image: The image. landmarks: The facial landmarks of the image (if present). features: The features of the image produced by each feature extractor (if present). \"\"\" _path : Path _image : Image _landmarks : Optional [ Landmarks ] _features : dict [ str , np . ndarray ] def __init__ ( self , path : Path , image : Image , landmarks : Optional [ Landmarks ] = None , features : Optional [ dict [ str , np . ndarray ]] = None , ) -> None : \"\"\" Creates a new image of a dataset element. Args: path: The path to the image file. image: The image. landmarks: The facial landmarks of the image (if present). features: The features of the image produced by each feature extractor (if present). \"\"\" self . _path = path self . _image = image self . _landmarks = landmarks self . _features = features or {} @property def path ( self ) -> Path : \"\"\" Gets the path to the image file. \"\"\" return self . _path @property def image ( self ) -> Image : \"\"\" Gets the image. \"\"\" return self . _image @property def landmarks ( self ) -> Optional [ Landmarks ]: \"\"\" Gets the facial landmarks of the image (if present). \"\"\" return self . _landmarks @property def features ( self ) -> dict [ str , np . ndarray ]: \"\"\" Gets the features of the image produced by each feature extractor (if present). \"\"\" return self . _features def __repr__ ( self ) -> str : return f \"ElementImage(path= { self . _path } )\" __init__ ( path : Path , image : Image , landmarks : Optional [ Landmarks ] = None , features : Optional [ dict [ str , np . ndarray ]] = None ) -> None Creates a new image of a dataset element. Parameters: Name Type Description Default path Path The path to the image file. required image Image The image. required landmarks Optional [ Landmarks ] The facial landmarks of the image (if present). None features Optional [ dict [ str , np . ndarray ]] The features of the image produced by each feature extractor (if present). None Source code in revelio/dataset/element.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , path : Path , image : Image , landmarks : Optional [ Landmarks ] = None , features : Optional [ dict [ str , np . ndarray ]] = None , ) -> None : \"\"\" Creates a new image of a dataset element. Args: path: The path to the image file. image: The image. landmarks: The facial landmarks of the image (if present). features: The features of the image produced by each feature extractor (if present). \"\"\" self . _path = path self . _image = image self . _landmarks = landmarks self . _features = features or {} features () -> dict [ str , np . ndarray ] property Gets the features of the image produced by each feature extractor (if present). Source code in revelio/dataset/element.py 141 142 143 144 145 146 @property def features ( self ) -> dict [ str , np . ndarray ]: \"\"\" Gets the features of the image produced by each feature extractor (if present). \"\"\" return self . _features image () -> Image property Gets the image. Source code in revelio/dataset/element.py 127 128 129 130 131 132 @property def image ( self ) -> Image : \"\"\" Gets the image. \"\"\" return self . _image landmarks () -> Optional [ Landmarks ] property Gets the facial landmarks of the image (if present). Source code in revelio/dataset/element.py 134 135 136 137 138 139 @property def landmarks ( self ) -> Optional [ Landmarks ]: \"\"\" Gets the facial landmarks of the image (if present). \"\"\" return self . _landmarks path () -> Path property Gets the path to the image file. Source code in revelio/dataset/element.py 120 121 122 123 124 125 @property def path ( self ) -> Path : \"\"\" Gets the path to the image file. \"\"\" return self . _path","title":"element"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElement","text":"An element of the dataset. Attributes: Name Type Description x tuple [ ElementImage , ...] The image(s) of the dataset element. y ElementClass The class of the dataset element. original_dataset str The name of the original dataset from which the element was taken. It is equal to the dataset name specified in the configuration file. dataset_root_path Path The path to the root directory of the dataset. Source code in revelio/dataset/element.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class DatasetElement : \"\"\" An element of the dataset. Attributes: x: The image(s) of the dataset element. y: The class of the dataset element. original_dataset: The name of the original dataset from which the element was taken. It is equal to the dataset name specified in the configuration file. dataset_root_path: The path to the root directory of the dataset. \"\"\" _dataset_root_path : Path _original_dataset : str _x : tuple [ ElementImage , ... ] _y : ElementClass def __init__ ( self , x : tuple [ ElementImage , ... ], y : ElementClass , * , dataset_root_path : Path , original_dataset : str , ) -> None : \"\"\" Creates a new dataset element. Args: x: The image(s) of the dataset element. y: The class of the dataset element. dataset_root_path: The path to the root directory of the dataset. original_dataset: The name of the original dataset from which the element was taken. \"\"\" self . _x = x self . _y = y self . _dataset_root_path = dataset_root_path self . _original_dataset = original_dataset @property def dataset_root_path ( self ) -> Path : \"\"\" Gets the path to the root directory of the dataset. \"\"\" return self . _dataset_root_path @property def original_dataset ( self ) -> str : \"\"\" Gets the name of the original dataset from which the element was taken. \"\"\" return self . _original_dataset @property def x ( self ) -> tuple [ ElementImage , ... ]: \"\"\" Gets the image(s) of the dataset element. \"\"\" return self . _x @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y def __repr__ ( self ) -> str : return f \"DatasetElement(x= { self . x } , y= { self . y } )\"","title":"DatasetElement"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElement.__init__","text":"Creates a new dataset element. Parameters: Name Type Description Default x tuple [ ElementImage , ...] The image(s) of the dataset element. required y ElementClass The class of the dataset element. required dataset_root_path Path The path to the root directory of the dataset. required original_dataset str The name of the original dataset from which the element was taken. required Source code in revelio/dataset/element.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , x : tuple [ ElementImage , ... ], y : ElementClass , * , dataset_root_path : Path , original_dataset : str , ) -> None : \"\"\" Creates a new dataset element. Args: x: The image(s) of the dataset element. y: The class of the dataset element. dataset_root_path: The path to the root directory of the dataset. original_dataset: The name of the original dataset from which the element was taken. \"\"\" self . _x = x self . _y = y self . _dataset_root_path = dataset_root_path self . _original_dataset = original_dataset","title":"__init__()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElement.dataset_root_path","text":"Gets the path to the root directory of the dataset. Source code in revelio/dataset/element.py 193 194 195 196 197 198 @property def dataset_root_path ( self ) -> Path : \"\"\" Gets the path to the root directory of the dataset. \"\"\" return self . _dataset_root_path","title":"dataset_root_path()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElement.original_dataset","text":"Gets the name of the original dataset from which the element was taken. Source code in revelio/dataset/element.py 200 201 202 203 204 205 @property def original_dataset ( self ) -> str : \"\"\" Gets the name of the original dataset from which the element was taken. \"\"\" return self . _original_dataset","title":"original_dataset()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElement.x","text":"Gets the image(s) of the dataset element. Source code in revelio/dataset/element.py 207 208 209 210 211 212 @property def x ( self ) -> tuple [ ElementImage , ... ]: \"\"\" Gets the image(s) of the dataset element. \"\"\" return self . _x","title":"x()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElement.y","text":"Gets the class of the dataset element. Source code in revelio/dataset/element.py 214 215 216 217 218 219 @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y","title":"y()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElementDescriptor","text":"A descriptor of a dataset element before it is loaded into memory. Attributes: Name Type Description x tuple [ Path , ...] The path to the image file(s). y ElementClass The class of the dataset element. Source code in revelio/dataset/element.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class DatasetElementDescriptor : \"\"\" A descriptor of a dataset element before it is loaded into memory. Attributes: x: The path to the image file(s). y: The class of the dataset element. \"\"\" _x : tuple [ Path , ... ] _y : ElementClass _root_path : Path _dataset_name : str def __init__ ( self , x : tuple [ Path , ... ], y : ElementClass ) -> None : \"\"\" Creates a new dataset element descriptor. Args: x: The path to the image file(s). y: The class of the dataset element. \"\"\" self . _x = x self . _y = y @property def x ( self ) -> tuple [ Path , ... ]: \"\"\" Gets the path to the image file(s). \"\"\" return self . _x @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y def __repr__ ( self ) -> str : return f \"DatasetElementDescriptor(x= { self . x } , y= { self . y } )\" def __eq__ ( self , other : Any ) -> bool : if not isinstance ( other , DatasetElementDescriptor ): return NotImplemented return ( self . x == other . x and self . y == other . y and self . _root_path == other . _root_path and self . _dataset_name == other . _dataset_name ) def __hash__ ( self ) -> int : return hash (( self . x , self . y , self . _root_path , self . _dataset_name ))","title":"DatasetElementDescriptor"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElementDescriptor.__init__","text":"Creates a new dataset element descriptor. Parameters: Name Type Description Default x tuple [ Path , ...] The path to the image file(s). required y ElementClass The class of the dataset element. required Source code in revelio/dataset/element.py 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , x : tuple [ Path , ... ], y : ElementClass ) -> None : \"\"\" Creates a new dataset element descriptor. Args: x: The path to the image file(s). y: The class of the dataset element. \"\"\" self . _x = x self . _y = y","title":"__init__()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElementDescriptor.x","text":"Gets the path to the image file(s). Source code in revelio/dataset/element.py 46 47 48 49 50 51 @property def x ( self ) -> tuple [ Path , ... ]: \"\"\" Gets the path to the image file(s). \"\"\" return self . _x","title":"x()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.DatasetElementDescriptor.y","text":"Gets the class of the dataset element. Source code in revelio/dataset/element.py 53 54 55 56 57 58 @property def y ( self ) -> ElementClass : \"\"\" Gets the class of the dataset element. \"\"\" return self . _y","title":"y()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementClass","text":"Bases: Enum The class of a dataset element, which can be either bona fide or morphed. Source code in revelio/dataset/element.py 12 13 14 15 16 17 18 class ElementClass ( Enum ): \"\"\" The class of a dataset element, which can be either bona fide or morphed. \"\"\" BONA_FIDE = 0.0 MORPHED = 1.0","title":"ElementClass"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementImage","text":"An image that is part of a dataset element. An image is represented with a Numpy array with shape (height, width, channels) and dtype uint8 or float32. The channels are ordered as BGR, following the OpenCV convention. Attributes: Name Type Description path Path The path to the image file. image Image The image. landmarks Optional [ Landmarks ] The facial landmarks of the image (if present). features dict [ str , np . ndarray ] The features of the image produced by each feature extractor (if present). Source code in revelio/dataset/element.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class ElementImage : \"\"\" An image that is part of a dataset element. An image is represented with a Numpy array with shape (height, width, channels) and dtype uint8 or float32. The channels are ordered as BGR, following the OpenCV convention. Attributes: path: The path to the image file. image: The image. landmarks: The facial landmarks of the image (if present). features: The features of the image produced by each feature extractor (if present). \"\"\" _path : Path _image : Image _landmarks : Optional [ Landmarks ] _features : dict [ str , np . ndarray ] def __init__ ( self , path : Path , image : Image , landmarks : Optional [ Landmarks ] = None , features : Optional [ dict [ str , np . ndarray ]] = None , ) -> None : \"\"\" Creates a new image of a dataset element. Args: path: The path to the image file. image: The image. landmarks: The facial landmarks of the image (if present). features: The features of the image produced by each feature extractor (if present). \"\"\" self . _path = path self . _image = image self . _landmarks = landmarks self . _features = features or {} @property def path ( self ) -> Path : \"\"\" Gets the path to the image file. \"\"\" return self . _path @property def image ( self ) -> Image : \"\"\" Gets the image. \"\"\" return self . _image @property def landmarks ( self ) -> Optional [ Landmarks ]: \"\"\" Gets the facial landmarks of the image (if present). \"\"\" return self . _landmarks @property def features ( self ) -> dict [ str , np . ndarray ]: \"\"\" Gets the features of the image produced by each feature extractor (if present). \"\"\" return self . _features def __repr__ ( self ) -> str : return f \"ElementImage(path= { self . _path } )\"","title":"ElementImage"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementImage.__init__","text":"Creates a new image of a dataset element. Parameters: Name Type Description Default path Path The path to the image file. required image Image The image. required landmarks Optional [ Landmarks ] The facial landmarks of the image (if present). None features Optional [ dict [ str , np . ndarray ]] The features of the image produced by each feature extractor (if present). None Source code in revelio/dataset/element.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , path : Path , image : Image , landmarks : Optional [ Landmarks ] = None , features : Optional [ dict [ str , np . ndarray ]] = None , ) -> None : \"\"\" Creates a new image of a dataset element. Args: path: The path to the image file. image: The image. landmarks: The facial landmarks of the image (if present). features: The features of the image produced by each feature extractor (if present). \"\"\" self . _path = path self . _image = image self . _landmarks = landmarks self . _features = features or {}","title":"__init__()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementImage.features","text":"Gets the features of the image produced by each feature extractor (if present). Source code in revelio/dataset/element.py 141 142 143 144 145 146 @property def features ( self ) -> dict [ str , np . ndarray ]: \"\"\" Gets the features of the image produced by each feature extractor (if present). \"\"\" return self . _features","title":"features()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementImage.image","text":"Gets the image. Source code in revelio/dataset/element.py 127 128 129 130 131 132 @property def image ( self ) -> Image : \"\"\" Gets the image. \"\"\" return self . _image","title":"image()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementImage.landmarks","text":"Gets the facial landmarks of the image (if present). Source code in revelio/dataset/element.py 134 135 136 137 138 139 @property def landmarks ( self ) -> Optional [ Landmarks ]: \"\"\" Gets the facial landmarks of the image (if present). \"\"\" return self . _landmarks","title":"landmarks()"},{"location":"reference/revelio/dataset/element/#revelio.dataset.element.ElementImage.path","text":"Gets the path to the image file. Source code in revelio/dataset/element.py 120 121 122 123 124 125 @property def path ( self ) -> Path : \"\"\" Gets the path to the image file. \"\"\" return self . _path","title":"path()"},{"location":"reference/revelio/dataset/loaders/","text":"","title":"loaders"},{"location":"reference/revelio/dataset/loaders/loader/","text":"DatasetLoader Bases: Registrable A dataset loader is responsible for loading a dataset from a given path. The configuration file specifies the dataset loader to use on the path given by the user, and its job is to correctly load the dataset from that path. To load a dataset element, the loader must return a list of descriptors, each containing the path of each image and the class of the element. In case of D-MAD, the convention that must be followed is that the first element in the tuple must be the probe image, and the second element must be the live capture image. To guarantee reproducibility of results, it is important that the list returned by the loader is the same for the same path, and its order must be deterministic. Also, please note that the glob functions in Python are not guaranteed to return the files in a deterministic order, so you should sort the list of files before returning it. Source code in revelio/dataset/loaders/loader.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class DatasetLoader ( Registrable ): \"\"\" A dataset loader is responsible for loading a dataset from a given path. The configuration file specifies the dataset loader to use on the path given by the user, and its job is to correctly load the dataset from that path. To load a dataset element, the loader must return a list of descriptors, each containing the path of each image and the class of the element. In case of D-MAD, the convention that must be followed is that the first element in the tuple must be the probe image, and the second element must be the live capture image. To guarantee reproducibility of results, it is important that the list returned by the loader is the same for the same path, and its order must be deterministic. Also, please note that the glob functions in Python are not guaranteed to return the files in a deterministic order, so you should sort the list of files before returning it. \"\"\" suffix = \"Loader\" @abstractmethod def load ( self , path : Path ) -> list [ DatasetElementDescriptor ]: \"\"\" Loads a dataset from a given path. Args: path: The path to the dataset. Returns: A list of descriptors, each containing the path of each image and the class of the element. \"\"\" raise NotImplementedError # pragma: no cover load ( path : Path ) -> list [ DatasetElementDescriptor ] abstractmethod Loads a dataset from a given path. Parameters: Name Type Description Default path Path The path to the dataset. required Returns: Type Description list [ DatasetElementDescriptor ] A list of descriptors, each containing the path of each image and the list [ DatasetElementDescriptor ] class of the element. Source code in revelio/dataset/loaders/loader.py 32 33 34 35 36 37 38 39 40 41 42 43 44 @abstractmethod def load ( self , path : Path ) -> list [ DatasetElementDescriptor ]: \"\"\" Loads a dataset from a given path. Args: path: The path to the dataset. Returns: A list of descriptors, each containing the path of each image and the class of the element. \"\"\" raise NotImplementedError # pragma: no cover","title":"loader"},{"location":"reference/revelio/dataset/loaders/loader/#revelio.dataset.loaders.loader.DatasetLoader","text":"Bases: Registrable A dataset loader is responsible for loading a dataset from a given path. The configuration file specifies the dataset loader to use on the path given by the user, and its job is to correctly load the dataset from that path. To load a dataset element, the loader must return a list of descriptors, each containing the path of each image and the class of the element. In case of D-MAD, the convention that must be followed is that the first element in the tuple must be the probe image, and the second element must be the live capture image. To guarantee reproducibility of results, it is important that the list returned by the loader is the same for the same path, and its order must be deterministic. Also, please note that the glob functions in Python are not guaranteed to return the files in a deterministic order, so you should sort the list of files before returning it. Source code in revelio/dataset/loaders/loader.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class DatasetLoader ( Registrable ): \"\"\" A dataset loader is responsible for loading a dataset from a given path. The configuration file specifies the dataset loader to use on the path given by the user, and its job is to correctly load the dataset from that path. To load a dataset element, the loader must return a list of descriptors, each containing the path of each image and the class of the element. In case of D-MAD, the convention that must be followed is that the first element in the tuple must be the probe image, and the second element must be the live capture image. To guarantee reproducibility of results, it is important that the list returned by the loader is the same for the same path, and its order must be deterministic. Also, please note that the glob functions in Python are not guaranteed to return the files in a deterministic order, so you should sort the list of files before returning it. \"\"\" suffix = \"Loader\" @abstractmethod def load ( self , path : Path ) -> list [ DatasetElementDescriptor ]: \"\"\" Loads a dataset from a given path. Args: path: The path to the dataset. Returns: A list of descriptors, each containing the path of each image and the class of the element. \"\"\" raise NotImplementedError # pragma: no cover","title":"DatasetLoader"},{"location":"reference/revelio/dataset/loaders/loader/#revelio.dataset.loaders.loader.DatasetLoader.load","text":"Loads a dataset from a given path. Parameters: Name Type Description Default path Path The path to the dataset. required Returns: Type Description list [ DatasetElementDescriptor ] A list of descriptors, each containing the path of each image and the list [ DatasetElementDescriptor ] class of the element. Source code in revelio/dataset/loaders/loader.py 32 33 34 35 36 37 38 39 40 41 42 43 44 @abstractmethod def load ( self , path : Path ) -> list [ DatasetElementDescriptor ]: \"\"\" Loads a dataset from a given path. Args: path: The path to the dataset. Returns: A list of descriptors, each containing the path of each image and the class of the element. \"\"\" raise NotImplementedError # pragma: no cover","title":"load()"},{"location":"reference/revelio/dataset/loaders/loaders/","text":"","title":"loaders"},{"location":"reference/revelio/face_detection/","text":"","title":"face_detection"},{"location":"reference/revelio/face_detection/detector/","text":"FaceDetector Bases: Registrable A face detector is responsible for detecting faces in the dataset images. It is also responsible for cropping the images to only contain the face, and for detecting the facial landmarks (if the algorithm supports it). A face detector must implement the process_element method, which takes an image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. The process method is responsible for loading the bounding box and landmarks from the disk, if they have been already computed, or else calling process_element and saving the results. The user should not override this method, but instead implement process_element . Source code in revelio/face_detection/detector.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class FaceDetector ( Registrable ): \"\"\" A face detector is responsible for detecting faces in the dataset images. It is also responsible for cropping the images to only contain the face, and for detecting the facial landmarks (if the algorithm supports it). A face detector must implement the `process_element` method, which takes an image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. The `process` method is responsible for loading the bounding box and landmarks from the disk, if they have been already computed, or else calling `process_element` and saving the results. The user should not override this method, but instead implement `process_element`. \"\"\" def __init__ ( self , * , _config : Config ) -> None : self . _config = _config self . _cacher = ZstdCacher () def _get_meta_path ( self , elem : DatasetElement , x_idx : int ) -> Path : output_path = Path ( self . _config . face_detection . output_path ) algorithm_name = type ( self ) . __name__ . lower () relative_img_path = elem . x [ x_idx ] . path . relative_to ( elem . dataset_root_path ) return ( output_path / algorithm_name / elem . original_dataset / relative_img_path . parent / f \" { relative_img_path . stem } .meta.xz\" ) @abstractmethod def process_element ( self , elem : Image ) -> tuple [ BoundingBox , Optional [ Landmarks ]]: \"\"\" Processes a single image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A tuple containing the bounding box (required) and the facial landmarks (optional). \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with each image cropped to only contain the face. Also, if the algorithm supports it, the facial landmarks are extracted and saved for each image of the element. This method saves the bounding box and the facial landmarks to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. Returns: A tuple containing the processed dataset element and a boolean indicating whether the face detection was loaded from cache. \"\"\" new_xs = [] cached = True for i , x in enumerate ( elem . x ): meta_path = self . _get_meta_path ( elem , i ) if meta_path . is_file (): try : meta = self . _cacher . load ( meta_path ) except ValueError as e : raise RuntimeError ( f \"Failed to load meta file: { meta_path } \" ) from e landmarks = meta [ \"landmarks\" ] if \"landmarks\" in meta else None if \"bb\" in meta : # Crop the image using the precomputed bounding box x1 , y1 , x2 , y2 = meta [ \"bb\" ] image = x . image [ y1 : y2 , x1 : x2 ] new_x = ElementImage ( path = x . path , image = image , landmarks = landmarks , ) new_xs . append ( new_x ) else : raise ValueError ( f \"No bounding box found in { meta_path } \" ) else : cached = False try : bb , landmarks = self . process_element ( x . image ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if bb is None or len ( bb ) != 4 : raise RuntimeError ( f \"Failed to process { x . path } : returned { bb } \" ) # Make sure that the bounding box is always inside the image clipped_bb = ( max ( 0 , bb [ 0 ]), max ( 0 , bb [ 1 ]), min ( x . image . shape [ 1 ], bb [ 2 ]), min ( x . image . shape [ 0 ], bb [ 3 ]), ) x1 , y1 , x2 , y2 = clipped_bb new_x = ElementImage ( path = x . path , image = x . image [ y1 : y2 , x1 : x2 ], landmarks = landmarks , ) # Create the meta file meta_path . parent . mkdir ( parents = True , exist_ok = True ) if landmarks is not None : self . _cacher . save ( meta_path , bb = clipped_bb , landmarks = landmarks ) else : self . _cacher . save ( meta_path , bb = clipped_bb ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , ) process ( elem : DatasetElement ) -> tuple [ DatasetElement , bool ] Processes a dataset element and returns an element with the same data, but with each image cropped to only contain the face. Also, if the algorithm supports it, the facial landmarks are extracted and saved for each image of the element. This method saves the bounding box and the facial landmarks to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to process. required Returns: Type Description DatasetElement A tuple containing the processed dataset element and a boolean indicating bool whether the face detection was loaded from cache. Source code in revelio/face_detection/detector.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def process ( self , elem : DatasetElement ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with each image cropped to only contain the face. Also, if the algorithm supports it, the facial landmarks are extracted and saved for each image of the element. This method saves the bounding box and the facial landmarks to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. Returns: A tuple containing the processed dataset element and a boolean indicating whether the face detection was loaded from cache. \"\"\" new_xs = [] cached = True for i , x in enumerate ( elem . x ): meta_path = self . _get_meta_path ( elem , i ) if meta_path . is_file (): try : meta = self . _cacher . load ( meta_path ) except ValueError as e : raise RuntimeError ( f \"Failed to load meta file: { meta_path } \" ) from e landmarks = meta [ \"landmarks\" ] if \"landmarks\" in meta else None if \"bb\" in meta : # Crop the image using the precomputed bounding box x1 , y1 , x2 , y2 = meta [ \"bb\" ] image = x . image [ y1 : y2 , x1 : x2 ] new_x = ElementImage ( path = x . path , image = image , landmarks = landmarks , ) new_xs . append ( new_x ) else : raise ValueError ( f \"No bounding box found in { meta_path } \" ) else : cached = False try : bb , landmarks = self . process_element ( x . image ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if bb is None or len ( bb ) != 4 : raise RuntimeError ( f \"Failed to process { x . path } : returned { bb } \" ) # Make sure that the bounding box is always inside the image clipped_bb = ( max ( 0 , bb [ 0 ]), max ( 0 , bb [ 1 ]), min ( x . image . shape [ 1 ], bb [ 2 ]), min ( x . image . shape [ 0 ], bb [ 3 ]), ) x1 , y1 , x2 , y2 = clipped_bb new_x = ElementImage ( path = x . path , image = x . image [ y1 : y2 , x1 : x2 ], landmarks = landmarks , ) # Create the meta file meta_path . parent . mkdir ( parents = True , exist_ok = True ) if landmarks is not None : self . _cacher . save ( meta_path , bb = clipped_bb , landmarks = landmarks ) else : self . _cacher . save ( meta_path , bb = clipped_bb ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , ) process_element ( elem : Image ) -> tuple [ BoundingBox , Optional [ Landmarks ]] abstractmethod Processes a single image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. Parameters: Name Type Description Default elem Image The image to process. required Returns: Type Description BoundingBox A tuple containing the bounding box (required) and the facial landmarks Optional [ Landmarks ] (optional). Source code in revelio/face_detection/detector.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @abstractmethod def process_element ( self , elem : Image ) -> tuple [ BoundingBox , Optional [ Landmarks ]]: \"\"\" Processes a single image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A tuple containing the bounding box (required) and the facial landmarks (optional). \"\"\" raise NotImplementedError # pragma: no cover","title":"detector"},{"location":"reference/revelio/face_detection/detector/#revelio.face_detection.detector.FaceDetector","text":"Bases: Registrable A face detector is responsible for detecting faces in the dataset images. It is also responsible for cropping the images to only contain the face, and for detecting the facial landmarks (if the algorithm supports it). A face detector must implement the process_element method, which takes an image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. The process method is responsible for loading the bounding box and landmarks from the disk, if they have been already computed, or else calling process_element and saving the results. The user should not override this method, but instead implement process_element . Source code in revelio/face_detection/detector.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class FaceDetector ( Registrable ): \"\"\" A face detector is responsible for detecting faces in the dataset images. It is also responsible for cropping the images to only contain the face, and for detecting the facial landmarks (if the algorithm supports it). A face detector must implement the `process_element` method, which takes an image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. The `process` method is responsible for loading the bounding box and landmarks from the disk, if they have been already computed, or else calling `process_element` and saving the results. The user should not override this method, but instead implement `process_element`. \"\"\" def __init__ ( self , * , _config : Config ) -> None : self . _config = _config self . _cacher = ZstdCacher () def _get_meta_path ( self , elem : DatasetElement , x_idx : int ) -> Path : output_path = Path ( self . _config . face_detection . output_path ) algorithm_name = type ( self ) . __name__ . lower () relative_img_path = elem . x [ x_idx ] . path . relative_to ( elem . dataset_root_path ) return ( output_path / algorithm_name / elem . original_dataset / relative_img_path . parent / f \" { relative_img_path . stem } .meta.xz\" ) @abstractmethod def process_element ( self , elem : Image ) -> tuple [ BoundingBox , Optional [ Landmarks ]]: \"\"\" Processes a single image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A tuple containing the bounding box (required) and the facial landmarks (optional). \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with each image cropped to only contain the face. Also, if the algorithm supports it, the facial landmarks are extracted and saved for each image of the element. This method saves the bounding box and the facial landmarks to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. Returns: A tuple containing the processed dataset element and a boolean indicating whether the face detection was loaded from cache. \"\"\" new_xs = [] cached = True for i , x in enumerate ( elem . x ): meta_path = self . _get_meta_path ( elem , i ) if meta_path . is_file (): try : meta = self . _cacher . load ( meta_path ) except ValueError as e : raise RuntimeError ( f \"Failed to load meta file: { meta_path } \" ) from e landmarks = meta [ \"landmarks\" ] if \"landmarks\" in meta else None if \"bb\" in meta : # Crop the image using the precomputed bounding box x1 , y1 , x2 , y2 = meta [ \"bb\" ] image = x . image [ y1 : y2 , x1 : x2 ] new_x = ElementImage ( path = x . path , image = image , landmarks = landmarks , ) new_xs . append ( new_x ) else : raise ValueError ( f \"No bounding box found in { meta_path } \" ) else : cached = False try : bb , landmarks = self . process_element ( x . image ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if bb is None or len ( bb ) != 4 : raise RuntimeError ( f \"Failed to process { x . path } : returned { bb } \" ) # Make sure that the bounding box is always inside the image clipped_bb = ( max ( 0 , bb [ 0 ]), max ( 0 , bb [ 1 ]), min ( x . image . shape [ 1 ], bb [ 2 ]), min ( x . image . shape [ 0 ], bb [ 3 ]), ) x1 , y1 , x2 , y2 = clipped_bb new_x = ElementImage ( path = x . path , image = x . image [ y1 : y2 , x1 : x2 ], landmarks = landmarks , ) # Create the meta file meta_path . parent . mkdir ( parents = True , exist_ok = True ) if landmarks is not None : self . _cacher . save ( meta_path , bb = clipped_bb , landmarks = landmarks ) else : self . _cacher . save ( meta_path , bb = clipped_bb ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , )","title":"FaceDetector"},{"location":"reference/revelio/face_detection/detector/#revelio.face_detection.detector.FaceDetector.process","text":"Processes a dataset element and returns an element with the same data, but with each image cropped to only contain the face. Also, if the algorithm supports it, the facial landmarks are extracted and saved for each image of the element. This method saves the bounding box and the facial landmarks to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to process. required Returns: Type Description DatasetElement A tuple containing the processed dataset element and a boolean indicating bool whether the face detection was loaded from cache. Source code in revelio/face_detection/detector.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def process ( self , elem : DatasetElement ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with each image cropped to only contain the face. Also, if the algorithm supports it, the facial landmarks are extracted and saved for each image of the element. This method saves the bounding box and the facial landmarks to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. Returns: A tuple containing the processed dataset element and a boolean indicating whether the face detection was loaded from cache. \"\"\" new_xs = [] cached = True for i , x in enumerate ( elem . x ): meta_path = self . _get_meta_path ( elem , i ) if meta_path . is_file (): try : meta = self . _cacher . load ( meta_path ) except ValueError as e : raise RuntimeError ( f \"Failed to load meta file: { meta_path } \" ) from e landmarks = meta [ \"landmarks\" ] if \"landmarks\" in meta else None if \"bb\" in meta : # Crop the image using the precomputed bounding box x1 , y1 , x2 , y2 = meta [ \"bb\" ] image = x . image [ y1 : y2 , x1 : x2 ] new_x = ElementImage ( path = x . path , image = image , landmarks = landmarks , ) new_xs . append ( new_x ) else : raise ValueError ( f \"No bounding box found in { meta_path } \" ) else : cached = False try : bb , landmarks = self . process_element ( x . image ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if bb is None or len ( bb ) != 4 : raise RuntimeError ( f \"Failed to process { x . path } : returned { bb } \" ) # Make sure that the bounding box is always inside the image clipped_bb = ( max ( 0 , bb [ 0 ]), max ( 0 , bb [ 1 ]), min ( x . image . shape [ 1 ], bb [ 2 ]), min ( x . image . shape [ 0 ], bb [ 3 ]), ) x1 , y1 , x2 , y2 = clipped_bb new_x = ElementImage ( path = x . path , image = x . image [ y1 : y2 , x1 : x2 ], landmarks = landmarks , ) # Create the meta file meta_path . parent . mkdir ( parents = True , exist_ok = True ) if landmarks is not None : self . _cacher . save ( meta_path , bb = clipped_bb , landmarks = landmarks ) else : self . _cacher . save ( meta_path , bb = clipped_bb ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , )","title":"process()"},{"location":"reference/revelio/face_detection/detector/#revelio.face_detection.detector.FaceDetector.process_element","text":"Processes a single image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. Parameters: Name Type Description Default elem Image The image to process. required Returns: Type Description BoundingBox A tuple containing the bounding box (required) and the facial landmarks Optional [ Landmarks ] (optional). Source code in revelio/face_detection/detector.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @abstractmethod def process_element ( self , elem : Image ) -> tuple [ BoundingBox , Optional [ Landmarks ]]: \"\"\" Processes a single image and returns the bounding box of the face and the facial landmarks, if the algorithm supports such extraction, or else None. The bounding box is a tuple of 4 integers, representing the top-left and bottom-right coordinates of the bounding box, while the landmarks is a NumPy array of variable length, where each row represents a landmark and each column represents the x and y integer coordinates of the landmark, with origin at the top-left corner of the image. If no landmarks can be computed from the image (e.g. because the chosen face detection algorithm does not support landmark extraction), the returned value for the landmarks should be None. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A tuple containing the bounding box (required) and the facial landmarks (optional). \"\"\" raise NotImplementedError # pragma: no cover","title":"process_element()"},{"location":"reference/revelio/face_detection/dlib_detector/","text":"","title":"dlib_detector"},{"location":"reference/revelio/face_detection/mtcnn_detector/","text":"","title":"mtcnn_detector"},{"location":"reference/revelio/face_detection/opencv_detector/","text":"","title":"opencv_detector"},{"location":"reference/revelio/feature_extraction/","text":"","title":"feature_extraction"},{"location":"reference/revelio/feature_extraction/extractor/","text":"FeatureExtractor Bases: Registrable A feature extractor is responsible for extracting various types of features from images. A feature extractor must implement the process_element method, which takes an image and returns the features extracted from it. The features are stored in a dictionary, where the key is the name of the feature extractor (without any \"extractor\" suffixes; e.g. the features produced by a Wavelets Extractor are accessible via the wavelets key) and the value contains the features extracted by the feature extractor. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. The process method is responsible for loading the extracted features from the disk, if they have been already computed, or else calling process_element and saving the results. The user should not override this method, but instead implement process_element . Source code in revelio/feature_extraction/extractor.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class FeatureExtractor ( Registrable ): \"\"\" A feature extractor is responsible for extracting various types of features from images. A feature extractor must implement the `process_element` method, which takes an image and returns the features extracted from it. The features are stored in a dictionary, where the key is the name of the feature extractor (without any \"extractor\" suffixes; e.g. the features produced by a Wavelets Extractor are accessible via the `wavelets` key) and the value contains the features extracted by the feature extractor. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. The `process` method is responsible for loading the extracted features from the disk, if they have been already computed, or else calling `process_element` and saving the results. The user should not override this method, but instead implement `process_element`. \"\"\" def __init__ ( self , * , _config : Config ) -> None : self . _config = _config self . _cacher = ZstdCacher () def _get_features_path ( self , elem : DatasetElement , x_idx : int ) -> Path : output_path = Path ( self . _config . feature_extraction . output_path ) algorithm_name = type ( self ) . __name__ . lower () relative_img_path = elem . x [ x_idx ] . path . relative_to ( elem . dataset_root_path ) return ( output_path / algorithm_name / elem . original_dataset / relative_img_path . parent / f \" { relative_img_path . stem } .features.xz\" ) @abstractmethod def process_element ( self , elem : ElementImage ) -> np . ndarray : \"\"\" Processes a single image and returns its features. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A Numpy array containing the features extracted from the image. \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement , force_online : bool = False ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with the extracted features added to the `features` dictionary of each image. This method saves the extracted features to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. force_online: If True, the features are always computed online, even if they have been already computed and saved to the disk. Returns: A tuple containing the processed dataset element and a boolean indicating whether the feature extraction was loaded from cache. \"\"\" new_xs = [] cached = True algorithm_name = type ( self ) . __name__ . lower () algorithm_name = algorithm_name . replace ( \"extractor\" , \"\" ) for i , x in enumerate ( elem . x ): features_path = self . _get_features_path ( elem , i ) if features_path . is_file () and not force_online : try : features = self . _cacher . load ( features_path )[ \"features\" ] except ValueError as e : raise RuntimeError ( f \"Failed to load features: { features_path } \" ) from e new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) else : cached = False try : features = self . process_element ( x ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if features is None : raise RuntimeError ( f \"Failed to process { x . path } : returned None\" ) if not force_online : # We don't need to save the features if we're forced to do it online # (that means we have one or more augmentation steps) features_path . parent . mkdir ( parents = True , exist_ok = True ) self . _cacher . save ( features_path , features = features ) new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , ) process ( elem : DatasetElement , force_online : bool = False ) -> tuple [ DatasetElement , bool ] Processes a dataset element and returns an element with the same data, but with the extracted features added to the features dictionary of each image. This method saves the extracted features to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to process. required force_online bool If True, the features are always computed online, even if they have been already computed and saved to the disk. False Returns: Type Description DatasetElement A tuple containing the processed dataset element and a boolean indicating bool whether the feature extraction was loaded from cache. Source code in revelio/feature_extraction/extractor.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def process ( self , elem : DatasetElement , force_online : bool = False ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with the extracted features added to the `features` dictionary of each image. This method saves the extracted features to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. force_online: If True, the features are always computed online, even if they have been already computed and saved to the disk. Returns: A tuple containing the processed dataset element and a boolean indicating whether the feature extraction was loaded from cache. \"\"\" new_xs = [] cached = True algorithm_name = type ( self ) . __name__ . lower () algorithm_name = algorithm_name . replace ( \"extractor\" , \"\" ) for i , x in enumerate ( elem . x ): features_path = self . _get_features_path ( elem , i ) if features_path . is_file () and not force_online : try : features = self . _cacher . load ( features_path )[ \"features\" ] except ValueError as e : raise RuntimeError ( f \"Failed to load features: { features_path } \" ) from e new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) else : cached = False try : features = self . process_element ( x ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if features is None : raise RuntimeError ( f \"Failed to process { x . path } : returned None\" ) if not force_online : # We don't need to save the features if we're forced to do it online # (that means we have one or more augmentation steps) features_path . parent . mkdir ( parents = True , exist_ok = True ) self . _cacher . save ( features_path , features = features ) new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , ) process_element ( elem : ElementImage ) -> np . ndarray abstractmethod Processes a single image and returns its features. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. Parameters: Name Type Description Default elem ElementImage The image to process. required Returns: Type Description np . ndarray A Numpy array containing the features extracted from the image. Source code in revelio/feature_extraction/extractor.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @abstractmethod def process_element ( self , elem : ElementImage ) -> np . ndarray : \"\"\" Processes a single image and returns its features. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A Numpy array containing the features extracted from the image. \"\"\" raise NotImplementedError # pragma: no cover","title":"extractor"},{"location":"reference/revelio/feature_extraction/extractor/#revelio.feature_extraction.extractor.FeatureExtractor","text":"Bases: Registrable A feature extractor is responsible for extracting various types of features from images. A feature extractor must implement the process_element method, which takes an image and returns the features extracted from it. The features are stored in a dictionary, where the key is the name of the feature extractor (without any \"extractor\" suffixes; e.g. the features produced by a Wavelets Extractor are accessible via the wavelets key) and the value contains the features extracted by the feature extractor. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. The process method is responsible for loading the extracted features from the disk, if they have been already computed, or else calling process_element and saving the results. The user should not override this method, but instead implement process_element . Source code in revelio/feature_extraction/extractor.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class FeatureExtractor ( Registrable ): \"\"\" A feature extractor is responsible for extracting various types of features from images. A feature extractor must implement the `process_element` method, which takes an image and returns the features extracted from it. The features are stored in a dictionary, where the key is the name of the feature extractor (without any \"extractor\" suffixes; e.g. the features produced by a Wavelets Extractor are accessible via the `wavelets` key) and the value contains the features extracted by the feature extractor. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. The `process` method is responsible for loading the extracted features from the disk, if they have been already computed, or else calling `process_element` and saving the results. The user should not override this method, but instead implement `process_element`. \"\"\" def __init__ ( self , * , _config : Config ) -> None : self . _config = _config self . _cacher = ZstdCacher () def _get_features_path ( self , elem : DatasetElement , x_idx : int ) -> Path : output_path = Path ( self . _config . feature_extraction . output_path ) algorithm_name = type ( self ) . __name__ . lower () relative_img_path = elem . x [ x_idx ] . path . relative_to ( elem . dataset_root_path ) return ( output_path / algorithm_name / elem . original_dataset / relative_img_path . parent / f \" { relative_img_path . stem } .features.xz\" ) @abstractmethod def process_element ( self , elem : ElementImage ) -> np . ndarray : \"\"\" Processes a single image and returns its features. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A Numpy array containing the features extracted from the image. \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement , force_online : bool = False ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with the extracted features added to the `features` dictionary of each image. This method saves the extracted features to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. force_online: If True, the features are always computed online, even if they have been already computed and saved to the disk. Returns: A tuple containing the processed dataset element and a boolean indicating whether the feature extraction was loaded from cache. \"\"\" new_xs = [] cached = True algorithm_name = type ( self ) . __name__ . lower () algorithm_name = algorithm_name . replace ( \"extractor\" , \"\" ) for i , x in enumerate ( elem . x ): features_path = self . _get_features_path ( elem , i ) if features_path . is_file () and not force_online : try : features = self . _cacher . load ( features_path )[ \"features\" ] except ValueError as e : raise RuntimeError ( f \"Failed to load features: { features_path } \" ) from e new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) else : cached = False try : features = self . process_element ( x ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if features is None : raise RuntimeError ( f \"Failed to process { x . path } : returned None\" ) if not force_online : # We don't need to save the features if we're forced to do it online # (that means we have one or more augmentation steps) features_path . parent . mkdir ( parents = True , exist_ok = True ) self . _cacher . save ( features_path , features = features ) new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , )","title":"FeatureExtractor"},{"location":"reference/revelio/feature_extraction/extractor/#revelio.feature_extraction.extractor.FeatureExtractor.process","text":"Processes a dataset element and returns an element with the same data, but with the extracted features added to the features dictionary of each image. This method saves the extracted features to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to process. required force_online bool If True, the features are always computed online, even if they have been already computed and saved to the disk. False Returns: Type Description DatasetElement A tuple containing the processed dataset element and a boolean indicating bool whether the feature extraction was loaded from cache. Source code in revelio/feature_extraction/extractor.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def process ( self , elem : DatasetElement , force_online : bool = False ) -> tuple [ DatasetElement , bool ]: \"\"\" Processes a dataset element and returns an element with the same data, but with the extracted features added to the `features` dictionary of each image. This method saves the extracted features to the disk, so that they can be loaded later without having to recompute them. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to process. force_online: If True, the features are always computed online, even if they have been already computed and saved to the disk. Returns: A tuple containing the processed dataset element and a boolean indicating whether the feature extraction was loaded from cache. \"\"\" new_xs = [] cached = True algorithm_name = type ( self ) . __name__ . lower () algorithm_name = algorithm_name . replace ( \"extractor\" , \"\" ) for i , x in enumerate ( elem . x ): features_path = self . _get_features_path ( elem , i ) if features_path . is_file () and not force_online : try : features = self . _cacher . load ( features_path )[ \"features\" ] except ValueError as e : raise RuntimeError ( f \"Failed to load features: { features_path } \" ) from e new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) else : cached = False try : features = self . process_element ( x ) except Exception as e : raise RuntimeError ( f \"Failed to process { x . path } : { e } \" ) from e if features is None : raise RuntimeError ( f \"Failed to process { x . path } : returned None\" ) if not force_online : # We don't need to save the features if we're forced to do it online # (that means we have one or more augmentation steps) features_path . parent . mkdir ( parents = True , exist_ok = True ) self . _cacher . save ( features_path , features = features ) new_x = ElementImage ( path = x . path , image = x . image , features = { ** x . features , algorithm_name : features }, ) new_xs . append ( new_x ) return ( DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ), cached , )","title":"process()"},{"location":"reference/revelio/feature_extraction/extractor/#revelio.feature_extraction.extractor.FeatureExtractor.process_element","text":"Processes a single image and returns its features. If the process_element method raises an exception, the dataset element will be skipped and the exception will be logged. Parameters: Name Type Description Default elem ElementImage The image to process. required Returns: Type Description np . ndarray A Numpy array containing the features extracted from the image. Source code in revelio/feature_extraction/extractor.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @abstractmethod def process_element ( self , elem : ElementImage ) -> np . ndarray : \"\"\" Processes a single image and returns its features. If the `process_element` method raises an exception, the dataset element will be skipped and the exception will be logged. Args: elem: The image to process. Returns: A Numpy array containing the features extracted from the image. \"\"\" raise NotImplementedError # pragma: no cover","title":"process_element()"},{"location":"reference/revelio/feature_extraction/fourier_extractor/","text":"","title":"fourier_extractor"},{"location":"reference/revelio/feature_extraction/prnu_extractor/","text":"","title":"prnu_extractor"},{"location":"reference/revelio/feature_extraction/same_size_extractor/","text":"","title":"same_size_extractor"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/","text":"StationaryNode Bases: pywt . BaseNode StationaryWaveletPacket tree node. Subnodes are called a and d , just like approximation and detail coefficients in the Stationary Wavelet Transform. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class StationaryNode ( pywt . BaseNode ): # pragma: no cover \"\"\" StationaryWaveletPacket tree node. Subnodes are called `a` and `d`, just like approximation and detail coefficients in the Stationary Wavelet Transform. \"\"\" A = \"a\" D = \"d\" PARTS = A , D PART_LEN = 1 def _create_subnode ( self , part , data = None , overwrite = True ): return self . _create_subnode_base ( node_cls = StationaryNode , part = part , data = data , overwrite = overwrite ) def _decompose ( self ): \"\"\" See also -------- swt : for 1D Stationary Wavelet Transform output coefficients. \"\"\" if self . is_empty : data_a , data_d = None , None if self . _get_node ( self . A ) is None : self . _create_subnode ( self . A , data_a ) if self . _get_node ( self . D ) is None : self . _create_subnode ( self . D , data_d ) else : coeffs = pywt . swt ( self . data , self . wavelet , level = 1 , axis = self . axes ) data_a , data_d = coeffs [ 0 ], coeffs [ 1 ] self . _create_subnode ( self . A , data_a ) self . _create_subnode ( self . D , data_d ) return self . _get_node ( self . A ), self . _get_node ( self . D ) def _reconstruct ( self , update ): data_a , data_d = None , None node_a , node_d = self . _get_node ( self . A ), self . _get_node ( self . D ) if node_a is not None : data_a = node_a . reconstruct () # TODO: (update) ??? if node_d is not None : data_d = node_d . reconstruct () # TODO: (update) ??? if data_a is None and data_d is None : raise ValueError ( \"Node is a leaf node and cannot be reconstructed\" \" from subnodes.\" ) else : rec = pywt . iswt ([ data_a , data_d ], self . wavelet , axis = self . axes ) if self . _data_shape is not None and ( rec . shape != self . _data_shape ): rec = rec [ tuple ([ slice ( sz ) for sz in self . _data_shape ])] if update : self . data = rec return rec StationaryNode2D Bases: pywt . BaseNode WaveletPacket tree node. Subnodes are called 'a' (LL), 'h' (HL), 'v' (LH) and 'd' (HH), like approximation and detail coefficients in the 2D Stationary Wavelet Transform Source code in revelio/feature_extraction/stationary_wavelet_packets.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class StationaryNode2D ( pywt . BaseNode ): # pragma: no cover \"\"\" WaveletPacket tree node. Subnodes are called 'a' (LL), 'h' (HL), 'v' (LH) and 'd' (HH), like approximation and detail coefficients in the 2D Stationary Wavelet Transform \"\"\" LL = \"a\" HL = \"h\" LH = \"v\" HH = \"d\" PARTS = LL , HL , LH , HH PART_LEN = 1 def _create_subnode ( self , part , data = None , overwrite = True ): return self . _create_subnode_base ( node_cls = StationaryNode2D , part = part , data = data , overwrite = overwrite ) def _decompose ( self ): \"\"\" See also -------- dwt2 : for 2D Discrete Wavelet Transform output coefficients. \"\"\" if self . is_empty : data_ll , data_lh , data_hl , data_hh = None , None , None , None else : data_ll , ( data_hl , data_lh , data_hh ) = pywt . swt2 ( self . data , self . wavelet , level = 1 , axes = self . axes )[ 0 ] self . _create_subnode ( self . LL , data_ll ) self . _create_subnode ( self . LH , data_lh ) self . _create_subnode ( self . HL , data_hl ) self . _create_subnode ( self . HH , data_hh ) return ( self . _get_node ( self . LL ), self . _get_node ( self . HL ), self . _get_node ( self . LH ), self . _get_node ( self . HH ), ) def _reconstruct ( self , update ): data_ll , data_lh , data_hl , data_hh = None , None , None , None node_ll , node_lh , node_hl , node_hh = ( self . _get_node ( self . LL ), self . _get_node ( self . LH ), self . _get_node ( self . HL ), self . _get_node ( self . HH ), ) if node_ll is not None : data_ll = node_ll . reconstruct () if node_lh is not None : data_lh = node_lh . reconstruct () if node_hl is not None : data_hl = node_hl . reconstruct () if node_hh is not None : data_hh = node_hh . reconstruct () if data_ll is None and data_lh is None and data_hl is None and data_hh is None : raise ValueError ( \"Tree is missing data - all subnodes of ` %s ` node \" \"are None. Cannot reconstruct node.\" % self . path ) else : coeffs = [ data_ll , ( data_hl , data_lh , data_hh )] rec = pywt . iswt2 ( coeffs , self . wavelet , axes = self . axes ) if self . _data_shape is not None and ( rec . shape != self . _data_shape ): rec = rec [ tuple ([ slice ( sz ) for sz in self . _data_shape ])] if update : self . data = rec return rec def expand_2d_path ( self , path ): expanded_paths = { self . HH : \"hh\" , self . HL : \"hl\" , self . LH : \"lh\" , self . LL : \"ll\" } return ( \"\" . join ([ expanded_paths [ p ][ 0 ] for p in path ]), \"\" . join ([ expanded_paths [ p ][ 1 ] for p in path ]), ) StationaryNodeND Bases: pywt . BaseNode WaveletPacket tree node. Unlike Node and Node2D self.PARTS is a dictionary. For 1D: self.PARTS has keys 'a' and 'd' For 2D: self.PARTS has keys 'aa', 'ad', 'da', 'dd' For 3D: self.PARTS has keys 'aaa', 'aad', 'ada', 'daa', ..., 'ddd' Parameters parent Parent node. If parent is None then the node is considered detached (ie root). 1D or 2D array Data associated with the node. 1D or 2D numeric array, depending on the transform type. string A name identifying the coefficients type. See Node.node_name and Node2D.node_name for information on the accepted subnodes names. int The number of data dimensions. int The number of dimensions that are to be transformed. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 class StationaryNodeND ( pywt . BaseNode ): # pragma: no cover \"\"\" WaveletPacket tree node. Unlike Node and Node2D self.PARTS is a dictionary. For 1D: self.PARTS has keys 'a' and 'd' For 2D: self.PARTS has keys 'aa', 'ad', 'da', 'dd' For 3D: self.PARTS has keys 'aaa', 'aad', 'ada', 'daa', ..., 'ddd' Parameters ---------- parent : Parent node. If parent is None then the node is considered detached (ie root). data : 1D or 2D array Data associated with the node. 1D or 2D numeric array, depending on the transform type. node_name : string A name identifying the coefficients type. See `Node.node_name` and `Node2D.node_name` for information on the accepted subnodes names. ndim : int The number of data dimensions. ndim_transform : int The number of dimensions that are to be transformed. \"\"\" def __init__ ( self , parent , data , node_name , ndim , ndim_transform ): super () . __init__ ( parent = parent , data = data , node_name = node_name ) self . PART_LEN = ndim_transform self . PARTS = OrderedDict () for key in product ( * (( \"ad\" ,) * self . PART_LEN )): self . PARTS [ \"\" . join ( key )] = None self . ndim = ndim self . ndim_transform = ndim_transform def _init_subnodes ( self ): # need this empty so BaseNode's _init_subnodes isn't called during # __init__. We use a dictionary for PARTS instead for the nd case. pass def _get_node ( self , part ): return self . PARTS [ part ] def _set_node ( self , part , node ): if part not in self . PARTS : raise ValueError ( \"invalid part\" ) self . PARTS [ part ] = node def _delete_node ( self , part ): self . _set_node ( part , None ) def _validate_node_name ( self , part ): if part not in self . PARTS : raise ValueError ( \"Subnode name must be in [ %s ], not ' %s '.\" % ( \", \" . join ( \"' %s '\" % p for p in list ( self . PARTS . keys ())), part ) ) def _create_subnode ( self , part , data = None , overwrite = True ): return self . _create_subnode_base ( node_cls = StationaryNodeND , part = part , data = data , overwrite = overwrite , ndim = self . ndim , ndim_transform = self . ndim_transform , ) def _evaluate_maxlevel ( self , evaluate_from = \"parent\" ): \"\"\" Try to find the value of maximum decomposition level if it is not specified explicitly. Parameters ---------- evaluate_from : {'parent', 'subnodes'} \"\"\" assert evaluate_from in ( \"parent\" , \"subnodes\" ) if self . _maxlevel is not None : return self . _maxlevel elif self . data is not None : return self . level + pywt . swt_max_level ( min ( self . data . shape )) if evaluate_from == \"parent\" : if self . parent is not None : return self . parent . _evaluate_maxlevel ( evaluate_from ) elif evaluate_from == \"subnodes\" : for node in self . PARTS . values (): if node is not None : level = node . _evaluate_maxlevel ( evaluate_from ) if level is not None : return level return None def _decompose ( self ): \"\"\" See also -------- dwt2 : for 2D Discrete Wavelet Transform output coefficients. \"\"\" if self . is_empty : coefs = { key : None for key in self . PARTS . keys ()} else : coefs = pywt . swtn ( self . data , self . wavelet , level = 1 , axes = self . axes )[ 0 ] for key , data in coefs . items (): self . _create_subnode ( key , data ) return ( self . _get_node ( key ) for key in self . PARTS . keys ()) def _reconstruct ( self , update ): coeffs = { key : None for key in self . PARTS . keys ()} nnodes = 0 for key in self . PARTS . keys (): node = self . _get_node ( key ) if node is not None : nnodes += 1 coeffs [ key ] = node . reconstruct () if nnodes == 0 : raise ValueError ( \"Tree is missing data - all subnodes of ` %s ` node \" \"are None. Cannot reconstruct node.\" % self . path ) else : rec = pywt . iswtn ([ coeffs ], self . wavelet , axes = self . axes ) if update : self . data = rec return rec StationaryWaveletPacket Bases: StationaryNode Data structure representing Wavelet Packet decomposition of signal. Parameters 1D ndarray Original data (signal) Wavelet object or name string Wavelet used in DWT decomposition and reconstruction int, optional Maximum level of decomposition. If None, it will be calculated based on the wavelet and data length using pywt.dwt_max_level . int, optional The axis to transform. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 class StationaryWaveletPacket ( StationaryNode ): # pragma: no cover \"\"\" Data structure representing Wavelet Packet decomposition of signal. Parameters ---------- data : 1D ndarray Original data (signal) wavelet : Wavelet object or name string Wavelet used in DWT decomposition and reconstruction maxlevel : int, optional Maximum level of decomposition. If None, it will be calculated based on the `wavelet` and `data` length using `pywt.dwt_max_level`. axis : int, optional The axis to transform. \"\"\" def __init__ ( self , data , wavelet , maxlevel = None , axis =- 1 ): super () . __init__ ( None , data , \"\" ) if not isinstance ( wavelet , pywt . Wavelet ): wavelet = pywt . Wavelet ( wavelet ) self . wavelet = wavelet self . mode = None self . axes = axis # self.axes is just an integer for 1D transforms if data is not None : data = np . asarray ( data ) if self . axes < 0 : self . axes = self . axes + data . ndim if not 0 <= self . axes < data . ndim : raise ValueError ( \"Axis greater than data dimensions\" ) self . data_size = data . shape if maxlevel is None : maxlevel = pywt . swt_max_level ( data . shape [ self . axes ]) else : self . data_size = None self . _maxlevel = maxlevel def __reduce__ ( self ): return ( StationaryWaveletPacket , ( self . data , self . wavelet , self . mode , self . maxlevel ), ) def reconstruct ( self , update = True ): \"\"\" Reconstruct data value using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default), then data values will be replaced by reconstruction values, also in subnodes. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes on the specified level. Parameters ---------- level : int Specifies decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional - \"natural\" - left to right in tree (default) - \"freq\" - band ordered decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- If nodes at the given level are missing (i.e. the tree is partially decomposed) and `decompose` is set to False, only existing nodes will be returned. Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"natural\" : return result elif order == \"freq\" : result = { node . path : node for node in result } graycode_order = get_graycode_order ( level ) return [ result [ path ] for path in graycode_order if path in result ] else : raise ValueError ( \"Invalid order name - %s .\" % order ) get_level ( level , order = 'natural' , decompose = True ) Returns all nodes on the specified level. Parameters int Specifies decomposition level from which the nodes will be collected. {'natural', 'freq'}, optional \"natural\" - left to right in tree (default) \"freq\" - band ordered bool, optional If set then the method will try to decompose the data up to the specified level (default: True). Notes If nodes at the given level are missing (i.e. the tree is partially decomposed) and decompose is set to False, only existing nodes will be returned. Frequency order ( order=\"freq\" ) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1] , [2] . References ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 Source code in revelio/feature_extraction/stationary_wavelet_packets.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes on the specified level. Parameters ---------- level : int Specifies decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional - \"natural\" - left to right in tree (default) - \"freq\" - band ordered decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- If nodes at the given level are missing (i.e. the tree is partially decomposed) and `decompose` is set to False, only existing nodes will be returned. Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"natural\" : return result elif order == \"freq\" : result = { node . path : node for node in result } graycode_order = get_graycode_order ( level ) return [ result [ path ] for path in graycode_order if path in result ] else : raise ValueError ( \"Invalid order name - %s .\" % order ) reconstruct ( update = True ) Reconstruct data value using coefficients from subnodes. Parameters bool, optional If True (default), then data values will be replaced by reconstruction values, also in subnodes. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def reconstruct ( self , update = True ): \"\"\" Reconstruct data value using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default), then data values will be replaced by reconstruction values, also in subnodes. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data StationaryWaveletPacket2D Bases: StationaryNode2D Data structure representing 2D Wavelet Packet decomposition of signal. Parameters 2D ndarray Data associated with the node. Wavelet object or name string Wavelet used in DWT decomposition and reconstruction int Maximum level of decomposition. If None, it will be calculated based on the wavelet and data length using pywt.dwt_max_level . 2-tuple of ints, optional The axes that will be transformed. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 class StationaryWaveletPacket2D ( StationaryNode2D ): # pragma: no cover \"\"\" Data structure representing 2D Wavelet Packet decomposition of signal. Parameters ---------- data : 2D ndarray Data associated with the node. wavelet : Wavelet object or name string Wavelet used in DWT decomposition and reconstruction maxlevel : int Maximum level of decomposition. If None, it will be calculated based on the `wavelet` and `data` length using `pywt.dwt_max_level`. axes : 2-tuple of ints, optional The axes that will be transformed. \"\"\" def __init__ ( self , data , wavelet , maxlevel = None , axes = ( - 2 , - 1 )): super () . __init__ ( None , data , \"\" ) if not isinstance ( wavelet , pywt . Wavelet ): wavelet = pywt . Wavelet ( wavelet ) self . wavelet = wavelet self . mode = None self . axes = tuple ( axes ) if len ( np . unique ( self . axes )) != 2 : raise ValueError ( \"Expected two unique axes.\" ) if data is not None : data = np . asarray ( data ) if data . ndim < 2 : raise ValueError ( \"WaveletPacket2D requires data with 2 or more dimensions.\" ) self . data_size = data . shape transform_size = [ data . shape [ ax ] for ax in self . axes ] if maxlevel is None : maxlevel = pywt . swt_max_level ( min ( transform_size )) else : self . data_size = None self . _maxlevel = maxlevel def __reduce__ ( self ): return ( StationaryWaveletPacket2D , ( self . data , self . wavelet , self . mode , self . maxlevel ), ) def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional If `natural` (default) a flat list is returned. If `freq`, a 2d structure with rows and cols sorted by corresponding dimension frequency of 2d coefficient array (adapted from 1d case). decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"freq\" : nodes = {} for ( row_path , col_path ), node in [ ( self . expand_2d_path ( node . path ), node ) for node in result ]: nodes . setdefault ( row_path , {})[ col_path ] = node graycode_order = get_graycode_order ( level , x = \"l\" , y = \"h\" ) nodes = [ nodes [ path ] for path in graycode_order if path in nodes ] result = [] for row in nodes : result . append ([ row [ path ] for path in graycode_order if path in row ]) return result get_level ( level , order = 'natural' , decompose = True ) Returns all nodes from specified level. Parameters int Decomposition level from which the nodes will be collected. {'natural', 'freq'}, optional If natural (default) a flat list is returned. If freq , a 2d structure with rows and cols sorted by corresponding dimension frequency of 2d coefficient array (adapted from 1d case). bool, optional If set then the method will try to decompose the data up to the specified level (default: True). Notes Frequency order ( order=\"freq\" ) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1] , [2] . References ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 Source code in revelio/feature_extraction/stationary_wavelet_packets.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional If `natural` (default) a flat list is returned. If `freq`, a 2d structure with rows and cols sorted by corresponding dimension frequency of 2d coefficient array (adapted from 1d case). decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"freq\" : nodes = {} for ( row_path , col_path ), node in [ ( self . expand_2d_path ( node . path ), node ) for node in result ]: nodes . setdefault ( row_path , {})[ col_path ] = node graycode_order = get_graycode_order ( level , x = \"l\" , y = \"h\" ) nodes = [ nodes [ path ] for path in graycode_order if path in nodes ] result = [] for row in nodes : result . append ([ row [ path ] for path in graycode_order if path in row ]) return result reconstruct ( update = True ) Reconstruct data using coefficients from subnodes. Parameters bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data StationaryWaveletPacketND Bases: StationaryNodeND Data structure representing ND Wavelet Packet decomposition of signal. Parameters ND ndarray Data associated with the node. Wavelet object or name string Wavelet used in DWT decomposition and reconstruction int, optional Maximum level of decomposition. If None, it will be calculated based on the wavelet and data length using pywt.dwt_max_level . tuple of int, optional The axes to transform. The default value of None corresponds to all axes. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 class StationaryWaveletPacketND ( StationaryNodeND ): # pragma: no cover \"\"\" Data structure representing ND Wavelet Packet decomposition of signal. Parameters ---------- data : ND ndarray Data associated with the node. wavelet : Wavelet object or name string Wavelet used in DWT decomposition and reconstruction maxlevel : int, optional Maximum level of decomposition. If None, it will be calculated based on the `wavelet` and `data` length using `pywt.dwt_max_level`. axes : tuple of int, optional The axes to transform. The default value of `None` corresponds to all axes. \"\"\" def __init__ ( self , data , wavelet , maxlevel = None , axes = None ): if ( data is None ) and ( axes is None ): # ndim is required to create a NodeND object raise ValueError ( \"If data is None, axes must be specified\" ) # axes determines the number of transform dimensions if axes is None : axes = range ( data . ndim ) elif np . isscalar ( axes ): axes = ( axes ,) axes = tuple ( axes ) if len ( np . unique ( axes )) != len ( axes ): raise ValueError ( \"Expected a set of unique axes.\" ) ndim_transform = len ( axes ) if data is not None : data = np . asarray ( data ) if data . ndim == 0 : raise ValueError ( \"data must be at least 1D\" ) ndim = data . ndim else : ndim = len ( axes ) super () . __init__ ( None , data , \"\" , ndim , ndim_transform ) if not isinstance ( wavelet , pywt . Wavelet ): wavelet = pywt . Wavelet ( wavelet ) self . wavelet = wavelet self . mode = None self . axes = axes self . ndim_transform = ndim_transform if data is not None : if data . ndim < len ( axes ): raise ValueError ( \"The number of axes exceeds the number of \" \"data dimensions.\" ) self . data_size = data . shape transform_size = [ data . shape [ ax ] for ax in self . axes ] if maxlevel is None : maxlevel = pywt . swt_max_level ( min ( transform_size )) else : self . data_size = None self . _maxlevel = maxlevel def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data def get_level ( self , level , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). \"\"\" if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) return result get_level ( level , decompose = True ) Returns all nodes from specified level. Parameters int Decomposition level from which the nodes will be collected. bool, optional If set then the method will try to decompose the data up to the specified level (default: True). Source code in revelio/feature_extraction/stationary_wavelet_packets.py 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 def get_level ( self , level , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). \"\"\" if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) return result reconstruct ( update = True ) Reconstruct data using coefficients from subnodes. Parameters bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data","title":"stationary_wavelet_packets"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryNode","text":"Bases: pywt . BaseNode StationaryWaveletPacket tree node. Subnodes are called a and d , just like approximation and detail coefficients in the Stationary Wavelet Transform. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class StationaryNode ( pywt . BaseNode ): # pragma: no cover \"\"\" StationaryWaveletPacket tree node. Subnodes are called `a` and `d`, just like approximation and detail coefficients in the Stationary Wavelet Transform. \"\"\" A = \"a\" D = \"d\" PARTS = A , D PART_LEN = 1 def _create_subnode ( self , part , data = None , overwrite = True ): return self . _create_subnode_base ( node_cls = StationaryNode , part = part , data = data , overwrite = overwrite ) def _decompose ( self ): \"\"\" See also -------- swt : for 1D Stationary Wavelet Transform output coefficients. \"\"\" if self . is_empty : data_a , data_d = None , None if self . _get_node ( self . A ) is None : self . _create_subnode ( self . A , data_a ) if self . _get_node ( self . D ) is None : self . _create_subnode ( self . D , data_d ) else : coeffs = pywt . swt ( self . data , self . wavelet , level = 1 , axis = self . axes ) data_a , data_d = coeffs [ 0 ], coeffs [ 1 ] self . _create_subnode ( self . A , data_a ) self . _create_subnode ( self . D , data_d ) return self . _get_node ( self . A ), self . _get_node ( self . D ) def _reconstruct ( self , update ): data_a , data_d = None , None node_a , node_d = self . _get_node ( self . A ), self . _get_node ( self . D ) if node_a is not None : data_a = node_a . reconstruct () # TODO: (update) ??? if node_d is not None : data_d = node_d . reconstruct () # TODO: (update) ??? if data_a is None and data_d is None : raise ValueError ( \"Node is a leaf node and cannot be reconstructed\" \" from subnodes.\" ) else : rec = pywt . iswt ([ data_a , data_d ], self . wavelet , axis = self . axes ) if self . _data_shape is not None and ( rec . shape != self . _data_shape ): rec = rec [ tuple ([ slice ( sz ) for sz in self . _data_shape ])] if update : self . data = rec return rec","title":"StationaryNode"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryNode2D","text":"Bases: pywt . BaseNode WaveletPacket tree node. Subnodes are called 'a' (LL), 'h' (HL), 'v' (LH) and 'd' (HH), like approximation and detail coefficients in the 2D Stationary Wavelet Transform Source code in revelio/feature_extraction/stationary_wavelet_packets.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class StationaryNode2D ( pywt . BaseNode ): # pragma: no cover \"\"\" WaveletPacket tree node. Subnodes are called 'a' (LL), 'h' (HL), 'v' (LH) and 'd' (HH), like approximation and detail coefficients in the 2D Stationary Wavelet Transform \"\"\" LL = \"a\" HL = \"h\" LH = \"v\" HH = \"d\" PARTS = LL , HL , LH , HH PART_LEN = 1 def _create_subnode ( self , part , data = None , overwrite = True ): return self . _create_subnode_base ( node_cls = StationaryNode2D , part = part , data = data , overwrite = overwrite ) def _decompose ( self ): \"\"\" See also -------- dwt2 : for 2D Discrete Wavelet Transform output coefficients. \"\"\" if self . is_empty : data_ll , data_lh , data_hl , data_hh = None , None , None , None else : data_ll , ( data_hl , data_lh , data_hh ) = pywt . swt2 ( self . data , self . wavelet , level = 1 , axes = self . axes )[ 0 ] self . _create_subnode ( self . LL , data_ll ) self . _create_subnode ( self . LH , data_lh ) self . _create_subnode ( self . HL , data_hl ) self . _create_subnode ( self . HH , data_hh ) return ( self . _get_node ( self . LL ), self . _get_node ( self . HL ), self . _get_node ( self . LH ), self . _get_node ( self . HH ), ) def _reconstruct ( self , update ): data_ll , data_lh , data_hl , data_hh = None , None , None , None node_ll , node_lh , node_hl , node_hh = ( self . _get_node ( self . LL ), self . _get_node ( self . LH ), self . _get_node ( self . HL ), self . _get_node ( self . HH ), ) if node_ll is not None : data_ll = node_ll . reconstruct () if node_lh is not None : data_lh = node_lh . reconstruct () if node_hl is not None : data_hl = node_hl . reconstruct () if node_hh is not None : data_hh = node_hh . reconstruct () if data_ll is None and data_lh is None and data_hl is None and data_hh is None : raise ValueError ( \"Tree is missing data - all subnodes of ` %s ` node \" \"are None. Cannot reconstruct node.\" % self . path ) else : coeffs = [ data_ll , ( data_hl , data_lh , data_hh )] rec = pywt . iswt2 ( coeffs , self . wavelet , axes = self . axes ) if self . _data_shape is not None and ( rec . shape != self . _data_shape ): rec = rec [ tuple ([ slice ( sz ) for sz in self . _data_shape ])] if update : self . data = rec return rec def expand_2d_path ( self , path ): expanded_paths = { self . HH : \"hh\" , self . HL : \"hl\" , self . LH : \"lh\" , self . LL : \"ll\" } return ( \"\" . join ([ expanded_paths [ p ][ 0 ] for p in path ]), \"\" . join ([ expanded_paths [ p ][ 1 ] for p in path ]), )","title":"StationaryNode2D"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryNodeND","text":"Bases: pywt . BaseNode WaveletPacket tree node. Unlike Node and Node2D self.PARTS is a dictionary. For 1D: self.PARTS has keys 'a' and 'd' For 2D: self.PARTS has keys 'aa', 'ad', 'da', 'dd' For 3D: self.PARTS has keys 'aaa', 'aad', 'ada', 'daa', ..., 'ddd'","title":"StationaryNodeND"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryNodeND--parameters","text":"parent Parent node. If parent is None then the node is considered detached (ie root). 1D or 2D array Data associated with the node. 1D or 2D numeric array, depending on the transform type. string A name identifying the coefficients type. See Node.node_name and Node2D.node_name for information on the accepted subnodes names. int The number of data dimensions. int The number of dimensions that are to be transformed. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 class StationaryNodeND ( pywt . BaseNode ): # pragma: no cover \"\"\" WaveletPacket tree node. Unlike Node and Node2D self.PARTS is a dictionary. For 1D: self.PARTS has keys 'a' and 'd' For 2D: self.PARTS has keys 'aa', 'ad', 'da', 'dd' For 3D: self.PARTS has keys 'aaa', 'aad', 'ada', 'daa', ..., 'ddd' Parameters ---------- parent : Parent node. If parent is None then the node is considered detached (ie root). data : 1D or 2D array Data associated with the node. 1D or 2D numeric array, depending on the transform type. node_name : string A name identifying the coefficients type. See `Node.node_name` and `Node2D.node_name` for information on the accepted subnodes names. ndim : int The number of data dimensions. ndim_transform : int The number of dimensions that are to be transformed. \"\"\" def __init__ ( self , parent , data , node_name , ndim , ndim_transform ): super () . __init__ ( parent = parent , data = data , node_name = node_name ) self . PART_LEN = ndim_transform self . PARTS = OrderedDict () for key in product ( * (( \"ad\" ,) * self . PART_LEN )): self . PARTS [ \"\" . join ( key )] = None self . ndim = ndim self . ndim_transform = ndim_transform def _init_subnodes ( self ): # need this empty so BaseNode's _init_subnodes isn't called during # __init__. We use a dictionary for PARTS instead for the nd case. pass def _get_node ( self , part ): return self . PARTS [ part ] def _set_node ( self , part , node ): if part not in self . PARTS : raise ValueError ( \"invalid part\" ) self . PARTS [ part ] = node def _delete_node ( self , part ): self . _set_node ( part , None ) def _validate_node_name ( self , part ): if part not in self . PARTS : raise ValueError ( \"Subnode name must be in [ %s ], not ' %s '.\" % ( \", \" . join ( \"' %s '\" % p for p in list ( self . PARTS . keys ())), part ) ) def _create_subnode ( self , part , data = None , overwrite = True ): return self . _create_subnode_base ( node_cls = StationaryNodeND , part = part , data = data , overwrite = overwrite , ndim = self . ndim , ndim_transform = self . ndim_transform , ) def _evaluate_maxlevel ( self , evaluate_from = \"parent\" ): \"\"\" Try to find the value of maximum decomposition level if it is not specified explicitly. Parameters ---------- evaluate_from : {'parent', 'subnodes'} \"\"\" assert evaluate_from in ( \"parent\" , \"subnodes\" ) if self . _maxlevel is not None : return self . _maxlevel elif self . data is not None : return self . level + pywt . swt_max_level ( min ( self . data . shape )) if evaluate_from == \"parent\" : if self . parent is not None : return self . parent . _evaluate_maxlevel ( evaluate_from ) elif evaluate_from == \"subnodes\" : for node in self . PARTS . values (): if node is not None : level = node . _evaluate_maxlevel ( evaluate_from ) if level is not None : return level return None def _decompose ( self ): \"\"\" See also -------- dwt2 : for 2D Discrete Wavelet Transform output coefficients. \"\"\" if self . is_empty : coefs = { key : None for key in self . PARTS . keys ()} else : coefs = pywt . swtn ( self . data , self . wavelet , level = 1 , axes = self . axes )[ 0 ] for key , data in coefs . items (): self . _create_subnode ( key , data ) return ( self . _get_node ( key ) for key in self . PARTS . keys ()) def _reconstruct ( self , update ): coeffs = { key : None for key in self . PARTS . keys ()} nnodes = 0 for key in self . PARTS . keys (): node = self . _get_node ( key ) if node is not None : nnodes += 1 coeffs [ key ] = node . reconstruct () if nnodes == 0 : raise ValueError ( \"Tree is missing data - all subnodes of ` %s ` node \" \"are None. Cannot reconstruct node.\" % self . path ) else : rec = pywt . iswtn ([ coeffs ], self . wavelet , axes = self . axes ) if update : self . data = rec return rec","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket","text":"Bases: StationaryNode Data structure representing Wavelet Packet decomposition of signal.","title":"StationaryWaveletPacket"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket--parameters","text":"1D ndarray Original data (signal) Wavelet object or name string Wavelet used in DWT decomposition and reconstruction int, optional Maximum level of decomposition. If None, it will be calculated based on the wavelet and data length using pywt.dwt_max_level . int, optional The axis to transform. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 class StationaryWaveletPacket ( StationaryNode ): # pragma: no cover \"\"\" Data structure representing Wavelet Packet decomposition of signal. Parameters ---------- data : 1D ndarray Original data (signal) wavelet : Wavelet object or name string Wavelet used in DWT decomposition and reconstruction maxlevel : int, optional Maximum level of decomposition. If None, it will be calculated based on the `wavelet` and `data` length using `pywt.dwt_max_level`. axis : int, optional The axis to transform. \"\"\" def __init__ ( self , data , wavelet , maxlevel = None , axis =- 1 ): super () . __init__ ( None , data , \"\" ) if not isinstance ( wavelet , pywt . Wavelet ): wavelet = pywt . Wavelet ( wavelet ) self . wavelet = wavelet self . mode = None self . axes = axis # self.axes is just an integer for 1D transforms if data is not None : data = np . asarray ( data ) if self . axes < 0 : self . axes = self . axes + data . ndim if not 0 <= self . axes < data . ndim : raise ValueError ( \"Axis greater than data dimensions\" ) self . data_size = data . shape if maxlevel is None : maxlevel = pywt . swt_max_level ( data . shape [ self . axes ]) else : self . data_size = None self . _maxlevel = maxlevel def __reduce__ ( self ): return ( StationaryWaveletPacket , ( self . data , self . wavelet , self . mode , self . maxlevel ), ) def reconstruct ( self , update = True ): \"\"\" Reconstruct data value using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default), then data values will be replaced by reconstruction values, also in subnodes. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes on the specified level. Parameters ---------- level : int Specifies decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional - \"natural\" - left to right in tree (default) - \"freq\" - band ordered decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- If nodes at the given level are missing (i.e. the tree is partially decomposed) and `decompose` is set to False, only existing nodes will be returned. Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"natural\" : return result elif order == \"freq\" : result = { node . path : node for node in result } graycode_order = get_graycode_order ( level ) return [ result [ path ] for path in graycode_order if path in result ] else : raise ValueError ( \"Invalid order name - %s .\" % order )","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket.get_level","text":"Returns all nodes on the specified level.","title":"get_level()"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket.get_level--parameters","text":"int Specifies decomposition level from which the nodes will be collected. {'natural', 'freq'}, optional \"natural\" - left to right in tree (default) \"freq\" - band ordered bool, optional If set then the method will try to decompose the data up to the specified level (default: True).","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket.get_level--notes","text":"If nodes at the given level are missing (i.e. the tree is partially decomposed) and decompose is set to False, only existing nodes will be returned. Frequency order ( order=\"freq\" ) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1] , [2] .","title":"Notes"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket.get_level--references","text":"..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 Source code in revelio/feature_extraction/stationary_wavelet_packets.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes on the specified level. Parameters ---------- level : int Specifies decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional - \"natural\" - left to right in tree (default) - \"freq\" - band ordered decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- If nodes at the given level are missing (i.e. the tree is partially decomposed) and `decompose` is set to False, only existing nodes will be returned. Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"natural\" : return result elif order == \"freq\" : result = { node . path : node for node in result } graycode_order = get_graycode_order ( level ) return [ result [ path ] for path in graycode_order if path in result ] else : raise ValueError ( \"Invalid order name - %s .\" % order )","title":"References"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket.reconstruct","text":"Reconstruct data value using coefficients from subnodes.","title":"reconstruct()"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket.reconstruct--parameters","text":"bool, optional If True (default), then data values will be replaced by reconstruction values, also in subnodes. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def reconstruct ( self , update = True ): \"\"\" Reconstruct data value using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default), then data values will be replaced by reconstruction values, also in subnodes. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D","text":"Bases: StationaryNode2D Data structure representing 2D Wavelet Packet decomposition of signal.","title":"StationaryWaveletPacket2D"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D--parameters","text":"2D ndarray Data associated with the node. Wavelet object or name string Wavelet used in DWT decomposition and reconstruction int Maximum level of decomposition. If None, it will be calculated based on the wavelet and data length using pywt.dwt_max_level . 2-tuple of ints, optional The axes that will be transformed. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 class StationaryWaveletPacket2D ( StationaryNode2D ): # pragma: no cover \"\"\" Data structure representing 2D Wavelet Packet decomposition of signal. Parameters ---------- data : 2D ndarray Data associated with the node. wavelet : Wavelet object or name string Wavelet used in DWT decomposition and reconstruction maxlevel : int Maximum level of decomposition. If None, it will be calculated based on the `wavelet` and `data` length using `pywt.dwt_max_level`. axes : 2-tuple of ints, optional The axes that will be transformed. \"\"\" def __init__ ( self , data , wavelet , maxlevel = None , axes = ( - 2 , - 1 )): super () . __init__ ( None , data , \"\" ) if not isinstance ( wavelet , pywt . Wavelet ): wavelet = pywt . Wavelet ( wavelet ) self . wavelet = wavelet self . mode = None self . axes = tuple ( axes ) if len ( np . unique ( self . axes )) != 2 : raise ValueError ( \"Expected two unique axes.\" ) if data is not None : data = np . asarray ( data ) if data . ndim < 2 : raise ValueError ( \"WaveletPacket2D requires data with 2 or more dimensions.\" ) self . data_size = data . shape transform_size = [ data . shape [ ax ] for ax in self . axes ] if maxlevel is None : maxlevel = pywt . swt_max_level ( min ( transform_size )) else : self . data_size = None self . _maxlevel = maxlevel def __reduce__ ( self ): return ( StationaryWaveletPacket2D , ( self . data , self . wavelet , self . mode , self . maxlevel ), ) def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional If `natural` (default) a flat list is returned. If `freq`, a 2d structure with rows and cols sorted by corresponding dimension frequency of 2d coefficient array (adapted from 1d case). decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"freq\" : nodes = {} for ( row_path , col_path ), node in [ ( self . expand_2d_path ( node . path ), node ) for node in result ]: nodes . setdefault ( row_path , {})[ col_path ] = node graycode_order = get_graycode_order ( level , x = \"l\" , y = \"h\" ) nodes = [ nodes [ path ] for path in graycode_order if path in nodes ] result = [] for row in nodes : result . append ([ row [ path ] for path in graycode_order if path in row ]) return result","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D.get_level","text":"Returns all nodes from specified level.","title":"get_level()"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D.get_level--parameters","text":"int Decomposition level from which the nodes will be collected. {'natural', 'freq'}, optional If natural (default) a flat list is returned. If freq , a 2d structure with rows and cols sorted by corresponding dimension frequency of 2d coefficient array (adapted from 1d case). bool, optional If set then the method will try to decompose the data up to the specified level (default: True).","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D.get_level--notes","text":"Frequency order ( order=\"freq\" ) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1] , [2] .","title":"Notes"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D.get_level--references","text":"..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 Source code in revelio/feature_extraction/stationary_wavelet_packets.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def get_level ( self , level , order = \"natural\" , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. order : {'natural', 'freq'}, optional If `natural` (default) a flat list is returned. If `freq`, a 2d structure with rows and cols sorted by corresponding dimension frequency of 2d coefficient array (adapted from 1d case). decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). Notes ----- Frequency order (``order=\"freq\"``) is also known as as sequency order and \"natural\" order is sometimes referred to as Paley order. A detailed discussion of these orderings is also given in [1]_, [2]_. References ---------- ..[1] M.V. Wickerhauser. Adapted Wavelet Analysis from Theory to Software. Wellesley. Massachusetts: A K Peters. 1994. ..[2] D.B. Percival and A.T. Walden. Wavelet Methods for Time Series Analysis. Cambridge University Press. 2000. DOI:10.1017/CBO9780511841040 \"\"\" if order not in [ \"natural\" , \"freq\" ]: raise ValueError ( f \"Invalid order: { order } \" ) if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) if order == \"freq\" : nodes = {} for ( row_path , col_path ), node in [ ( self . expand_2d_path ( node . path ), node ) for node in result ]: nodes . setdefault ( row_path , {})[ col_path ] = node graycode_order = get_graycode_order ( level , x = \"l\" , y = \"h\" ) nodes = [ nodes [ path ] for path in graycode_order if path in nodes ] result = [] for row in nodes : result . append ([ row [ path ] for path in graycode_order if path in row ]) return result","title":"References"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D.reconstruct","text":"Reconstruct data using coefficients from subnodes.","title":"reconstruct()"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacket2D.reconstruct--parameters","text":"bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacketND","text":"Bases: StationaryNodeND Data structure representing ND Wavelet Packet decomposition of signal.","title":"StationaryWaveletPacketND"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacketND--parameters","text":"ND ndarray Data associated with the node. Wavelet object or name string Wavelet used in DWT decomposition and reconstruction int, optional Maximum level of decomposition. If None, it will be calculated based on the wavelet and data length using pywt.dwt_max_level . tuple of int, optional The axes to transform. The default value of None corresponds to all axes. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 class StationaryWaveletPacketND ( StationaryNodeND ): # pragma: no cover \"\"\" Data structure representing ND Wavelet Packet decomposition of signal. Parameters ---------- data : ND ndarray Data associated with the node. wavelet : Wavelet object or name string Wavelet used in DWT decomposition and reconstruction maxlevel : int, optional Maximum level of decomposition. If None, it will be calculated based on the `wavelet` and `data` length using `pywt.dwt_max_level`. axes : tuple of int, optional The axes to transform. The default value of `None` corresponds to all axes. \"\"\" def __init__ ( self , data , wavelet , maxlevel = None , axes = None ): if ( data is None ) and ( axes is None ): # ndim is required to create a NodeND object raise ValueError ( \"If data is None, axes must be specified\" ) # axes determines the number of transform dimensions if axes is None : axes = range ( data . ndim ) elif np . isscalar ( axes ): axes = ( axes ,) axes = tuple ( axes ) if len ( np . unique ( axes )) != len ( axes ): raise ValueError ( \"Expected a set of unique axes.\" ) ndim_transform = len ( axes ) if data is not None : data = np . asarray ( data ) if data . ndim == 0 : raise ValueError ( \"data must be at least 1D\" ) ndim = data . ndim else : ndim = len ( axes ) super () . __init__ ( None , data , \"\" , ndim , ndim_transform ) if not isinstance ( wavelet , pywt . Wavelet ): wavelet = pywt . Wavelet ( wavelet ) self . wavelet = wavelet self . mode = None self . axes = axes self . ndim_transform = ndim_transform if data is not None : if data . ndim < len ( axes ): raise ValueError ( \"The number of axes exceeds the number of \" \"data dimensions.\" ) self . data_size = data . shape transform_size = [ data . shape [ ax ] for ax in self . axes ] if maxlevel is None : maxlevel = pywt . swt_max_level ( min ( transform_size )) else : self . data_size = None self . _maxlevel = maxlevel def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data def get_level ( self , level , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). \"\"\" if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) return result","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacketND.get_level","text":"Returns all nodes from specified level.","title":"get_level()"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacketND.get_level--parameters","text":"int Decomposition level from which the nodes will be collected. bool, optional If set then the method will try to decompose the data up to the specified level (default: True). Source code in revelio/feature_extraction/stationary_wavelet_packets.py 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 def get_level ( self , level , decompose = True ): \"\"\" Returns all nodes from specified level. Parameters ---------- level : int Decomposition `level` from which the nodes will be collected. decompose : bool, optional If set then the method will try to decompose the data up to the specified `level` (default: True). \"\"\" if level > self . maxlevel : raise ValueError ( \"The level cannot be greater than the maximum\" \" decomposition level value ( %d )\" % self . maxlevel ) result = [] def collect ( node ): if node . level == level : result . append ( node ) return False return True self . walk ( collect , decompose = decompose ) return result","title":"Parameters"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacketND.reconstruct","text":"Reconstruct data using coefficients from subnodes.","title":"reconstruct()"},{"location":"reference/revelio/feature_extraction/stationary_wavelet_packets/#revelio.feature_extraction.stationary_wavelet_packets.StationaryWaveletPacketND.reconstruct--parameters","text":"bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. Source code in revelio/feature_extraction/stationary_wavelet_packets.py 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 def reconstruct ( self , update = True ): \"\"\" Reconstruct data using coefficients from subnodes. Parameters ---------- update : bool, optional If True (default) then the coefficients of the current node and its subnodes will be replaced with values from reconstruction. \"\"\" if self . has_any_subnode : data = super () . reconstruct ( update ) if self . data_size is not None and ( data . shape != self . data_size ): data = data [[ slice ( sz ) for sz in self . data_size ]] if update : self . data = data return data return self . data # return original data","title":"Parameters"},{"location":"reference/revelio/feature_extraction/wavelets_extractor/","text":"","title":"wavelets_extractor"},{"location":"reference/revelio/model/","text":"","title":"model"},{"location":"reference/revelio/model/model/","text":"","title":"model"},{"location":"reference/revelio/model/random_guess/","text":"","title":"random_guess"},{"location":"reference/revelio/model/metrics/","text":"","title":"metrics"},{"location":"reference/revelio/model/metrics/accuracy/","text":"","title":"accuracy"},{"location":"reference/revelio/model/metrics/bpcer_at_apcer/","text":"","title":"bpcer_at_apcer"},{"location":"reference/revelio/model/metrics/eer/","text":"","title":"eer"},{"location":"reference/revelio/model/metrics/metric/","text":"","title":"metric"},{"location":"reference/revelio/model/metrics/tnr/","text":"","title":"tnr"},{"location":"reference/revelio/model/metrics/tpr/","text":"","title":"tpr"},{"location":"reference/revelio/model/metrics/utils/","text":"","title":"utils"},{"location":"reference/revelio/model/nn/","text":"","title":"nn"},{"location":"reference/revelio/model/nn/alexnet/","text":"","title":"alexnet"},{"location":"reference/revelio/model/nn/feature_inception_resnet/","text":"","title":"feature_inception_resnet"},{"location":"reference/revelio/model/nn/inception_resnet/","text":"","title":"inception_resnet"},{"location":"reference/revelio/model/nn/mobilenet/","text":"","title":"mobilenet"},{"location":"reference/revelio/model/nn/neuralnet/","text":"","title":"neuralnet"},{"location":"reference/revelio/model/nn/resnet/","text":"","title":"resnet"},{"location":"reference/revelio/model/nn/squeezenet/","text":"","title":"squeezenet"},{"location":"reference/revelio/model/nn/utils/","text":"","title":"utils"},{"location":"reference/revelio/model/nn/vgg/","text":"","title":"vgg"},{"location":"reference/revelio/model/nn/vision_transformer/","text":"","title":"vision_transformer"},{"location":"reference/revelio/model/nn/callbacks/","text":"","title":"callbacks"},{"location":"reference/revelio/model/nn/callbacks/callback/","text":"","title":"callback"},{"location":"reference/revelio/model/nn/callbacks/early_stopping/","text":"","title":"early_stopping"},{"location":"reference/revelio/model/nn/callbacks/model_checkpoint/","text":"","title":"model_checkpoint"},{"location":"reference/revelio/model/nn/callbacks/tensorboard/","text":"","title":"tensorboard"},{"location":"reference/revelio/model/nn/losses/","text":"","title":"losses"},{"location":"reference/revelio/model/nn/losses/loss/","text":"","title":"loss"},{"location":"reference/revelio/model/nn/losses/losses/","text":"","title":"losses"},{"location":"reference/revelio/model/nn/optimizers/","text":"","title":"optimizers"},{"location":"reference/revelio/model/nn/optimizers/optimizer/","text":"","title":"optimizer"},{"location":"reference/revelio/model/nn/optimizers/optimizers/","text":"","title":"optimizers"},{"location":"reference/revelio/preprocessing/","text":"","title":"preprocessing"},{"location":"reference/revelio/preprocessing/color_space/","text":"","title":"color_space"},{"location":"reference/revelio/preprocessing/normalize/","text":"","title":"normalize"},{"location":"reference/revelio/preprocessing/resize/","text":"","title":"resize"},{"location":"reference/revelio/preprocessing/select_channel/","text":"","title":"select_channel"},{"location":"reference/revelio/preprocessing/step/","text":"PreprocessingStep Bases: Registrable A preprocessing step is applied to all dataset elements before they are passed to the model, and can be used to perform any preprocessing that is not part of the model itself (e.g. resizing, cropping, normalization, etc.). The process_element method is called for each image of the dataset element. A preprocessing step must implement the process_element method, which takes an image and returns its preprocessed version. Source code in revelio/preprocessing/step.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class PreprocessingStep ( Registrable ): \"\"\" A preprocessing step is applied to all dataset elements before they are passed to the model, and can be used to perform any preprocessing that is not part of the model itself (e.g. resizing, cropping, normalization, etc.). The `process_element` method is called for each image of the dataset element. A preprocessing step must implement the `process_element` method, which takes an image and returns its preprocessed version. \"\"\" @abstractmethod def process_element ( self , elem : ElementImage ) -> ElementImage : \"\"\" Processes a single image of a dataset element, and returns its preprocessed version. Args: elem: The image to be preprocessed. Returns: The preprocessed image. \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its preprocessed version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be preprocessed. Returns: The preprocessed dataset element. \"\"\" new_xs = [ self . process_element ( x ) for x in elem . x ] return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ) process ( elem : DatasetElement ) -> DatasetElement Processes a dataset element, and returns its preprocessed version. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to be preprocessed. required Returns: Type Description DatasetElement The preprocessed dataset element. Source code in revelio/preprocessing/step.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its preprocessed version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be preprocessed. Returns: The preprocessed dataset element. \"\"\" new_xs = [ self . process_element ( x ) for x in elem . x ] return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , ) process_element ( elem : ElementImage ) -> ElementImage abstractmethod Processes a single image of a dataset element, and returns its preprocessed version. Parameters: Name Type Description Default elem ElementImage The image to be preprocessed. required Returns: Type Description ElementImage The preprocessed image. Source code in revelio/preprocessing/step.py 19 20 21 22 23 24 25 26 27 28 29 30 31 @abstractmethod def process_element ( self , elem : ElementImage ) -> ElementImage : \"\"\" Processes a single image of a dataset element, and returns its preprocessed version. Args: elem: The image to be preprocessed. Returns: The preprocessed image. \"\"\" raise NotImplementedError # pragma: no cover","title":"step"},{"location":"reference/revelio/preprocessing/step/#revelio.preprocessing.step.PreprocessingStep","text":"Bases: Registrable A preprocessing step is applied to all dataset elements before they are passed to the model, and can be used to perform any preprocessing that is not part of the model itself (e.g. resizing, cropping, normalization, etc.). The process_element method is called for each image of the dataset element. A preprocessing step must implement the process_element method, which takes an image and returns its preprocessed version. Source code in revelio/preprocessing/step.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class PreprocessingStep ( Registrable ): \"\"\" A preprocessing step is applied to all dataset elements before they are passed to the model, and can be used to perform any preprocessing that is not part of the model itself (e.g. resizing, cropping, normalization, etc.). The `process_element` method is called for each image of the dataset element. A preprocessing step must implement the `process_element` method, which takes an image and returns its preprocessed version. \"\"\" @abstractmethod def process_element ( self , elem : ElementImage ) -> ElementImage : \"\"\" Processes a single image of a dataset element, and returns its preprocessed version. Args: elem: The image to be preprocessed. Returns: The preprocessed image. \"\"\" raise NotImplementedError # pragma: no cover def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its preprocessed version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be preprocessed. Returns: The preprocessed dataset element. \"\"\" new_xs = [ self . process_element ( x ) for x in elem . x ] return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , )","title":"PreprocessingStep"},{"location":"reference/revelio/preprocessing/step/#revelio.preprocessing.step.PreprocessingStep.process","text":"Processes a dataset element, and returns its preprocessed version. This method should not be overridden by the user, but instead the process_element method should be implemented. Parameters: Name Type Description Default elem DatasetElement The dataset element to be preprocessed. required Returns: Type Description DatasetElement The preprocessed dataset element. Source code in revelio/preprocessing/step.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def process ( self , elem : DatasetElement ) -> DatasetElement : \"\"\" Processes a dataset element, and returns its preprocessed version. This method should not be overridden by the user, but instead the `process_element` method should be implemented. Args: elem: The dataset element to be preprocessed. Returns: The preprocessed dataset element. \"\"\" new_xs = [ self . process_element ( x ) for x in elem . x ] return DatasetElement ( dataset_root_path = elem . dataset_root_path , original_dataset = elem . original_dataset , x = tuple ( new_xs ), y = elem . y , )","title":"process()"},{"location":"reference/revelio/preprocessing/step/#revelio.preprocessing.step.PreprocessingStep.process_element","text":"Processes a single image of a dataset element, and returns its preprocessed version. Parameters: Name Type Description Default elem ElementImage The image to be preprocessed. required Returns: Type Description ElementImage The preprocessed image. Source code in revelio/preprocessing/step.py 19 20 21 22 23 24 25 26 27 28 29 30 31 @abstractmethod def process_element ( self , elem : ElementImage ) -> ElementImage : \"\"\" Processes a single image of a dataset element, and returns its preprocessed version. Args: elem: The image to be preprocessed. Returns: The preprocessed image. \"\"\" raise NotImplementedError # pragma: no cover","title":"process_element()"},{"location":"reference/revelio/preprocessing/to_float/","text":"","title":"to_float"},{"location":"reference/revelio/registry/","text":"","title":"registry"},{"location":"reference/revelio/registry/registry/","text":"","title":"registry"},{"location":"reference/revelio/utils/","text":"","title":"utils"},{"location":"reference/revelio/utils/files/","text":"glob_multiple ( root : Path , pattern : str , extensions : Iterable [ str ]) -> list [ Path ] Find all files matching a pattern with one of the given extensions in a directory. Parameters: Name Type Description Default root Path The root directory to search in. required pattern str The pattern to match. required extensions Iterable [ str ] The extensions to match. required Source code in revelio/utils/files.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def glob_multiple ( root : Path , pattern : str , extensions : Iterable [ str ]) -> list [ Path ]: \"\"\" Find all files matching a pattern with one of the given extensions in a directory. Args: root: The root directory to search in. pattern: The pattern to match. extensions: The extensions to match. \"\"\" result = [] for extension in extensions : result . extend ( list ( root . glob ( f \" { pattern } . { extension } \" ))) return sorted ( result ) rglob_multiple ( root : Path , pattern : str , extensions : Iterable [ str ]) -> list [ Path ] Find all files matching a pattern with one of the given extensions in a directory tree. Parameters: Name Type Description Default root Path The root directory to search in. required pattern str The pattern to match. required extensions Iterable [ str ] The extensions to match. required Source code in revelio/utils/files.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def rglob_multiple ( root : Path , pattern : str , extensions : Iterable [ str ]) -> list [ Path ]: \"\"\" Find all files matching a pattern with one of the given extensions in a directory tree. Args: root: The root directory to search in. pattern: The pattern to match. extensions: The extensions to match. \"\"\" result = [] for extension in extensions : result . extend ( list ( root . rglob ( f \" { pattern } . { extension } \" ))) return sorted ( result )","title":"files"},{"location":"reference/revelio/utils/files/#revelio.utils.files.glob_multiple","text":"Find all files matching a pattern with one of the given extensions in a directory. Parameters: Name Type Description Default root Path The root directory to search in. required pattern str The pattern to match. required extensions Iterable [ str ] The extensions to match. required Source code in revelio/utils/files.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def glob_multiple ( root : Path , pattern : str , extensions : Iterable [ str ]) -> list [ Path ]: \"\"\" Find all files matching a pattern with one of the given extensions in a directory. Args: root: The root directory to search in. pattern: The pattern to match. extensions: The extensions to match. \"\"\" result = [] for extension in extensions : result . extend ( list ( root . glob ( f \" { pattern } . { extension } \" ))) return sorted ( result )","title":"glob_multiple()"},{"location":"reference/revelio/utils/files/#revelio.utils.files.rglob_multiple","text":"Find all files matching a pattern with one of the given extensions in a directory tree. Parameters: Name Type Description Default root Path The root directory to search in. required pattern str The pattern to match. required extensions Iterable [ str ] The extensions to match. required Source code in revelio/utils/files.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def rglob_multiple ( root : Path , pattern : str , extensions : Iterable [ str ]) -> list [ Path ]: \"\"\" Find all files matching a pattern with one of the given extensions in a directory tree. Args: root: The root directory to search in. pattern: The pattern to match. extensions: The extensions to match. \"\"\" result = [] for extension in extensions : result . extend ( list ( root . rglob ( f \" { pattern } . { extension } \" ))) return sorted ( result )","title":"rglob_multiple()"},{"location":"reference/revelio/utils/iterators/","text":"","title":"iterators"},{"location":"reference/revelio/utils/logging/","text":"","title":"logging"},{"location":"reference/revelio/utils/random/","text":"","title":"random"},{"location":"reference/revelio/utils/rounding/","text":"","title":"rounding"},{"location":"reference/revelio/utils/caching/","text":"","title":"caching"},{"location":"reference/revelio/utils/caching/cacher/","text":"","title":"cacher"},{"location":"reference/revelio/utils/caching/npz_cacher/","text":"","title":"npz_cacher"},{"location":"reference/revelio/utils/caching/zstd_cacher/","text":"","title":"zstd_cacher"}]}